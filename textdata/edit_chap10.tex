--------- はじめに　記号の意味と注意（以下Xは数字を表します） -------------
%ID:XXXX(4桁の数字) ・・・IDがXXXXのスライドに対応したテキスト　文章が途中で分かれていて（2つ目）などと表記があってもIDはそのままです。

%ID:なし　　　　     ・・・スライドに直接関係のない、繋ぎや補足の説明。この「ID:なし」の部分から解答を取ってこないようにしてください。
% figure X.X        ・・・図番号。本来ならここに図が入ります、という意味だけで無視して大丈夫です。解答に含めないようにお願いします。
% equation X.X などその他％から始まる記号 ・・・上記同様、解答に含めないようにお願いします。
$\theta$ など       ・・・数式、引用などのTex記号も無視で、解答に含めないようにお願いします。
\cite{mitchell97}

---------　以下からスライドに対応したテキストデータ ------------------


%ID:1001

アンサンブル学習とは，識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法です
（図10.1）．ここでの問題設定は識別問題ですが，アンサンブル学習の考え方は，ほぼそのまま回帰問題にも適用できます．

% figure 10.1

アンサンブル学習の説明には，「三人寄れば文殊の知恵」ということわざがよく引き合いに出されます．確かに，一つの識別器を用いて出した結果よりは，多数の識別器が一致した結果のほうが，信用できそうな気はします．

そのような直観的な議論ではなく，本当に多数が出した結論の方が信用できるのかどうかを検討してみましょう，ここで，同じ学習データを用いて，異なる識別器を$L$個の作成したとします．仮定として，識別器の誤り率$\epsilon$はすべて等しく，その誤りは独立であるとします．誤りが独立であるとは，評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということです．このような仮定をおくと，この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります．

% equation 10.1

%ID:1002

具体的な数値で考えるために，識別器の個数$L=11$，誤り率$\epsilon=0.2$として，二項分布$B(m; 0.2, 11)$をグラフにかくと，図
10.2のようになります．

% figure 10.2

識別結果を多数決で決めるとすると，全体として誤った判定をする確率は，6個以上の識別器が誤る確率の総和になります．
これを計算すると約1.2\%ということになります．

個々の識別器は誤り率20\%の性能しかないのに，異なる識別器を11個用意すれば誤り率1.2\%の識別器ができてしまいます．

%ID:1003

ただし，これは理論上のことで，現実にはこんなにはうまくゆきません．どこが一番現実と合っていないかというと，
識別器の出す誤りが「独立である」というところです．識別対象のデータには，同じクラスのデータとまとまりを形成していて，比較的識別しやすいデータと，境界線付近にあって，識別がしにくいデータがあります（図1.7参照）．識別しにくいデータは，やはり多くの識別器が誤った結果を出してしまうので，多数決をとることによる誤りの減少は見込まれません．

そこで，いかにして独立な識別器を作るかという工夫が，アンサンブル学習においてなされてきました．以下，それらを
説明してゆきます．

%ID:1004

異なった振る舞いをする識別器を複数作るための最初のアイディアは，異なった学習データを複数用意する，ということです．
ここまでの教師あり学習の説明からもわかるように，学習データが異なれば，たいていその学習結果も異なります．
バギング(Bagging)
はこのアイディアに基づいて，学習データから復元抽出
することで，
もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成する
ものです（図10.3}）．

% figure 10.3

%ID:1006

復元抽出によって作成されたデータ集合が，もとのデータ集合とどれぐらい異なるのかを試算してみましょう．
データ集合の要素数を$N$とします．もとのデータ集合に含まれる，あるデータが復元抽出後のあるデータ集合に含まれない
確率は，$(1-1/N)^N$です．$N=10$の小さいデータ集合の場合，この確率は$0.349$となり，$N=100$で，$0.366$です．
$N \to \inf$で，この確率は$1/e \fallingdotseq 0.368$となるので，おおよそどのような$N$に対しても，復元抽出後のデータ集合には，もとのデータ集合の約$1/3$のデータが
含まれないことになります．

そして，識別器を作成するアルゴリズムは不安定（学習データの違いに敏感）な方が，少しデータが異なるだけで異なった
識別器になるので，よいといわれています．不安定な識別器としては，枝刈りをしない決定木などがあります．
それぞれの識別器が同じ学習データ量で学習しているので，全ての識別器は同程度に信用できると考え，
結果の統合は単純な多数決とします．

%ID:1007

バギングでは，不安定な学習アルゴリズムを用いて，異なる識別器を作成しました．しかし，
復元抽出によって作られた個々のデータ集合は，もとの学習データ集合と約$2/3$のデータを共有しているので，とくに，元のデータのまとまりがよい場合，それほど
極端に異なった識別器にはなりません．ここで説明する
ランダムフォレスト(RandomForest)
は，識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法です．

%ID:1008

ランダムフォレストの学習手順は，基本的にバギングアルゴリズムと同様，学習データから
復元抽出して，同サイズのデータ集合を複数作るところから始まります．
次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を
選ぶ際に，全特徴からあらかじめ決められた数

だけ特徴をランダムに選びだし，その中から最も
分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．
そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．

%ID:1009

通常の決定木の
学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に
枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは
意図的に過学習をさせ，できるだけ異なった木を作るようにします．

%ID:1008 (2つ目)

図10.4に，ランダムフォレストによる個々の木の作成の様子を示します．
たとえば，学習データの特徴が$\{A,B,C,D,E\}$と5種類あるとして，ルートノードの分岐特徴を決める際は，
ここから予め決められた数（たとえば3種類）をランダムに選択します．ここで，$\{A,B,E\}$が選択された
とすると，それぞれの分類能力を計算し，最も分類能力の高い属性を選んで，その値によってデータを
分割します．分けられたデータ集合に対しても，同様にランダムに属性集合を選択して，その中で最も
分類能力の高い属性を選んで木を成長させます．

% figure 10.4

このような手順で，識別器の方を強制的に異なったものにするのがランダムフォレストの手法です．

%ID:1010

バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，
ブースティング (Boosting) 
では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります．

%ID:1012

作成する識別器に対して，誤りを減らすことに特化させるために，個々のデータに対して重みを設定します．バギングではすべてのデータの重みは平等でした．
一方，ブースティングのアイディアは，
各データに重みを付け，そのもとで識別器を作成します．
最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成してゆきます．後から作られる
識別器は，前段の識別器が誤ったデータを優先的に識別するようになるので，前段の識別器とは異なり，かつその弱いところを補うような相補的働きをします（図10.5）．

% figure 10.5

ブースティングに用いる識別器の学習アルゴリズムは，基本的にはデータの重みを識別器作成の
基準として取り入れている必要があります．ただし，学習アルゴリズムが重みに対応していない場合は，
重みに比例した数を復元抽出してデータ集合を作ることで対応可能です．

このように，前段での誤りに特化して逐次的に作成された識別器は，もとの学習データをゆがめて作成されているので，未知の入力に対しては，もとの学習データに忠実に作られた識別器（たとえば，図10.5の識別器1）とは，信頼性が異なります．したがって，バギングのように単純な多数決で
結論を出すわけにはゆきません．各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票
で識別結果を出します．

%ID:1013

このようなブースティングアルゴリズムの代表的なものに
AdaBoost
があります．
AdaBoostでは，誤識別されたデータの重みの和と，正しく識別されたデータの重みの和とが等しくなるように，
各データの重みの変更が行われます．学習ステップにおける$t$段階目の識別器の誤り率を$\epsilon_t$とすると，
誤識別されたデータの重みに$\frac{1}{2\epsilon_t}$，正しく識別されたデータの重みに$\frac{1}{2(1-\epsilon_t)}$を
掛ければ，それぞれのデータの割合が$\epsilon_t:1-\epsilon_t$であることから，更新後のそれぞれの重みの総和が
$\frac{1}{2}$になることがわかります．なお，この重みの更新は，誤識別されたデータの重みが大きくなるように
更新されなければ意味がないので，更新中に$\epsilon_t \ge 0.5$になると繰り返しが中断されます．

%ID:なし

次に，このような重み付きデータで作成された識別器の重み$\alpha_t$を，どう設定すればよいか考えてみましょう．
識別器の重み$\alpha_t$は，誤り率$\epsilon_t$が増えるに従って小さくなってゆき，$\epsilon_t=0.5$で$0$になる
ように考えます．$\alpha_t = 0.5-\epsilon_t$のような簡単な式が思いつくかもしれませんが，AdaBoostでは以下
のように$\alpha_t$を設定します．

% equation

このように$\alpha_t$を設定すると，AdaBoostは，統合後の識別器の出力$\hat{c}(\bm{x}_i)$と
正解ラベル$y_i=\{1,-1\}$から
求めることができる指数損失$\sum_{i=1}^{N}\exp(-\hat{c}(\bm{x}_i) y_i)$を最小化していることになり，
その手順の理論的な正しさが示されていることになります．

アルゴリズム10.1に，AdaBoostアルゴリズムの学習手順を示します
．
ここで$I( )$は引数の条件が成立すれば1，そうでなければ0を返す関数です．

% algorithm 10.1

%ID:1014

前章で，Adaboostは，特定の損失関数を最小化していることと同じであることを説明しました．Adaboostは，比較的素朴な方法で識別器の逐次的組み合わせをおこなった結果，それが理論的に損失関数最小化の枠組みに当てはまるというものでしたが，結局，損失関数最小化をおこなっているのであれば，直接その損失関数が最も減るような識別器を加えることを逐次的におこなうことで，よりよい性能が実現できそうです．

そのようなアイディアでブースティングをおこなっているのが，
勾配ブースティング
です．以下，勾配ブースティングの考え方を説明します．

ブースティングでは，全体の識別器の出力は，式(10.2)に示すように，M個の識別器$h_m$の重み付き和で得られます．

% equation 10.2

この形式で，ブースティングにおける逐次加法的な学習プロセスを表現すると，
式(10.3)のようになります．

% equation 10.3

学習の各ステップで，新たに加える識別器$h_m(\bm{x})$を選択するために，損失関数$L$を導入し，その値が最小となるものを選ぶこととします．

% equation 10.4

%ID:1015

式(10.4)で，差が最小になるものを求めている部分を，損失関数の勾配に置き換えた式(10.5)に従って，新しい識別器を構成する方法が，勾配ブースティングです．

% equation 10.5

損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます．

%ID:なし

本章では，識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法であるアンサンブル学習について説明しました．

機械学習においては，モデルのバイアスと学習結果の分散（不安定性）はトレードオフの関係にあることを，これまでもいくつか見てきました．この章で説明したバギング・ランダムフォレストとブースティングもこの観点で見ることができます．バギング・ランダムフォレストは，分散の大きい識別器の平均的を取ることで分散を減らすテクニックであると見ることができます．一方，ブースティングはバイアスの強い識別器を組み合わせて，単独の識別器では実現できない機能を実現するもので，バイアスを緩めるテクニックと見ることができます．

アンサンブル学習を回帰問題に適用する際には，それぞれのモデルの出力値の平均値や中央値を用いることで，単一のモデルと比較した場合の性能の向上が見込まれます．
