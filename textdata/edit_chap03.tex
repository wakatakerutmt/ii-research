%chap03/1

この章では，各次元がカテゴリカルデータである特徴ベクトルと，その正解クラスの情報からなる学習データを用いて，「クラスの概念を得る方法」について説明します．例えば，「乳癌が再発しやすい」という概念を，年齢・腫瘍のサイズ・放射線治療の有無などの情報を組合せて表現することを学習することが目的になります．

%chap03/2
%%chap03/1

第3章から第5章では，正解情報の付いた学習データを用いる教師あり学習の設定で，識別をおこなうモデルを学習する方法について説明します．
まず第3章と第4章は，カテゴリカルデータからなる特徴ベクトルを入力として，それをクラス分けする（すなわち属するクラスラベルを出力する）識別器を作る方法について学びます（図3.1）．

%%chap03/2

識別問題は教師あり学習なので，学習データは特徴ベクトル$\bm{x}_i$と正解情報$y_i$のペアからなります．

ここでの設定は，特徴ベクトル$\bm{x}_i$の各次元および正解情報$y_i$がいずれもカテゴリです．特にカテゴリ形式の正解情報のことを
クラス
とよびます．

このカテゴリ特徴に対する「教師あり・識別」問題に対して，いかに納得のゆく概念モデルを獲得するか，という点に重点を置いたものが，この章で説明する概念学習です．一方，特徴が与えられたときに，それがあるクラスに属する確率を計算するモデルの獲得を目的とするものが，第4章で説明する統計的手法です．

%chap03/4

機械学習において与えられるデータは，個々の事例です．その個々の事例から，あるクラスについて共通点を見つけることが，
概念学習
です．共通点は，特徴の値の組み合わせによって表現されます．

%chap03/3

具体的に学習データを見ながら考えてゆきましょう．表3.1に示すデータは，Weka 付属の contact-lenses データで，「ソフトコンタクトレンズの使用を勧める」・「ハードコンタクトレンズの使用を勧める」・「コンタクトレンズの使用を勧めない」という概念を獲得するための架空のデータです．

それぞれの特徴は，表3.2に示すいずれかの値をとります．

このデータから，「コンタクトレンズの使用を勧めない」という概念を獲得する手順を考えてみます．対象とする概念に当てはまるデータ（クラスnoneのデータ）が正例，当てはまらないデータ（クラスsoftとクラスhardのデータ）が負例となります．

%chap03/5

概念学習手法が研究されていた初期の頃には，概念の表現形式を限定することで，データに当てはまる概念の仮説を少なくし，その仮説の空間を探索することで概念を求める手法が開発されました．そのような手法として，FIND-Sアルゴリズムや，候補削除アルゴリズムがあります（図3.2）．

FIND-Sアルゴリズムは，仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限します．このように，仮説に対して課す制約を
バイアス
とよびます．最初は，最も特殊な仮説（いかなる事例も正ではない）からスタートし，正例を一つずつ読み込んで，その事例の値を受け入れるように仮説を最低限一般化します．

たとえば，表3.1のデータにおいて，最初の正例である1番のデータから，論理式「age=young $\wedge$ spectacle-prescrip=myope $\wedge$ astigmatism=no $\wedge$  tear-prod = reduced」が得られます．次の正例である3番のデータは，age, spectacle-prescrip, tear-prodの値はこの論理式に当てはまりますが，astigmatismの値が異なります．
1番と3番のデータの両方が当てはまるようにするために，この論理式から，astigmatismの条件を取り除き，新たな仮説を「age=young $\wedge$ spectacle-prescrip=myope $\wedge$ tear-prod = reduced」とします．

これを続けると，5番のデータでspectacle-prescripの条件が落ち，9番のデータでageの条件が落ち，最後は16番のデータでtear-prodの条件まで落ちて，条件が何もなくなってしまいます．これでは，すべての入力が正例であるという概念になり，明らかにおかしな結果になってしまいました．

%chap03/6

候補削除アルゴリズムは，FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法です．しかし，候補削除アルゴリズムでも表現できる仮説の制約は同じなので，FIND-Sアルゴリズムと同じ手順で，概念の学習に失敗します．

これらのアルゴリズムが，概念の学習に失敗する理由は，仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないことです．

%chap03/0

ここで，FIND-Sアルゴリズムや候補削除アルゴリズムが想定している概念の表現として可能な仮説は何種類あるかを数えてみましょう．
表3.1のデータにおいて，全特徴に対して「特徴値の種類+1」（+1はその特徴の値を問わない場合）を掛け合わせると$4 \times 3 \times 3 \times 3 = 108$となります．また，全てを負に分類する仮説も1つ必要です．したがって，すべての可能な仮説は109種類ということになります．
このように仮説を絞った場合でも，学習データを説明できる仮説が見つかった場合は，その条件を満たす未知のデータも正しく分類できる可能性は高くなります．つまり，仮説の表現に関するバイアスが強い状況にも関わらず見つかった概念は，一般化してもよいだろうということがいえます．しかし，上の例で見たように，小規模かつ特殊なデータでない限りは，このような概念が見つかる可能性は低くなります．

それでは，特徴値のOR結合を仮説表現として認めればどうなるでしょう．これは論理式を求めるという設定においてはバイアスをなくしてしまうことになり，仮説空間は原理的に全ての可能な特徴値の組合せを要素とした冪集合となります．具体的にいうと，表3.2から，可能な特徴値の組合せは$3 \times 2 \times 2 \times 2 = 24$種類となり，それぞれが正例または負例となり得るので，正例の集合として可能なもの（すなわち求める概念の候補数）は$2^{24}$個ということになります．

バイアスをなくすことによる弊害は，この候補数の多さだけには止まりません．バイアスという単語は統計の文脈で出てくるときは「偏り」と訳されますが，日常的な文脈では「偏見」です．「この特徴がこの値をとるデータは正例に決まっている」という偏見を持っているからこそ，学習結果が未知データに対する汎化性能を持っているといえるわけです．一方，偏見をなくしてしまうと，学習データの事例とたった1カ所だけ値が異なる事例に対しても「似ているからといって正例とは限らないぞ」という判断になります．

すなわち，バイアスのない仮説空間上で，候補削除アルゴリズムと同様の方法で候補を絞り込んでゆくと，最も特殊な仮説は正例のOR結合，最も一般的な仮説は負例のOR結合の否定になり，この方法では，未知のテストデータに関して判断する根拠を持たないことになります．つまり，目標概念に関して何の仮定も設けていないバイアスなしの学習アルゴリズムは，本来の機械学習の目的を果たしません．「学習データにあるもののみ正しく答える」という暗記学習とでも呼ぶべきものになります．

%chap03/7

それでは，特徴値のOR結合を仮説とした機械学習は不可能なのでしょうか．もちろんそんなことはありません．仮説空間にバイアスをかけられないのなら，探索手法にバイアスをかけます．「このような探索手法で見つかった概念ならば，汎化性能が高いに決まっている」というバイアスです．ここではそのような学習手法の代表である決定木について説明します．

決定木とは，データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現です．
正例のリーフに到るノードの分岐の値をAND条件で結合し，それらをさらにOR条件で結合することで等価な論理式に変換できますが，
木構造のままの方が，人間の目から見て学習結果がわかりやすいので，こちらの表現が好まれます．

コンタクトレンズデータ (contact-lens.arff)（表3.1）から作成した決定木の例を図3.3に示します．

%chap03/8

決定木の説明には，「二十の扉」という遊びがよく用いられます．「二十の扉」は，出題者が思い浮かべた概念（例えば「犬」）を，回答者が二十個以内の質問（例えば「それは生き物ですか」）を重ねて当てるクイズ遊びです．このとき，うまく対象を絞れる質問をなるべく初期に持ってくる方が，答えにたどり着く確率が高くなります．最初の質問があまりにも特殊（例えば「それは毒キノコの一種ですか」）だと，当たれば一気に候補が絞り込めますが，大抵の場合，はずれてほとんど何の情報を得たことにもなりません．

この「二十の扉」の必勝法のように，得られる情報が多い質問（ここでは特徴）をなるべく木の上のノードに持ってくるように構成されたものが決定木です．

%chap03/7

図3.3の木では特徴tear-prod-rate（涙量）が最初の質問で，この値がreduced（減少）であると，コンタクトレンズは勧められない，という結論になります．この値がnormal（正常）であれば，次の特徴astigmatism（乱視）を調べる，という手順になります．

%chap03/9,10

このような決定木を作成するもっとも基本的な手順がID3アルゴリズム（アルゴリズム3.1）です．

%chap03/11

ID3アルゴリズムの中で，詳しい説明のない「特徴集合A中で最も分類能力の高い特徴」を決定する方法について説明します．
分類能力が高いとは，「その特徴を使った分類を行うことによって，なるべくきれいに正例と負例に分かれる」という
ことです．いいかえると，乱雑さが少なくなるように分類を行うということですね．乱雑さの尺度として，エントロピーを
用います．学習データ集合$D$の乱雑さを計算するために，まず正例の割合: $P_+$,  負例の割合: $P_-$を計算し，それを元に
式(3.1)によって，その集合の乱雑さ（エントロピー）$E(D)$を求めます．

エントロピーの値は$P_+=1$または$P_-=1$のとき最小値0となり，$P_+=P_-=0.5$のとき，最大値1となるので，
エントロピーの値が小さいほど，集合が乱雑でない，すなわち整っている（同じクラスのものが大半を占めている）という
ことになります．

このエントロピーの減り具合を計算したいのですが，単純に引き算はできません．
エントロピーは集合に対して定義できるものです．分類前は1つの集合で，分類後はその特徴値の種類数だけ集合ができます．
そこで，分類後の集合の要素数の割合で重みを付けて計算します．この値を情報獲得量と定義します．

%chap03/12

情報獲得量は，ある特徴を用いた分類後のエントロピーの減少量とします．特徴$a$の可能な値$Values(a)$の中から，値$v \in Values(a)$を取る学習データの集合を$D_v$，集合$D_v$の要素数を$|D_v|$と表現したとき，情報獲得量は式(3.2)のように定義できます．

%chap03/0

このID3アルゴリズムのバイアスを考えましょう．
ID3アルゴリズムの学習結果である決定木は，正例の集合を表すリーフを複数持つことができるので，その
仮説空間は特徴値のOR結合を許していることになります．先述したように，OR結合を許した仮説空間に何のバイアスもかけなければ，
正例のOR結合と，負例のOR結合の否定が得られるだけで，学習になりません．そのような仮説空間に
バイアスとして，「分類能力の高いノードをなるべく根の近くに持ち，（その結果として）なるべく小さな木」というものを与えることによって，未知データへ対処できる結果を得ます．また，探索空間が大きく全探索が難しいので，分岐毎によい結果を積み重ねてゆく
欲張り(greedy)法
で目標概念を探します（図3.4）．この方法で作成された決定木は，最適であることが保証されないので，このことが解の不安定性をもたらしていることになります．

ここで説明した決定木と，初歩的な概念学習アルゴリズムの違いは，正解概念が必ず仮説空間に含まれており，それを短時間で見つけることができる点です．また，1度に全ての学習データを用いるのでエラーデータの影響を吸収できるという実用的な利点もあります．
一方欠点としては，学習の高速化のために欲張り法で探索を行っているので，結果として得られた木が最小のものではない
可能性があります．また，アルゴリズムのパラメータ設定によっては，学習データに特化しすぎた概念が得られる，いわゆる過学習の
問題が生じます．

%chap03/13

ここで，ID3アルゴリズムにおける過学習について考えてみます．過学習とは，文字通りの意味は学習しすぎるということですが，機械学習においては，モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象を指します．

ID3アルゴリズムのバイアスは単純に表現すると，「小さい木を好む」となります．なぜ小さな木を結果とするのでしょうか．これはオッカムの剃刀 (Occam's razor) と呼ばれる「データに適合する最も単純な仮説を選べ」という基準に基づいています．長い仮説なら表現能力が高いので，偶然に学習データを説明できるかもしれないのですが，短い仮説だと，表現能力が低いので，偶然データを説明できる確率は低くなります．もし，ID3アルゴリズムによって，小さな木で学習データが説明できたとすると，これは偶然である確率は相当低くなります．すなわち偶然でなければ必然である，というわけです．

ところが，このバイアスを持って学習を行ったとしても，最後の1例までエラーがなくなるように決定木を作成してしまうと，その決定木は成長しすぎて，学習データに適応しすぎた過学習になりがちです．そこで，過学習への対処として，
適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法があります．

%chap03/14

リーフの要素数は，学習データの量や性質によって左右され，事前に決めるのが難しいので，枝刈りの方が実用上有効です．
枝刈りは，学習データを学習用データと検証用データに分割し，学習用データで十分に木を成長させた（すなわち
過学習させた）のち，検証用データを用いて余分な枝を見つけて刈り取ることによって行います．決定木の枝刈りの
手順をアルゴリズム3.2に示します．ただし，$\mbox{accuracy}(T, D)$は決定木$T$を用いてデータ$D$を識別したときの正解率，
$\mbox{majority}(D)$はデータ$D$中の最頻正解ラベルの割合，$\mbox{majority\_class}(D)$はその最頻正解ラベルを求める処理を示します．

%chap03/15

ID3アルゴリズムで用いた情報獲得量は，値の種類が多い特徴ほど大きな値になる傾向があります．一般に，その性質は悪いもの
ではないのですが，値の種類が極端に多い場合には問題があります．
例えば表3.3の特徴として，日付(date)があったとし，その値が全てのデータで異なっているとします．

この場合，dateによって分割した集合は要素数が1となって，そのエントロピーは0となりますので，$\mbox{Gain}(D, date)$の値は
最大値である$E(D)$になって，この特徴が決定木のルートに選ばれることになります．こうして出来た決定木ではテスト例
は分類できません．

そこで，分割の程度を式(3.3)によって評価し，分割が少ない方が有利になるように式(3.4)で定義された獲得率を用いて特徴を選択することもあります．

また，学習データの性質や学習の目的によって，データの乱雑さを評価する基準も変化することがあります．
データの乱雑さを不純度(impurity)と定義すると，先述のエントロピー以外にいくつかの可能性を考える
ことができます．

式(3.5)で計算されるGini Inpurityは，分割後のデータの分散を表します．この性質は回帰木の作成
で用いますので，そこで再度，解説します．

また，Gini Inpurityの平方根を取って，最大値がGini Inpurityと同じ0.5になるように係数を補正したものを
RootGiniImpurityとして，式(3.6)で定義します．

いずれも，分割前後の値の差によって選ぶ特徴を決めるのですが，獲得率やジニ不純度
は，正例・負例の数に偏りがあると，多数派の性能の影響が大きくなってしまいます．
一方，RootGiniImpurityは，分割前のGini Inpurityと，分割後の重み付きGini Inpurityの比を計算していることになり，
正例・負例の数に偏りがあっても，分割基準としては影響を受けないようになります．

%chap03/16

ここでは，特徴ベクトルのなかで数値を値とする特徴がある場合の決定木学習について説明します．

基本的なアイディアとして，連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなうことで，決定木での学習を可能にします．
具体的には，数値特徴$a_i$に対して$a_i < \theta$という条件式を値とするノードを作成します．
この条件式を満たすデータと満たさないデータに分割することで，ラベルを値として持つ特徴に対する決定木学習と同じアルゴリズムが
使えます．
問題は，閾値$\theta$をどうやって決めるかということです．

具体的にデータを見ながら，考えてゆきましょう．Weka付属のweather.numericデータ（表3.5）は，
weather.nominalデータの特徴temperatureと特徴humidityを数値にした
ものです．たとえば，特徴humidityの値でデータを分割する場合を考えてみましょう．

最もエントロピーが低くなるような切り方を見つけたいので，同じクラスの中で切ることは意味がありません．クラスの境目を探すと，図3.6に点線で示す箇所になります．この境目の値はその前後の値の平均値をしておきましょう．

これらの中で最も情報獲得量の多い場所を選びます．カテゴリ特徴のときと同様の計算を行うと，$c=82.5$となるときに（すなわち，82.5未満の値はyes，82.5以上はnoとなるカテゴリ特徴に変換したときに）最も情報獲得量が多い分割になります．

この操作をすべての数値特徴についておこなった後は，通常のID3アルゴリズムと同じです．

%chap03/0

この章では概念学習について説明しました．
決定木は学習データに対して特徴値による分割を繰り返し適用するもので，分割された集合のラベルが等しいものに
偏るように特徴が選ばれてゆくものです．結果として得られた木は，学習データを分割する規則集合とみなすことも
でき，学習された結果の妥当性を人間が見て判断することが容易なことから，この決定木学習は様々な分野で
応用されています．

概念学習に関しては，文献\cite{mitchell97}の2章，3章に詳しく説明されています．

