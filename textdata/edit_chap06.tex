%chap06/1

本章で扱う回帰問題は，過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする
問題です
過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても
妥当な数値を出力してくれる関数を学習することが目標です．

回帰問題は，正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題と定義できます
(図6.1)．

% figure 6.1

%chap06/2

関数を学習するためのデータは，すべての要素が数値である特徴ベクトルと，その出力値（スカラーの場合も，
ベクトルの場合もあります）の対として与えられます．

% equation 6.1

識別問題との違いは，正解情報$y$が数値であるということです．特に数値型の正解情報のことを
ターゲット
とよびます．

しかし，回帰と
「数値特徴を入力としてクラスラベルを出力する」識別問題との境界はそれほど明確ではありません．
例えば，クラスによって異なる値をとるクラス変数を導入し，入力からクラス変数の値を予測する問題と考える
と，識別問題を回帰問題として考えることもできます．

実際，カーネル法など共通して使われる手法も多くあり，混乱しそうになってしまうのですが，
まずはここでは「数値特徴を入力として数値を出力する」手法の習得に集中し，その全体像が
見えてから，他の問題との関係を考えてゆきましょう．

%chap06/3


まず，最も単純な，入力も出力もスカラーである場合の回帰問題（図6.2）を考えましょう．

% figure 6.2

この学習データから，入力$x$を出力$y$に写像する関数$\hat{c}(x)$を推定します．
図6.2のデータからは，入力$x$が大きくなると，出力$y$も大きい値になる傾向が
見えます．そこで，この傾向を直線であらわして，入力$x$と出力$y$を関係付けることを試みます．


もし学習データのすべての点が，その上にあるように直線を決めることができれば，これで問題は
終わりなのですが，通常そのようなことはありません．そこで，図6.3のようになるべく誤差の
少ない直線を求めることとします．

% figure 6.3

そうすると，ここでの定式化は5.3.2項で説明した最小二乗法による学習と等しくなります．
違いは，識別問題における教師信号$y_i$が1または0であったのに対して，回帰問題の教師信号$y_i$が
連続値であるということですが，学習アルゴリズム自体は変更なく適用することができます．

回帰式を式（6.2)

% equation　6.2

とする．

%chap06/4

誤差の二乗和は以下のように

% align 6.3, 6.4

なります．ただし，$\bm{X}$は1列目のすべての要素が1，2列目$i$行目の要素が$x_i$であるパターン行列，$\bm{W}$は，$(w_0, w_1)^T$です．

式(6.4)を$\bm{w}$で微分したものを$0$とおいて解くと，式(6.5)

% equation 6.5

となります．このような線形回帰式の求め方は，入力$\bm{x}$が一般の$d$次元の場合も，そのまま通用します．

解が解析的に求まってしまい，探索や最急勾配法による逐次的な修正も行っていないので，何か
機械学習という感じはしませんが，一応これは，学習データから得られる最も誤差の小さい線形回帰式です．

%chap06/5

ここでは回帰モデルの評価について考えます．
教師付き学習においては，未知データに対する
誤差が問題となります．この回帰式は未知データに対してもうまく値を予測してくれるのでしょうか．

回帰問題の評価は，交差確認法との相性はあまりよくありません．識別問題では，交差確認に用いるデータの部分集合は，そこに含まれるクラスの割合が，全体の割合と整合するように分割すれば，一回ごとの評価値がそれほど極端にはぶれず，ある程度適切な評価が行えます．しかし，回帰では何をもって部分集合の構成が近いかを定義することが難しくなります．したがって，計算能力に余裕があれば，一つ抜き法で評価することをお勧めします．

そこでの評価指標は，学習の基準に合わせると平均二乗誤差ということになります．しかし，この値はデータが異なれば，スケールがまったく異なるので，結果がよいものかどうか直観的にはわかりにくいものです．そこで，回帰の場合は，正解と予測とがどの程度似ているかを表す相関係数や，式(6.6)で計算できる決定係数で評価します．決定係数は，「正解との離れ具合」と「平均との離れ具合」の比を1から引いたものですが，式変形により相関係数の二乗と一致するので，$R^2$とも表記されます．

% equation 6.6

%chap06/6

次に，線形回帰式の重みに注目します．

一般的に，入力が少し変化したときに，出力も少し変化するような線形回帰式が，汎化能力という点では望ましいと思われます．このような性質を持つ線形回帰式は，重みの大きさが全体的に小さいものです．逆に，重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．

また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．

つまり，回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫を
正則化
とよび，誤差の式に正則化項と呼ばれる項を追加することで実現します．

パラメータ$\bm{w}$の二乗を正則化項とするものを
Ridge回帰
とよびます．
Ridge回帰に用いる誤差評価式を式(6.7)に示します．
ここで，$\lambda$は正則化項の重みで，大きければ性能よりも正則化の結果を重視，小さければ性能を重視するパラメータとなります．

% equation 6.7

最小二乗法でパラメータを求めたときと同様に，$\bm{w}$で微分した値が0となるときの$\bm{w}$の
値を求めると，式(6.8)のようになります．

% equation 6.8

Ridgeは山の尾根という意味で，単位行列が尾根のようにみえるところから，このように名付けられたといわれています．
一般に，Ridge回帰は，パラメータの値が小さくなるように正則化されます．

%chap06/7

また，パラメータ$\bm{w}$の絶対値を正則化項とするものを
Lasso回帰
とよびます．一般に，Lasso回帰は値を0とするパラメータが多くなるように正則化されます．
英単語のlassoは「投げ縄」という意味で，投げ縄回帰と訳されることがあります．多くの特徴がひしめき合っている中に
投げ縄を投げて，小数のものを捕まえるというイメージをもってこのように呼ばれているのかもしれませんが，
Lassoのオリジナルの論文では，Lasso は least absolute shrinkage and selection operator の意味だと書かれています．

Lasso回帰に用いる誤差評価式を，式(6.9)に示します
．ここで，$\lambda$は正則化項の重みで，大きければ値を0とする重みが多くなります．

% equation 6.9

Lasso回帰の解は，原点で微分不可能な絶対値を含むため，最小二乗法のように解析的に求めることはできません．
そこで，正則化項の上限を微分可能な二次関数で押さえ，その二次関数のパラメータを誤差が小さくなるように
繰り返し更新する方法などが提案されています．

%chap06/8

Ridge回帰とLasso回帰における正則化の振る舞いの違いを図6.4に示します．

% figure 6.4

Ridge回帰は，図6.4(a)に示すように，パラメータの存在する範囲を円（一般の$d$次元では超球）の中に限定することで，それぞれの重みが大きな値を撮れないようにします．一般に，誤差関数の等高線との接点は，円周上の点になり，これが重みの値となります．一方，Lasso回帰は，パラメータの和が一定という
条件なので，図6.4(b)に示すように，それぞれの軸で角を持つ領域に値が制限されます．そして，その角のところで誤差関数の等高線と接します．
角の部分は，多くのパラメータが0になるので，これがLasso回帰の正則化に反映されます．

%chap06/9

前節で説明した最小二乗法は，回帰式を高次方程式に置き換えてもそのまま適用できます．一般に，特徴ベクトルに対して，
式(6.10)で示す基底関数を考え
，回帰式を式(6.11)のように定義すれば，係数が線形であるという条件の下で，最小二乗法が適用可能です．

% equation 6.10

% equation 6.11

複雑な関数を用いれば，真のモデルに近い形を表現できると考えられるので，より複雑な関数を用いればよいようにみえますが，
はたしてそうでしょうか．

%chap06/10

ここで，バイアスと分散の関係を考えてみます．バイアスは真のモデルとの距離，分散は学習結果の
散らばり具合と理解してください．

線形回帰式のような単純なモデルは，個別のデータに対する誤差は比較的大きくなってしまう傾向があるのですが，
学習データが少し変動しても，結果として得られるパラメータはそれほど大きく変動しません．これをバイアスは
大きいが，分散は小さいと表現します．
逆に，複雑なモデルは個別のデータに対する誤差を小さくしやすいのですが，学習データの値が少し異なるだけで，
結果が大きく異なることがあります．これをバイアスは小さいが，分散は大きいと表現します．

このバイアスと分散は，片方を減らせば片方が増える，いわゆるトレードオフの関係にあります．

このテーマは第3章の概念学習でも出てきました．概念学習では，バイアスを強くすると，安定的に
解にたどり着きますが，解が探索空間に含まれず，学習が失敗する場合がでてきてしまいました．一方，バイアスを弱くすると，
探索空間が大きくなりすぎるので，探索方法にバイアスをかけました．探索方法にバイアスをかけてしまうと，最適な概念（オッカムの剃刀に
従うと，最小の表現）が求めることが難しくなり，学習データのちょっとした違いで，まったく異なった結果が得られることが
あります．

回帰問題でも同様の議論ができます．線形回帰式に制限すると，求まった平面は，一般的には学習データ内の点をほとんど通らないので
バイアスが大きいといえます．一方，「学習データの個数-1」次の高次回帰式を仮定すると，「係数の数」と「学習データを回帰式に代入した制約の数」が等しく
なるので，連立方程式で解くことで，重みの値が求まります．つまり，「学習データの個数-1」次の回帰式は，全学習データを通る式に
することができます．これが第1章の図1.8(c)に示したような例になります．
この図からもわかるように，データが少し動いただけでこの高次式は大きく変動します．つまり，バイアスが弱いので
学習データと一致する関数が求まりますが，変動すなわち結果の分散はとても大きくなります．

このように，機械学習は常にバイアス－分散のトレードオフを意識しなければなりません．前節で説明した正則化は
モデルそのものを制限するよりは少し緩いバイアスで，分散を減らすのに有効な役割を果たします．

%chap06/11

回帰木とは，識別における決定木の考え方を回帰問題に適用する方法です．このような判断をしたから，この値が求まった
というように，結果に説明を付けやすくなるのが特徴です．

決定木では，同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，
という考え方でした．それに対して，回帰では，出力値の近いデータが集まるように，特徴の値によって学習データを分割して
ゆきます．結果として得られる回帰木は図6.5のように，特徴をノードとし，出力値をリーフとするものに
なります．

% figure 6.5

%chap06/12

CART(classification and regression tree)は，木の構造を二分木に限定し
分類基準としてGini Impurity （ジニ不純度）を用いた決定木です．

Gini Impurityは識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて
分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\Delta G$が最も大きい
ものをノードに選ぶことを再帰的に繰り返します．

% equation 6.12

% equation 6.13

ただし，$T$はあるノードに属する要素の全体，$N(j)$は要素中のクラス$j$の割合，$T_L$は左の部分木，$P_L$は$T_L$に属するデータの割合（$L$を$R$に変えたものも同様）
を示します．

%chap06/13

このCARTを回帰問題に用いる時は，分類基準としてデータの散らばり$SS$の減り方$\Delta SS$が最大になるものを選びます．

% equation 6.14

% equation 6.15

ここで，$\tilde{y}$は$T$に属するデータの平均値です．
式(6.14)は木$T$に含まれるデータの分散を求めていることになるので，式(6.15)の基準は，
分割後の分散が最小となるような分割を求めていることに対応します．

%chap06/14

モデル木は，回帰木と線形回帰の双方のよいところを取った方法です．
CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式
とします．

回帰木と同じ考え方で，データの出力値が近い区間を切り出せる特徴を
選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます．

%chap06/0

ここでは回帰問題への解法として，線形回帰と回帰木を説明し，それらのよいところをとって，いわゆる
区分線形問題として回帰へアプローチした，モデル木についても説明しました．

%chap06/16

次に考えられる手法としては，
区分線形よりももっとなめらかな非線形関数を用いて回帰式を得られないか，ということになるのですが，
一般に非線形式ではデータにフィットしすぎてしまうため，過学習が問題になります．そこで，SVMのところで
も説明したカーネル法を使って，非線形空間へ写像した後に，線形識別を正規化項を入れながら学習するという
アプローチが有効になります．この手法については，文献\cite{akaho08}に詳しく書かれています．

