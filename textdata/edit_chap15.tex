--------- はじめに　記号の意味と注意（以下Xは数字を表します） -------------
%ID:XXXX(4桁の数字) ・・・IDがXXXXのスライドに対応したテキスト　文章が途中で分かれていて（2つ目）などと表記があってもIDはそのままです。

%ID:なし　　　　     ・・・スライドに直接関係のない、繋ぎや補足の説明。この「ID:なし」の部分から解答を取ってこないようにしてください。
% figure X.X        ・・・図番号。本来ならここに図が入ります、という意味だけで無視して大丈夫です。解答に含めないようにお願いします。
% equation X.X などその他％から始まる記号 ・・・上記同様、解答に含めないようにお願いします。
$\theta$ など       ・・・数式、引用などのTex記号も無視で、解答に含めないようにお願いします。
\cite{mitchell97}

---------　以下からスライドに対応したテキストデータ ------------------

%ID:1501

この章では，強化学習について説明します．強化学習は，教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので，中間的学習として位置付けています（図15.1）．

%ID:1502

強化学習
とは，「報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習」と定義することができます（図15.2）．

実世界で行為を行う意思決定エージェントというと，ロボットが思いつきます．
バーチャルな世界で思いつきやすいのは，将棋や囲碁などを行うプログラムでしょうか．強化学習は，このような意思決定を行うエージェントを賢くする学習法です．

エージェントには，環境についての情報が与えられます．たとえば，ロボットでは，センサ・カメラ・マイクなどからの入力が環境となります．多種多様な環境を連続的に考えるのは難しいので，環境は離散的な状態の集合$S=\{s|s \in S\}$
でモデル化できると仮定します．時刻$t$で，ある状態$s_t$において，エージェントが行為$a_t$を行うと，報酬$r_{t+1}$が得られ，状態$s_{t+1}$に遷移します．
一般に，状態遷移は確率的で，その確率は遷移前の状態にのみ依存すると考えます．このような問題の定式化を
マルコフ決定過程(Markov Decision Process: MDP)
とよびます．

また，強化学習で考えている問題では，報酬$r$はたまにしか与えられません．
将棋やチェスなどのゲームを考えると，個々の手が良いか，悪いかはその手だけでは判断できず，
最終的に勝ったときに報酬が与えられます．ロボットが迷路を移動する問題でも，個々の道の
選択には報酬は与えられず，ゴールにだとりついた段階で報酬が与えられます．この場合，
回り道をすれことを避けるために，選択毎にマイナスの報酬を与える場合もあります．

このように定式化すると，強化学習は，なるべく多くの報酬を得ることを目的として，
状態(ラベル)または状態の確率分布（連続値）を入力として，行為（ラベル）
を出力する関数を学習することと定義できます．

ただし強化学習は，その設定上，これまでの教師あり／教師なし学習とは違う問題になります．
他の機械学習手法との違いは以下のようになります．

\begin{itemize}
\item 教師信号が間接的\\
　「何が正解か」ではなく，時々報酬の形で与えられる
\item 報酬が遅れて与えられる\\
　例)将棋の勝利，迷路のゴール
\item 探求が可能\\
　エージェントが自分で学習データの分布を変えられる
\item 状態が確定的でない場合がある\\
　確率分布でそれぞれの状態にいる確率を表す
\end{itemize}

%ID:1503

このような設定で，最も単純な例から始めましょう．対象とするものは
K-armed banditと呼ばれる，$K$本のアームを持つスロットマシンです（図15.3）
．

$K$本のアームのうち，どのアームを引くかによって賞金が変わるものとします．
これは，1状態，$K$種の行為，即時報酬の問題となります．学習結果は，この
スロットマシン（すなわちこの唯一の状態）で，最大の報酬を得る行為
（$K$本のうちどのアームを引くか）になります．

もし，報酬が決定的であれば，学習は非常に簡単です．
全ての行為を順に試みて最も報酬の高い行為を選べばよいのです．
あまりにも単純ですが，今後のことを考えて学習過程を定式化しておきましょう．

行為$a$の価値を$Q(a)$と定義し，学習過程によって正しい$Q(a)$の値(以後Q値といいます)が得られれば，
Q値を最大とする行為が学習の結果になります．
最初は，行為$a$を行ってどれだけの報酬が得られるのかわからないので，全ての$a$について$Q(a)$の値を0に
初期化します．次に，可能な$a$を順番に行って($K$本のアームを順番に引いて)，
そのときの報酬$r_a$を得ます．そして，各$a$について$Q(a)= r_a$として，
Q値がいちばん高い$a$が求める行為になります．


一方，報酬が非決定的な場合は，こんなに簡単にはゆきません．各行為$a$に対応する報酬
$r$は，非決定的ですがまったくでたらめではなく，確率分布$p(r|a)$に従うと仮定します
．
つまり，決定的ではないが，確率的であると仮定します．ただし，この確率分布は未知だとします．

そのような状況では，各アームを1回だけ引くのではなく，何度も引いて，平均的に多くの報酬が得られるアーム
を選ぶことになります．何回も試行することで，確率分布$p(r|a)$を推定するわけです．

何度も試行して学習を行うので，定式化に時刻$t$を持ち込みます．扱いやすいように，$t$は離散的であるとして，
時刻$t$で一回試行，時刻$t+1$で次の試行と続けてゆくと考えます．この場合，行為$a$の価値の時刻$t$における
見積りを$Q_t(a)$とします．このQ値を時刻$t$以前での，行為$a$による報酬の平均値に一致させることを目指します．
そうすると，その行為が平均的にどれぐらいうまくゆくか，ということがわかります．

ただし，単純に平均値を求めるためには，それまでの行為$a$の試行回数を記憶しておかなければなりませんし，
Q値はずっと変動し続けます．そこで，式(15.1)のように，時刻$t$の行為$a$による試行の
報酬$r_{t+1}$と，現在の
Q値との差を変動幅とし，学習係数$\eta$をかけてQ値の更新を行います．学習率$\eta$は最初は1以下の適当な値に
設定し，時刻$t$の増加に従って減少するようにしておけば，試行を重ねてゆけばQ値が収束します．

%ID:1505

次に，複数の状態を持つ問題に拡張しましょう．図15.4のような迷路をロボットRが
移動するという状況です．ゴールGに着けば，報酬が得られます．

単純なケースでは報酬は決定的（ゴールに着けば必ずもらえる）で，部屋の移動にあたる状態遷移も
決定的（必ず意図した部屋に移動できる）です．問題を一般化して，報酬や遷移が確率的である場合も
想定できます．これらが確率的になる原因として，例えばロボットのゴールを探知するセンサーが
ノイズで誤動作をしたり，路面状況でスリップが生じるなどの不確定な要因で行為が成功しない状況が
考えられます．これらは，非決定的であるとはいえ，学習中に状況が変化してしまうとどうしようもないので，
この非決定性が確率的であるとし，確率分布は学習期間中を通じて一定であるとします．

このような問題は，以下のようなマルコフ決定過程として定式化
することができます．

\begin{itemize}
\item 時刻$t$における状態$s_t \in S$
\item 時刻$t$における行為$a_t \in A(s_t)$
\item 報酬 $r_{t+1} \in R$，確率分布$p(r_{t+1}|s_t, a_t)$
\item 次状態$s_{t+1} \in S$，確率分布$P(s_{t+1}|s_t, a_t)$
\end{itemize}

マルコフ決定過程は，「マルコフ性」を持つ確率過程における意思決定問題で，「マルコフ性」とは
次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質です．
ここでは，報酬と次状態への遷移の確率が現在の状態と行為のみに依存しているという定式化に
なっています．

%ID:1506

マルコフ決定過程における学習は，各状態でどの行為を取ればよいのかという意思決定規則を
獲得してゆくプロセスです．意思決定規則のことを政策$\pi$と呼び，状態から行為への関数の形で表現
します．

政策の良さは，その政策に従って行動したときの累積報酬の期待値で評価します．
状態$s_t$から政策$\pi$に従って行動したときに得られる累積報酬の期待値は式(15.2)のように計算できます．
ただし，$\gamma (0 \le \gamma < 1)$は割引率で，後に得られる報酬ほど割り引いて計算するための係数です．この係数を
累積計算に組み込むことで，同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を
小さくすることで累積計算を収束させることにもなります．

累積報酬の期待値を全ての状態に対して最大となる政策を最適政策$\pi^*$といいます．
マルコフ決定過程における学習の目標は，この最適政策$\pi^*$を獲得することです．

%ID:1507

最適政策$\pi^*$に従ったときの累積報酬の期待値$V^{\pi^*} (s_t)$は，見やすさのため，以後$V^* (s_t)$と
表記します．

この最適政策を求めるための考え方は，K-armed bandit問題と同じで，各状態において
$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って
求めてゆく，というものです．

$Q^*(s_t, a_t)$を状態$s_t$ で行為$a_t$ を行い，その後最適政策に従ったときの期待累積報酬の
見積もりとすると，$V^*$と$Q^*$の関係から，以下の式が導けます．

%ID:1508

式(15.4)は，無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したものです．
この再帰方程式を
ベルマン方程式 
(Bellman equation)といいます．状態遷移確率を明示的にすると，
ベルマン方程式は以下のように書き換えられます．

さらに，式(15.5)を，Q値を用いて書き換えると，以下のようになります．

求めるべき最適政策は，Q値を用いて，以下のように表現できます．

後は，どのようにしてQ値を推定するか，という問題になります．

%ID:1509

Q値を推定する方法は，モデルの関する知識の前提によって大きく2つに分類されます．
環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合，
Q値は，動的計画法の考え方を用いて求めることができます．この方法を
モデルベースの手法 と
呼びます．一方，環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が
未知の場合，試行錯誤を通じて環境と相互作用をした結果を使って学習を行います．この方法を
モデルフリーの手法 と呼びます．

本節ではモデルベースの手法を，次節ではモデルフリーの手法を説明します．

モデルベースの手法では，状態遷移確率$P(s_{t+1}|s_t, a_t)$と，報酬の確率分布$p(r_{t+1}|s_t, a_t)$が
与えられているものとします．その前提で，アルゴリズム15.1に示すValue iterationアルゴリズムを実行すると，
状態価値関数$V(s)$の最適値を求めることができ，それぞれの状態でQ値を最大とする行為が求まりますので，
これが最適政策ということになります．

アルゴリズム15.1 中の報酬の期待値$E(r|s,a)$は報酬の確率分布$p(r_{t+1}|s_t, a_t)$から求めます．
このアルゴリズムは，迷路中で報酬がもらえる状態（ゴール）が1つだけある場合，まずそのゴール
状態の1つ手前での最適行為が得られ，次にその1つ手前，さらにその1つ手前と，繰り返しを回る毎に
正しい最適値が得られている状態がゴールを中心に広がってゆくイメージをしていただけると，
わかりやすいと思います．

また，モデルベースの手法には，Value iterationアルゴリズムの他にも，
適当な政策を初期値として，そのもとでの状態価値関数$V(s)$を計算し，各状態で現在の
知識から得られる最適行為を選び直すことを繰り返すPolicy iterationアルゴリズムもあります．

%ID:1510

環境モデルが未知の場合，TD(Temporal Difference)学習と呼ばれる方法を使います．

環境の探索が必要なので，探索戦略としてε-greedy法を使います．
ε-greedy法は確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称で，
実際にはQ値を確率に変換したものを基準に行為を選択します．

ただし，探索の初期はいろいろな行為を試し，落ち着いてくると最適な行為を多く選ぶように
するように，温度の概念を導入します．温度を$T$として，式(15.8)で表される確率に従って
行為を選びます．

$T$を
アニーリング(焼き鈍し)
における温度と呼び，高ければすべての行為を等確率に近い確率で選択し，
低ければ最適なものに偏ります．学習が進むにつれて，$T$の値を小さくすることで，学習結果が安定します．

%ID:1511

まず，報酬と遷移は，未知ではあるが決定的に定まる，という状況でのTD学習を考えます．
このような状況の例としては図15.5のような迷路での最適行為の獲得を考えます．

この場合のベルマン方程式は式(15.9)のようになります．

%ID:1512

このベルマン方程式を用いて，以下のアルゴリズムでQ値が求まります．

%ID:なし

たとえば，図15.6左のように状態$s_1$にロボットがいて，
この時点のQ値が図15.6左に示す通りであったとします．

次に，上記のアルゴリズムに基づき，右に移動するという行為$a_{right}$
をとると，報酬は0で，状態が$s_2$になります(図15.6右)．
このときのQ値は以下のように更新されます．

 

これを可能な全ての遷移系列について繰り返せば，ゴールの
報酬が末端にまで伝播して，全ての状態での最適行動が求まります．

%ID:1513

次に報酬と遷移が非決定的なTD学習を考えます．

この場合，報酬$r$が確率的であるので，決定性のアルゴリズムでは値が変化し続けることになります．
そこで，1状態・非決定性の問題で検討したように，Q値の更新を現在のQ値に一定割合の更新分の加え，
その一定割合を時間とともに減らしてゆく，という更新式を用います．式は以下のようになります．

式(15.10)の学習係数$\eta$を適切に設定すると，各状態ですべての行為を十分な回数行える
という前提で，Q値が収束することが証明されています．しかし，これはあくまで理論上で，
実際にロボットを動かして強化学習を行わせるようなケースは少なく，パラメータを変えて
シミュレーション結果を評価する事例が多く見られます．

%ID:1514

MDP設定下での強化学習では，エージェントは行為後の次状態を環境から受け取るという仮定を置いています．
つまり，エージェントは今，どの状態にいるのか，確定した情報を持っているということです．

しかし，現実世界でロボットを動かした場合，ロボットの入力はカメラやセンサから得る値ですので，
これらの情報から確実に状態を特定できるとは限りません．エージェントは観測された情報(部分的な状態の情報)
を受け取る，という設定の方が現実的です．つまり，エージェントは現在の状態が確定できない
状況で，意思決定を行うということになります（図15.7）．

このような状況は，部分観測マルコフ決定過程(POMDP; Partially Observable MDP)による定式化が適しています．

部分観測マルコフ決定過程の要素は以下のものです．

\begin{itemize}
\item 状態$s_t$で行為$a_t$を行うと観測$o_{t+1}$が確率的に得られる
\item エージェントは状態の確率分布を信念状態$b_t$として持つ
\item エージェントは，信念状態$b_t$，行為$a_t$，観測$o_{t+1}$から次の信念状態$b_{t+1}$を推定する
状態見積器(state estimator)を内部に持つ
\end{itemize}

%ID:なし

このように定式化すると，POMDPにおけるQ値の計算は以下のようになります．

このように定式化ができた後，Q値を求める方法は，基本的にはMDPと同じです．
正確なモデルが与えられた場合でも，十分に計算量が多くなるので，近似解を求めるアルゴリズムが存在
します．一方，モデルが与えられない場合，すなわち，エージェントが事前に環境に関する知識を持っていない場合は，
強化学習を用いても厳密解を求めるのはほとんど不可能です．状態を分割するなど，問題設定に制限を
加えた近似解法がいくつか提案されています．

POMDPは，音声対話システムにおける対話管理（音声の誤認識の可能性を考慮すると，現在の対話の状態が
確信できない問題）などに適用され，いくつかの成功事例が報告されています．

%ID:なし

本章では強化学習について説明しました．基本的な強化学習の設定はマルコフ決定過程に基づいて行いますが，
部分観測マルコフ決定過程の方が，より現実に近い設定になります．しかし，状態数が増えるとどちらも
単純な方法では学習が難しく，状態数をまとめるなど，問題設定そのものの工夫が必要になることがあります．

MDPのツールとしては，
MDPtoolbox
がMATLAB, GNU Octave, Scilab, Rのライブラリとして実装されています．

強化学習の入門的解説は，文献\cite{mitchell97}の13章，文献\cite{alpaydin10}の18章がよいでしょう．
