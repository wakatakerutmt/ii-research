--------- はじめに　記号の意味と注意（以下Xは数字を表します） -------------
%ID:XXXX(4桁の数字) ・・・IDがXXXXのスライドに対応したテキスト　文章が途中で分かれていて（2つ目）などと表記があってもIDはそのままです。

%ID:なし　　　　     ・・・スライドに直接関係のない、繋ぎや補足の説明。この「ID:なし」の部分から解答を取ってこないようにしてください。
% figure X.X        ・・・図番号。本来ならここに図が入ります、という意味だけで無視して大丈夫です。解答に含めないようにお願いします。
% equation X.X などその他％から始まる記号 ・・・上記同様、解答に含めないようにお願いします。
$\theta$ など       ・・・数式、引用などのTex記号も無視で、解答に含めないようにお願いします。
\cite{mitchell97}

---------　以下からスライドに対応したテキストデータ ------------------

%ID:0101

我々人間は，日々五感を通して得られる情報から，対象を分類する能力や，事象の背後にある規則性を獲得してゆきます．これらと同じ能力，あるいは人間を超える能力をコンピュータに与えることを目指したものが，機械学習です．本章では，近年機械学習が注目されている背景を紹介し，機械学習とは何かを明確にした後，本書で扱う機械学習の概要について説明します．

%ID:0102

近年，日常生活やビジネスにおけるさまざまな場面で
人工知能 (aritificial intelligence)
を活用した製品やサービスの開発が注目されています．人工知能は，人と対話を行うアプリやロボット・自動運転・病気の診断の補助・高度な生産システムなどの中心的技術として位置づけられています．人工知能はさまざまな立場から異なった定義がされていますが，本書では人工知能を「現在，人が行っている知的な判断を代わりに行う技術」と定義します．

このように定義すると，探索・知識表現・推論などの技術とともに，データから規則性を導く
機械学習 (machine learning)
も「人が行っている知的な判断を代わりに行う」技術を実現するための，ひとつの方法ということになります（図1.1）．

この定義のもとでは，「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます．日常生活で我々が便利だと感じている技術の大半は，人間が作成したプログラムで動いています．また，機械可読な web 情報源の構築を目指した LOD (linked open data) の取り組みや，システムが結論を出した過程をわかりやすく人に説明するための推論・プランニングの技術は，直接的には機械学習とは関係がなくとも，知的なシステムを作成するために重要な人工知能の要素技術です．

%ID:0103

機械学習の出番は，簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるときです．このような大量のデータは，
ビッグデータ (big data)
とよばれます．
ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです．この大量・多様なデータから規則性を抽出したり，データを分類するモデルを獲得することで，購買記録からのお勧め商品提示のようなおなじみの機能に加えて，不審者の行動パターンの検出や，インフルエンザの流行の予想など，これまでになかったサービスや機能を実現することもできます（図1.2）．

%ID:0104

データから規則や知見を得る機械学習技術のなかでも，特に
深層学習 (deep learning)
は，高い性能を実現する方法として近年注目を集めています．
深層学習は，一般に隠れ層を多くもつニューラルネットワーク（図1.3）によって実装されています．

深層学習が他の機械学習手法と異なるのは，深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところでです．近年の深層学習の流行を見ると，他の機械学習技術はもう不要に見えるかもしれません．しかし，深層学習がその強さを発揮しているのは，音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題であるという傾向があります．さまざまな問題に対して機械学習アルゴリズムの性能を競うサイトでは，深層学習と並んで勾配ブースティングなどの手法が上位を占めることがあります．また一方で，性能は多少低くてもよいので判定結果に至るプロセスがわかりやすい手法や，運用後のチューニングが容易な手法が好まれる場合もあり，さまざまな状況でさまざまな問題に取り組むためには，深層学習だけではなく機械学習手法全般に関して理解しておくことが必要であるといえます．本書では機械学習全般に関して，設定した問題に対する基本的な手法の概要と，フリーソフトを用いた例題の解法について説明します．

%ID:0105

ここでは，もう少し詳細に機械学習の中身をみてゆきましょう．機械学習で扱うのは，人手では規則を記述することが難しい問題です．すなわち，解法が明確にはわかっていない問題であると言い換えることができます．ここでは，機械学習で対象とする問題を
タスク (task)
とよびます．

たとえば，人間は文字や音声の認識能力を学習によって身につけますが，どうやって認識をおこなっているかを説明することはできません．人間の認識能力を何らかの手法で
モデル (model)
化して，コンピュータでその能力を再現しようとする技術が，機械学習の一種である
パターン認識 (pattern recognition) 
です．

ここでのアイディアは，明示的にその手順は記述できないけれども，データ（この場合は入力とその答え）は大量に用意することができるので，そのデータを使って人間の知的活動（場合によってはそれを超えるもの）のモデルを作成しようというものです．これ以降，機械学習のために用いるデータを
学習データ (training data)
とよびます．

ここまでをまとめると，機械学習の基本的な定義は，

アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること

となります．

%ID:なし

また，文献\cite{mitchell97}では，コンピュータプログラムが学習するとは
\begin{itemize}
\item あるタスクにおいて
\item ある学習データによって
\item モデルの性能測定基準の値が向上すること
\end{itemize}
であるとされています．単にモデルを構築するだけではなく，その性能測定基準の値が向上するということを、機械学習の定義として定めています．

単に知的なモデルを作って「それで終わり」というわけではなく，学習を続けて性能が向上しつづけることが，定義の中に含まれています．例として，迷惑メールフィルタを考えます．製品によって相対的に性能の良し悪しはありますが，誰にとっても最高の性能であるような迷惑メールフィルタはありません．各人の利用環境によって迷惑メールの基準が異なるので，利用者が「これは迷惑メール」，「これは迷惑メールではない」と操作した履歴に基づいて再学習をおこない，その利用者の操作の結果が適切に分類に反映されれば，このプログラムは学習によってユーザの要求に適応しているといえるでしょう．

これらの定義に沿って考えてゆくと，機械学習のタスクとしては，認識・予測・適応などを伴う，人間の知的活動の様々な分野がその対象となりそうです．これらは人間の高度な知的活動なので実現するのは非常に難しそうですが，これらの機能を規則や関数などでモデル化して，それらを性能測定基準に沿って最適化してゆくプロセスを学習とみなすことで，適切な理論化が行えます．

%ID:0105

また，学習データも一見多様に見えますが，金額やセンサーからの入力のような「
数値データ
」，あるいは商品名や性別のような「
カテゴリカルデータ
」が並んだものであるとすると，数値データの並びからなるデータ，カテゴリカルデータの並びからなるデータ，それらが混合したデータというように整理して考えることができます（図1.4）．
観測対象から問題設定に適した情報を選んでデータ化する処理は，
特徴抽出
とよばれます．

機械学習の役割をこのように位置付けると，図1.4中の「機械学習」としてまとめられた中身は，タスクの多様性によらず，目的とする規則・関数などのモデルを得るために，どのような学習データに対して，どのようなアルゴリズムを適用すればよいか，ということを決める学習問題と，その学習の結果得られたモデルを，新たに得られる入力に対して適用する実運用段階に分割して考えることができます
（図1.5）．

本書の対象は，主として図1.5の学習問題と定義された部分ですが，いかに実運用の際によい性能を出すか，すなわち，学習段階ではみたことのない入力に対して，いかによい結果を出力するかということを常に考えることになります．この能力は，「学習データからいかに一般化されたモデルが獲得されているか」ということになるので
汎化 (generalization) 
能力といいます．

%ID:0109

ここでは，前節で説明した学習データと出力を基準に機械学習の分類を試みます．
機械学習にはさまざまなアルゴリズムがあり，その分類に関してもさまざまな視点があります．
機械学習の入門的な文献では，モデルの種類に基づく分類が行われていることが多いのですが，そもそもそのモデルがどのようなものかというイメージをもっていない初学者には，なかなか納得しにくい分類にみえてしまいます．そこで本書では，入力である学習データの種類と出力の種類の組合せで機械学習のタスクの分類をおこない，それぞれに分類されたタスクを解決する手法としてモデルを紹介します．

まず，学習データにおいて，正解（各データに対してこういう結果を出力して欲しいという情報）が付いているか，いないかで大きく分類します（図1.6）．学習データに正解が付いている場合の学習を
教師あり学習  (supervised learning) 
，正解が付いていない場合の学習を
教師なし学習  (unsupervised learning)
とよびます．また，少し曖昧な定義ですが，それらのいずれにも当てはまらない手法を
中間的学習 
とよぶことにします．

教師あり／なしの学習については，それぞれの出力の内容に基づいてさらに分類をおこないます．教師あり学習では，入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するものを回帰とします．
一方，教師なし学習では観点を変えて，入力となるデータ集合全体を説明する情報を出力するものをモデル推定とし，
入力となるデータ集合の一部から得られる特徴的な情報を出力するものをパターンマイニングとします．

中間的学習に関しては，何が「中間」であるのかに着目します．学習データが中間である場合（すなわち，正解付きの学習
データと正解なしの学習データが混在している場合）である半教師あり学習という設定と，正解が間接的・確率的に与えられるという意味で，教師あり／なしの中間的な強化学習という設定を取り上げます．

以下では，それぞれの分類について，その問題設定を説明します．

%ID:0110

教師あり学習では，正解の付いた学習データを用います．このデータを訓練例とよぶこともあります．学習データは，入力データに対応するベクトル$\bm{x}_i$と，正解情報$y_i$のペアからなります．

ここで，$N$は学習データの総数，添字$i$は学習データ中の$i$番目の事例であることを示します．

当面，入力ベクトル$\bm{x}_i$は次元数$d$の固定長ベクトルであると考えておきます．

図1.4の上部に示したような， (134.1, 34.6, 12.9) や， (女, 68, 165, 44, no) などが入力ベクトル$\bm{x}_i$の例です．

入力ベクトルの各要素 $x_{i1},\dots,x_{id}$ を，
特徴 (feature)
 あるいは
 属性 (attribute) 
 とよびます．
特徴は，数値データあるいはカテゴリカルデータのいずれかです．数値データは長さや温度などの連続値をとる場合もあれば，商品の購入個数や単語の出現回数などの離散値をとることもあります．また，カテゴリカルデータは一般に文字列として表記され，たとえば性別を表す「男・女」や天候を表す「晴・曇・雨」などのカテゴリを値とします．

教師あり学習は，この学習データから，入力$\bm{x}$を正解$y$に写像する関数$c$を学習することを目的とします．

ここで$\bm{x}$は，学習データ中の$\bm{x}_i$に限らず，今後この関数に入力され得るすべてのデータを表しているので，関数$c(\bm{x})$はあらゆる入力に対して正しい出力を与える理想的な写像ということになります．機械学習では，そのような理想的な写像を求める問題に対して，関数の形を扱いやすいものに仮定して，その関数のパラメータを学習データから推定するという問題に置き換えます．この推定する関数を
$\hat{c}(\bm{x})$と記述します．関数$\hat{c}(\bm{x})$の実際の形は，入力ベクトル$\bm{x}$と正解$y$の種類によって異なります．

また，正解情報（あるいは関数$\hat{c}(\bm{x})$の出力）$y$も，数値あるいはカテゴリのいずれかになります．正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします．

具体的な教師あり学習問題の説明に入る前に，性能測定基準について少し説明します．学習結果である関数$\hat{c}(\bm{x})$は，学習データに含まれていない未知のデータ$\bm{x}$に対してなるべく正しい答えを出力するように一般化されなければなりません．学習データに対する正解率ではなく，未知のデータに対する正解率が重要なのです．学習データに対しては，その正解をすべて表形式で記録しておけば，間違いなく正解を出力することができます．しかし，未知データに対して正解を出力するには，「学習データの背後にある法則のようなもの」を獲得する必要があるのです．機械学習は，人間が解き方のわからない問題に対して適用するものであることを，前節で説明しました．「学習データの背後にある法則のようなもの」をいかにして獲得するか，ということが教師あり学習のテーマになります．

%ID:0111

識別は，入力をあらかじめ定められたクラスに分類する問題です．典型的な識別問題には，音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定などがあります．

ここで，識別問題としてもっとも単純な，2値分類問題を考えてみましょう．2値分類問題とは，たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題です．さらに，入力を数値のみを要素とするベクトルと仮定します．入力ベクトルが2次元の場合，学習データは図1.7(a)に示すように，平面上の点集合になります．クラスの値に対応させて，それぞれ丸とバツで表しました．

最も単純に考えると，識別問題はこの平面上で二つのクラスを分ける境界線を決めるという問題になります．未知のデータが入力されたとき，この境界線のどちらにあるかを調べるだけで，属するクラスを解答できるので，このことによって，識別能力を身につけたとみなすことができます．

境界線として1本の直線を考えてみましょう．図1.7(b)に示すように，このデータでは切片や傾きをどのように調整しても1本の直線で二つのクラスをきれいに分離することはできません．一方，図1.7(c)のように複雑に直線を組み合わせると，すべての学習データに対して同じクラスに属するデータが境界線の片側に位置するようになり，きれいに分離できたことになります．しかしここで，「識別とは図1.7(c)のようなすべての学習データをきれいに分離する，複雑な境界線を探すことだ」という早とちりをしてはいけません．識別の目的は，学習データに対して100\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした．もう一度図1.7(a)に戻って，二つのクラスの塊をぼんやりとイメージしたとき，その塊を区切る線として図1.7(b)，(c)いずれがよいと思えるでしょうか．おそらく大半の人が(b)を支持するでしょう．これが我々人間が身につけている汎化能力で，そのようなことを学習結果に反映させるように，学習アルゴリズムを考える必要があります．

しかし，一般的な識別問題は，このような単純なものばかりではありません．識別結果が一つのクラスになるとは限らない場合があります．たとえば，受信した電子メールに対して，重要・緊急・予定・締切...などのタグを付与する自動タグ付けを識別問題に当てはめると，一つの入力に対して，複数の出力の可能性がある問題設定になります．また，どのクラスにも当てはまらない入力が入ってくる可能性もあります．たとえば，スキャンした文書に対して文字認識をおこなっているときに，文字コードにない記号が含まれている場合があるかもしれません．

本書で扱う識別は，これらの複数出力の可能性や，識別不可能な入力の問題を除外して，すべての入力に対してあらかじめ決められたクラスのうちの一つを出力とする，というように単純化します．識別の代表的な手法には決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなどがあります．これらを第3章から第8章で説明します．近年注目を集めている深層学習は，主としてこの識別問題に適用されて高い性能を実現しています．深層学習に関しては第9章で説明します．また，第10章では複数の識別器を組み合わせる手法を，第13章では系列データの識別手法を扱います．

%ID:0112

回帰は入力から予測される妥当な出力値を求める問題です．典型的な回帰問題には，消費電力の予測，中古車の価格算出，生産量計画などがあります．
回帰の単純な例として，入力を気温，出力をビールの売上高とした架空のデータ（図1.8(a)）を考えてみます．

未知データに対して妥当な出力値を求めるために，入力データがある関数に基づいてターゲットを出力していると考え，その関数を求める問題が回帰問題です．ただし，関数の形として1次関数，2次関数，3次関数だけでなく，三角関数や指数関数などとの組み合わせまで考えてもよいとなると手がつけられなくなるので，通常は関数の形を先に決めて，その係数を学習データから推定するという問題とみなします．

1次関数を仮定して，学習データとの誤差が最も少なくなるように係数を求めると，図1.8(b)に示すような直線が得られます．ほとんどの点がこの直線を外れているので，あまりよい近似とはいえないようにみえるかもしれません．一方，複雑な高次の関数を前提とすれば，図1.8(c)に示すように，すべての学習データを通る関数を求めることができます．このどちらを採用すべきかについて，回帰問題でも識別問題と同様の立場をとります．すなわち，未知データに対する出力として，どちらが妥当かということを考えます．気温の少しの変化に対して売上が大きく変わるところがある図1.8(c)の関数は，やはり不自然な回帰に見えます．この例のように入力と出力を2次元で眺めることができれば，その妥当性をある程度直観的に議論できますが，通常の場合，入力は多次元なので，直観に頼らずに学習結果の妥当性を吟味する方法を考えなければなりません．

回帰の代表的な手法には線形回帰，回帰木，モデル木などがあります．これらを第6章で説明します．

%ID:0113

教師なし学習では，学習に用いられるデータに正解情報が付いていません．

入力ベクトル$\bm{x}_i$の次元数に関しては，教師あり学習の場合と同様に，$d$次元の固定長ベクトルで，各要素は数値あるいはカテゴリのいずれかの値をとると考えておきます．

教師なし学習は，入力データに潜む規則性を学習することを目的とします．ここで着目すべき規則性としては，2通り考えられます．一つめは，入力データ全体を支配する規則性で，これを学習によって推定するの問題が
モデル推定 (model estimation)
です．もう一つは，入力データの部分集合内あるいはデータの部分集合間に成り立つ規則性で，通常は多数のデータの中に埋もれてみえにくくなっているものです．これを発見する問題が
パターンマイニング (pattern mining) 
です．

%ID:0114

モデル推定は，入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するものです．
図1.9にモデル推定の考え方を示します．

観測されたデータは，もともと何らかのクラスに属していたものが，揺らぎを伴って生成されたものと考えます．その逆のプロセスをたどることができれば，データを生成したもととなったと推定されるクラスを見つけることができます．発見されたクラスの性質は，そこから生成されたと推定されるデータを分析することでわかります．もしかしたら，その発見されたクラスは，誰も考えつかなかった性質をもつものかもしれません．

このように，入力データ集合から適切なまとまりを作ることでクラスを推定する手法を
クラスタリング (clustering) 
とよびます．顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用などが考えられています．

一方，もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法を
密度推定  (density estimation) 
とよびます．密度推定はクラスタリングを発展させたものとみることができますが，その応用はクラスタリングだけでなく，不完全データを対象としたモデル推定の問題や，異常なデータの検出など，いろいろな場面で用いられています．

クラスタリングの代表的な手法には，階層的クラスタリングや k-means 法があり，密度推定の手法としてはEMアルゴリズムがあります．これらを第11章で説明します．

%ID:0115

パターンマイニングは，データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法です．スーパーマーケットなどで同時に購入される商品の組み合わせを発見するバスケット分析が代表的な応用例です．図1.10にパターンマイニングの考え方を示します．

パターンマイニングの敵は膨大な計算量です．まさに，大量のデータの中から，貴重な知見をマイニング（＝発掘）する作業です．図1.10に示した例では，発見された規則の条件部も結論部も要素数が一つなので，すべての商品の組み合わせに対してその出現頻度を計算することは，それほど膨大な計算量にはみえません．しかし，一般的なパターンマイニングでは，条件部・結論部のいずれも要素の集合となります．それらのあらゆる組み合わせに対して，マイニングの対象となる大きなデータ集合から出現数を数えあげなければならないので，単純な方法では気の遠くなるような計算量になってしまいます．そこで効率よく頻出パターンを見つけ出す手法が必要になります．

パターンマイニングの代表的な手法としては Apriori アルゴリズムやその高速化版である FP-Growth があります．これらを第12章で説明します．

%ID:0116

ここでは，これまでに説明した教師あり学習／教師なし学習に当てはまらない問題について説明します．
学習データが教師あり／教師なしの混在となっているものが半教師あり学習です．また，与えられる正解が間接的で，教師あり／教師なしの中間的な状況となっているものが強化学習です．

%ID:0117

これまでに述べてきた機械学習の分類では，学習データすべてに対して正解が与えられているか，あるいはまったく与えられていないかのいずれかでした．その中間的な設定として，学習データの一部にだけ正解が与えられている場合が考えられます．

学習データに正解を与えるのは人間なので，正解付きのデータを作成するにはコスト（費用・時間）がかかります．一方，正解なしのデータならば，容易にかつ大量に入手可能であるという状況があります．たとえば，ある製品の評価をしているブログエントリーのPN判定をおこなう問題では，正解付きデータを1,000件作成するのはなかなか大変ですが，ブログエントリーそのものは，webクローラプログラムを使えば，自動的に何万件でも集まります．このような状況で，正解付きデータから得られた識別器の性能を，正解なしデータを使って向上させる問題を半教師あり学習 (semi-supervised learning) といいます．半教師あり学習は主として識別問題に対して用いられます．半教師あり学習の代表的な手法のアイディアを図1.11に示します．

図1.11左のように，全データの中で正解の付加されたデータを丸・バツで表し，正解のないデータを三角形で表します．最初は丸・バツが付いたデータだけから識別器を作り，たとえば，その中間あたりに境界直線を引いたものとします．これに従って三角形のデータを分類しますが，境界線近辺のデータはあまり信用せず，境界線から大きく離れたものを確信度が高いとみなして正解を付与します．今度は，これらの新しく正解を付与されたデータも加えて，再度識別境界を計算します．これを，新しい正解付きデータが増えなくなるまで繰り返します．

この学習法は，識別するべきクラスがうまくまとまっているようなデータや，識別結果によって有効な特徴が増えてゆくような，やや特殊なデータに対して適用するときにうまくゆきます．この手法を第14章で説明します．

%ID:0118

問題の性質によっては，間接的に正解が与えられる場合があります．図1.12のような迷路を抜けるロボットを学習させる場合を考えてみましょう．この場合，入力はロボットの持つセンサーからの情報で，これによって，ロボットはどの部屋にいるかがわかるものとします．出力はロボットの移動コマンド（この場合，上下左右いずれかへ進む）であるとします．もし，すべての部屋（すなわちすべての状態）において正解（各状態での最適な移動コマンド）が与えられれば，ロボットはスタートから回り道することなくゴールにたどり着けます．

このようなロボットに対して，ゴールに着いたときだけ報酬を与えるという方法で，ロボットに試行錯誤を繰り返させながら，最終的に各状態における最適な出力（この場合は移動コマンド）を獲得させる学習手法を強化学習 (reinforcement learning) とよびます．報酬を教師信号とみなすと，これは教師時々あり学習ということができます．すなわち，教師あり／なしの中間的な設定とみなすことができ，個々の決定に正解は与えられず，決定の連続に対して，後で形を変えた間接的な教師信号が与えられる，という難しい設定になります．強化学習に関しては第15章で説明します．

%ID:0119

この章では，近年注目されてきている人工知能・機械学習・深層学習の関係を説明し，その中心的な技術である機械学習について概要を示しました．また，教師あり／なしという基準から機械学習における様々な問題を分類し，それぞれの目的を説明しました．

%ID:なし

機械学習の分類に関しては，ここで説明した以外にも様々な観点のものがあります．文献\cite{flach12}では，
モデルを構成する原理から，幾何的・確率的・論理的モデルという観点と，それとは直交して，モデルの振るまいから，
グループ化・グレード化モデルという観点を挙げています．また，文献\cite{sugiyama09}，\cite{sugiyama13}では，採用する
確率モデルの推定法の違いから，生成モデル\cite{sugiyama09}と識別モデル\cite{sugiyama13}とという観点が立てられています．
体系的に機械学習を学ぶには，このようなモデルに基づく分類からスタートする方法もありますが，
本書では，まず全体像をできるだけ平易に俯瞰することを目指したので，問題設定に基づく分類をおこないました．

%ID:なし　（元は0119でダブり）

以後，本書ではここで示した分類に基づいて，順に機械学習技術を紹介してゆきます．
識別・回帰など個別の問題に対して，それらを解決する機械学習アルゴリズムを示してゆきますが，それぞれはそこで取り上げられた問題専用のアルゴリズムではありません．カテゴリカルデータを入力とする識別問題を解決するアルゴリズムが，数値入力の識別問題にも適用できたり，識別問題のアルゴリズムを回帰問題に適用することなども可能です．それらの方法を逐一解説していては非常に読みにくくなるので，いくつかの重要な手法を除いては，その章で示した問題を解決する手順としてアルゴリズムを考えてゆきます．

それでは，機械学習を楽しむ旅に出かけましょう．

