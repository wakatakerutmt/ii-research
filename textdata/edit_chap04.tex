%chap04/1

この章では前章に引き続き，教師あり学習における識別問題で，特徴ベクトルの要素がすべてカテゴリの場合を扱います．前章と異なるのは，識別に統計的手法を用いることによって，結果に確信度を付与することができる点です．

%chap04/2

第3章の決定木は，ある事例がある概念に，当てはまるか否かだけを答えるものでした．しかし，たとえば病気の診断のように，その答えがどれだけ確からしいか，ということを知りたい場合も多くあります．学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法が，統計的識別手法です．

本節と次節では，第3章でも取り上げたweather.nominalデータを例に，統計的識別の考え方を説明します．

%chap04/4

突然ですが，現在の気象に関する情報が何も知らされていない状況で，weather.nominalデータだけが与えられて，今日この人がゴルフをするかどうかと尋ねられたらどう答えますか．weather.nominalデータを眺めると，全14事例のうちyesが9事例，noが5事例です．したがって，yesと答えた方が正解する確率が高そうです．この場合はあまり確信を持ってyesと答えられるとはいえませんが，プロゴルファーのような人の1年分のデータが与えられて，yesが360事例，noが5事例だったら，躊躇なくyesと答える人が多いでしょう．

この判断は，それぞれのクラスの起こりやすさの確率に基づいたものです，この入力を観測する前にもっているそれぞれのクラスの起こりやすさを，
事前確率 (prior probability) 
とよびます．
クラス$\omega_i$の事前確率は$P(\omega_i) ~~ (i=1,\dots,c)$ （ただし$c$はクラス数）と表します
．

%chap04/2

次に，入力$\bm{x}$が観測されたとします．この観測によって，事前確率からのみ判断した結果とは異なる結果が導き出される場合があります．たとえば，$\bm{x}$が悪天候を示唆しているならば，ゴルフをする確率は下がると考えられます．入力$\bm{x}$が観測されたときに，結果がクラス$\omega_i$である確率を，条件付き確率 $P(\omega_i \vert \bm{x})$ で表現します．
この確率は，入力を観測した後で計算される確率なので，
事後確率 (posterior probability) 
とよびます．

統計的識別手法の代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法で，この決定規則を
最大事後確率則(maximum a posteriori probability rule)
とよびます．
式(4.1)は，事後確率最大のクラス$C_{\mbox{\scriptsize{MAP}}}$を求める式です．

% equation 4.1

それでは，この事後確率の値を学習データから求める方法を考えてゆきましょう．一般的な条件付き確率の値は，条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます．weather.nominalデータの場合では，たとえば (sunny, hot, high, FALSE) という特徴をとるデータを大量に集めてきて，その中でyesが何回，noが何回出現したという頻度を得て，その頻度から条件付き確率値を推定します．そして，この推定をあらゆる特徴値の組合せについておこないます．しかし，weather.nominalデータをみると，特徴値の組み合わせのうちのいくつかが1回ずつ出てきているだけで，可能な特徴値の組み合わせの大半は表の中に出てきません．これでは条件付き確率の推定はできません．

そこで，この事後確率の値を直接求めるのではなく，式(4.2)に示すベイズの定理
を使って，より求めやすい確率値から計算します．

% equation 4.2

式(4.1)にベイズの定理を適用すると，式(4.3)が得られます．

% align 4.3

ここで，式(4.3)の右辺の分母は，クラス番号$i$を変化させても一定なので，右辺全体が最大となるクラス番号$i$を求める際には，その値を考慮する必要がありません．したがって，事後確率を最大とするクラス番号$i$を求める式は，式(4.4)のようになります．

% equation 4.4

%chap04/5

式(4.4)右辺第1項の条件付き確率$P(\bm{x} \vert \omega_i)$
を{ゆう|ど} (likelihood) 
とよびます．あるクラス$\omega_i$から特徴ベクトル$\bm{x}$が出現する\ruby{尤}{もっと}もらしさを表します．
結論として，事後確率が最大となるクラスは，尤度と事前確率の積を最大とするクラスを求めることによって得られます．

%chap04/0

例題4.1において，箱がクラス，玉が特徴ベクトルだとすると，問題(2)は，各クラスの事前確率が等しい場合に，
ある特徴ベクトルを観測したときに，どちらのクラスのものであった確率が高いか（つまり，どちらの箱の事後確率が高いか）を求める問題とみなすことが
できます．この場合は，箱Bの方の確率が高くなっており，図4.1をみせられて，「白玉が出たけど
どっちの箱から取ったと思う？」という質問に対する答えとして，妥当なものになっていることがわかります．また，
計算結果から，箱Bから取った確率が$16/21$であるということがいえます．答えに対してどれだけ確からしいか
という情報を添えることができるわけです．

また，問題(3)は，各クラスに異なる事前確率が与えられたもとで，ある特徴ベクトルを観測したときに，どちら
のクラスのものであった確率が高いかを求める問題，つまり通常の統計的識別の問題と同じということになります．
今度は箱Aが9個，箱Bが1個あるという状況で，「白玉が出たけど，AとBのどちらの種類の箱から取ったと思う？」と聞かれた
状況になります．即座には答えにくいかもしれませんが，事後確率を計算すると，箱Aから取った方の確率が
高いという結論になります．この場合も同様に，答えがどちら，というだけではなく，その確信度も得られています．


例題4.1の設問(1)や(3)では，それぞれの箱を選ぶ確率が与えられていました．また，設問(2)
では，箱の中身，すなわち尤度がわかっていたので，それらに基づいて事後確率を計算することができました．

%chap04/6

しかし，一般の機械学習の問題では，どのクラスが出やすいかという事前確率や，各クラスから生じる特徴の尤もらしさを表す
尤度はわかりません．

そこで，この事前確率や尤度を計算する確率モデルを仮定し，そのパラメータを学習データに最も合うように調整することを考えます．
それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に
生成されたものと仮定します．この仮定を
i.i.d. (independent and identically distributed) 
と表記します．学習データ全体$D$が
生成される確率$P(D)$は，個々の事例$\{\bm{x}_1,\dots,\bm{x}_N\}$の独立性、すなわち i.i.d. を仮定すると，式(4.5)のように，個々の事例が生成される確率の積で求めることができます．

% equation 4.5

$P$は，データの生成確率を何らかのパラメータに基づいて計算するモデルです．ある程度複雑なモデルでは，パラメータが複数あることが一般的なので，これらのパラメータをまとめて$\bm{\theta}$と表記して明示すると，式(4.5)は式(4.6)のように書けます．

% equation 4.6

%chap04/7

こちらは，モデルのパラメータが与えられたときの，学習データ全体が生成される尤度を表しています．
ここで，確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので，式(4.6)の対数をとって計算します．式(4.7)で計算される値を対数尤度$\mathcal{L}(D)$とよびます．

% equation 4.7

この対数尤度の値は，大きければ大きいほど学習データがそのモデルから生成された確率が高い、ということがいえます．そして，学習データが，真のモデルから偏りなく生成されたものであると仮定すると，この方法で求めたモデルは真の分布に近い，と考えることができます．
したがって，式(4.7)を最大にするパラメータが求まればよいわけです．

%chap04/8

ここで，特徴ベクトルが1次元で，値として2値$x \in \{0,1\}$を取り，その出現がベルヌーイ分布
に従うと仮定します．そうすると，$P(x|\theta)$は，確率$\theta$をパラメータとして，式(4.8)のように書くことが
できます．

% equation 4.8

式(4.8)を使うと，対数尤度は，式(4.9)のように書くことが
できます．

% align 4.9

%chap04/9

ここで，対数尤度$\mathcal{L}(D)$を最大にするパラメータ$\hat{\theta}$は，$d \mathcal{L}(D) / d\theta = 0$の解として
式(4.10)のように求めることができます．

% equation 4.10

式(4.10)右辺の分子は値1をとる事例数，分母は全事例数です．このように，推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．
この推定法を
最尤推定法 (maximum likelihood estimation) 
とよびます．

%chap04/10

それでは，この最尤推定法を使って，式(4.4)の尤度を具体的に求める方法をみてゆきましょう．

多次元ベクトル$\bm{x}$を要素に分けて表記すると，式(4.11)のようになります．

% equation 4.11

尤度$ P(x_{1}, \dots, x_d \vert \omega_i)$を統計で求めるためには，学習データ中からクラス$\omega_i$に属するデータを取り出し，そのデータに対してすべての特徴値の組み合わせが，それぞれ何回起こっているかをカウントすることになります．式(4.1)のところでの考察に比べると，条件部にあてはまるデータがない，ということはないので少しはましですが，結論部にあてはまるデータが，統計をとれるほとに十分に揃っているということはなかなか望めそうにありません．

そこで，各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法を
ナイーブベイズ識別法 (naive Bayes classifier)
または単純ベイズ識別法とよびます．識別の結果を$C_{\mbox{NB}}$とすると，ナイーブベイズ識別法は式(4.12)のように定義できます．

% equation 4.12

この$P(x_{j} \vert \omega_i)$であれば，クラス$\omega_i$のデータの中で，特徴値$x_{j}$をとるデータの頻度を数えることで確率を推定する，最尤推定をおこなうことで求めることができます．

%chap04/11

しかし，このように少ないデータでも学習が行えるように尤度計算の方法を単純にしても，学習データが少ないがゆえに生じる問題がまだあります．

$n_i$を「学習データ中で，クラス$\omega_i$に属するデータ数」，$n_{j}$を「クラス$\omega_i$のデータ中で，ある特徴が値$x_j$をとるデータ数」としたとき，ナイーブベイズ識別に用いる尤度は，式(4.13)で最尤推定されます．

% equation 4.13

ここで$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になるという
ゼロ頻度問題
が生じます．
たとえば，表3.3に示したweather.nominalデータでは， play=no のクラスで，outlook=overcast を特徴とする事例がありません．

このようなゼロ頻度問題へ対処するには，確率の
m推定
という考え方を用います．
これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．
各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます．このときの$m$を，等価標本サイズとよびます．

% equation 4.14

この工夫によって，$n_j = 0$のときでも，式(4.14)の右辺の値が0にならず，ゼロ頻度問題が回避できることになります．

%chap04/0

例題4.2で得られたナイーブベイス識別器を用いて，weather.nominalデータの最初の事例$\bm{x}_1$=(sunny, hot, high, FALSE)を識別してみましょう．

% align*

% align*

したがって，求める事後確率$P(\mbox{Class}|\bm{x}_1)$は，以下のようになります．

% align*

% equation*

これより，事例1は71\%の確率で，noに分類されるという結果が得られました．

%chap04/12

ナイーブベイズ識別器の「すべての特徴が，あるクラスのもとで独立」であるという仮定は，一般的には成り立ちません．
だからといって，必ずしもすべての特徴が依存し合っているということでもありません．あいだをとって，「特徴の部分集合が，あるクラスのもとで独立である」と仮定することが現実的です．このような仮定を表現したものが，ベイジアンネットワークです．

ベイジアンネットワークとは，確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデルです．依存関係は，アークに付随する条件付き確率表で定量的に表現されます．

ベイジアンネットワークでは，確率変数間に条件付き独立の仮定を設けます．この仮定は，

確率変数（ノード）の値は，その親（アークの元）となる確率変数の値以外のものには影響を受けない

というものです．数式で表すと，確率変数の値$\{z_1,\dots,z_n\}$の結合確率は，以下のように計算されます．

% equation 4.15

ただし$\mbox{Parents}(Z_i)$は，値$z_i$をとる確率変数を表すノードの親ノードの値です．親ノードは複数になる場合もあります．

%chap04/0

それでは，ベイジアンネットワークの例をみてゆきましょう．
計算を簡単にするために，すべての確率変数は2値（true または false）をとるものとします．図4.2は二つの確率変数Rain（trueなら「雨が降った」）と，Wet grass（trueなら「芝生が濡れている」）の関係を示しています．以後，式や図の中では確率変数Rainの値がtrueであることを$R$, falseであることを$\lnot R$と，変数名の頭文字と論理否定の記号（$\lnot$ (not)）を使って表現します．ほかの確率変数についても同様です．「芝生が濡れている」確率は，「雨が降った」かどうかに左右されると考えられるので，図4.2の関係が導かれます．ベイジアンネットワークでは，親ノードを持たないノードには事前確率が与えられます．この場合のRainの事前確率$P(R)$を
0.4とします．また，アークには，矢印のもとの確率変数を条件部，矢印の先の確率変数を結論部とする条件付き確率が，それらの変数のすべての組合せについて与えられます．

% figure 4.2

%chap04/0

この確率モデルに従うと，何も情報がない状態で，「芝生が濡れている」確率は以下のようになります．

% align*

また，「芝生が濡れている」ことが観測されたもとで「雨が降った」確率は，以下のようになります．

% align*

これは，何も情報がない状態での「雨が降った」確率（すなわち事前確率）の0.4が，「芝生が濡れている」という情報を得たことで，
その確率が0.75に変化したことを示します．

%chap04/13

上記の例は，普通の条件付き確率をベイジアンネットワークで表現したものです．しかし，ベイジアンネットワーク
の利点は，変数間の独立性を表現できることです．以下では，独立性を表現する基本パターンと，それぞれの
確率計算の例を示します．

最初のパターンはHead-to-tail connectionで，これは三つのノードが直線上に並んだものです．
図4.6に，「曇っている」(Cloudy)，「雨が降った」(Rain)，「芝生が濡れている」(Wet grass)
がHead-to-tail connectionでつながっている例を示します．

% figure 4.6

これは，真ん中のノードの値が与えられると，左のノードと右のノードが独立になるパターンです．
もし，Rainの値が定まっていれば，Wet grassの値はCloudyの値とは無関係に，RainからのWet grassへのアークに付随している
条件付き確率表のみから定まります．一方，Rainの値がわからないときは，Rainの値はCloudyの値に影響され，
Wet grassの値はRainの値に影響されるので，CloudyとWet grassは独立ではありません．

何も情報がない状態での「芝生が濡れている」確率は以下のようになります．
まず，「曇っている」の事前確率$P(C)$を使って「雨が降った」確率$P(R)$を求め，それを使って
「芝生が濡れている」確率$P(W)$を求めます．

% align*

ここで，「曇っている」ことが観測されたとします．そうすると，その条件の下で「芝生が濡れている」確率$P(W|C)$は，
以下のようになります．

% align*

つまり，「曇っている」ことの観測が，「芝生が濡れている」確率を変化させているので，これらは独立していない
ことになります．

なお，確率伝播の計算は，逆方向にも可能です．「芝生が濡れている」ことがわかったときに，
その日が「曇っている」確率は以下のようになります．

% align*

%chap04/14

二つめの独立性のパターンは，Tail-to-tail connectionで，二つのノードが親ノードを共有する
パターンです．図4.7に「スプリンクラーが動作」(Sprinkler)，「雨が降った」(Rain)
が共通の親ノード「曇っている」(Cloudy)をもつパターンを示します．

% figure 4.7

このパターンで，たとえば「雨が降った」 ことがわかると，「曇っている」確率は，事前確率$P(C)=0.5$から以下の
$P(C|R)$のように変化します．

% align*

このことによって，「スプリンクラーが動作」の確率も変化するので，「雨が降った」と「スプリンクラーが動作」
は独立ではありません．

一方，Cloudyの値がわかると，Sprinkler，Wet Grassそれぞれの値は，その条件付き確率表だけから求まるので，
それぞれ独立になります．すなわち，このTail-to-tail connectionパターンでは，親ノードの値が与えられると，
子ノードが独立になります．

%chap04/15

最後は，子ノードを共有するHead-to-head connectionパターンです．このパターンの例を図4.8に示します．

% figure 4.8

この場合，「スプリンクラーが動作」と「雨が降った」は，共有する祖先ノードを持たないので，独立です．
しかし，「芝生が濡れている」の値が与えられると，独立ではなくなります．

このことを計算によって確認してゆきましょう．
まず，$P(W)$の事前確率を計算します．$S,R$それぞれのtrueまたはfalseの組合せが起こる確率を事前確率から求め，
条件付き確率表の値と掛け合わせたものを，すべての組合せに対して計算するので，少し面倒な計算になります．

% align*

同様にして，$P(W|S), P(W|R)$を求めます．

% align*

% align*

この$P(W)$と$P(W|S)$を使って，「芝生が濡れている」ことが観測されたときの「スプリンクラーが動作」の確率$P(S|W)$を計算します．

% align*

また，「雨が降った」と「芝生が濡れている」の両方が観測されたとき，「スプリンクラーが動作」の確率$P(S|R,W)$は
% align*
になります．条件部に$R$が加わることで，条件付き確率の値が変わっているので，Wet grassの値が与えられると，RainとSprinklerは独立ではなくなったといえます．

前二つの独立性の議論と比べて，直観的にわかりにくいのですが，「芝生が濡れている」ことがわかっていて，
「雨が降った」が否定されるのなら，「スプリンクラーが動作」の確率が高くならざるを得ないとみれば，納得がゆくと
思います．


%chap04/12

これらのパターンを組み合わせて，図4.9のようなベイジアンネットワークを構成することができます．

% figure 4.9

ベイジアンネットにおけるノードの値の確率計算は，この3パターンと，そのバリエーション（親や子の数が異なる場合）だけなので，
この計算を順に行うことで，ネットワーク全体の確率計算が行えます．

%chap04/16

ここでは，ベイジアンネットワークがすでにできている，すなわち図4.9に示したようなネットワーク
の構造と，全アークに対応する条件付き確率表が得られているものとして，それを用いて識別を行う手順
を説明します．一般にクラスは親ノードに，特徴は子孫ノードに配置します．
求めるものは，特徴を現すノードの値が与えられたもとで，クラスを表すノードが真となる確率ですが，ここではネットワーク中の一部のノードの
値が与えられたときに，値が与えられていないノードが真となる確率を求める問題に一般化して考えます．

ここで，値が真となる確率を知りたいノードが表す変数を，目的変数とよびます．
目的変数以外のすべての変数の値が観測された場合（実際は，目的変数の親ノードの値が観測された場合，あるいは，さらなる親ノードの値から計算可能な場合）は，
目的変数から遠い順に条件付き確率表を使ってノードの値を計算することで，目的変数の
値が求まります．しかし，効率を求める場合や，一部のノードの値しか観測されなかった場合
にも対応できる方法として，確率伝播による計算法があります．

%chap04/0

図4.6のHead-to-tail connectionのネットワークで，確率伝播による計算の概要を
示します．今，求めたいものは真ん中のノードの確率$P(R)$であるとします．ここで，Cloudyノードを含む，
Rainの親ノード集合で，値のわかっている変数の情報をまとめて$e^+$と表現します．また，Wet grass
を含む，Rainの子ノード集合で，値のわかっている変数の情報をまとめて$e^-$と表現します．
そうすると，求める条件付き確率は，式(4.16)のようになります．

% equation 4.16

Head-to-tail connectionのところで説明したように，Rainの値が決まると，$e^+$と$e^-$は独立になるので，
右辺第一項の条件部中の$e^+$は消えます．また，分母はRainの値に関係がないので定数とみなして，
$\alpha = 1 / P(e^- \vert e^+)$とおくと，式(4.16)は式(4.17)のように変形できます．

% equation 4.17

$P(e^- \vert Rain)$は，子ノードであるWet grassの値が得られていれば，条件付き確率表から求まります．値が得られていない場合，
もし末端のノードなら何の情報も得られていないので，すべてのとり得る値を等確率とみなし，もし子ノードがあればこの計算を
再帰的に行います．一方，$P(Rain \vert e^+)$は，親ノードであるCloudyの値が得られていれば，条件付き確率表から求まります．
値が得られていない場合，さらに親ノードを持たない場合は，事前確率が与えられているはずです．親ノードを持つ場合は，
この計算を再帰的に行います．

%chap04/16

このようにノード間の独立性を使いながら，確率を伝搬させて任意のノードの確率を求めることができます．ただし，
この方法はアークを無向とみなした結合を考えたときに，ループが形成されていれば値が収束しないことがあるので，
適用することができません．そのような場合は，確率的シミュレーションも用いられます．

%chap04/0

ここでは，確率的シミュレーションのうち，比較的単純なGibbsサンプリングを紹介します．
基本的な考え方は，ベイジアンネットワークの条件付き確率表に基づいた確率で，乱数を使って変数の値の組合せを多数発生させ，
観測データと一致するものを残して，そのときの目的変数が真であった割合をカウントするというものです．
アルゴリズムを示すと，以下のようになります．

% algorithm 4.1

%chap04/17

最後に，ベイジアンネットワークの学習について説明します．ベイジアンネットワークにおいて学習するべき項目
は，ネットワークの構造とアークの条件付き確率表です．

まず，ネットワークの構造が得られているものとして，アークの条件付き確率表を得る方法について説明します．

学習データにすべての変数の値が含まれる場合は，単純ベイズ法と同様な数え上げによって確率値を決めることができます．
ここでも，ゼロ頻度問題を回避するために，データカウント数の初期値を一定値にしておくなどの
工夫が必要になります．一方，学習データに値が観測されない変数がある場合は，適当な初期値を設定して，
第5章で説明する最急勾配法により学習することになります．

また，ベイジアンネットワークの構造の学習は，そのネットワークによって計算される式(4.7)の対数尤度が大きくなるように，
アークを探索的に追加してゆく方法が考えられます．その基本的な方法がK2アルゴリズムで，概要は以下のようになります．ここで$Node$は特徴集合とクラスからなるノード全体の集合を表します．

% algorithm 4.2

一般に，複雑なネットワークのほうが対数尤度は大きくなるので，このアルゴリズムは簡単に過学習に陥りやすいといわれています．過学習への対処法としては，親ノードの数をあらかじめ制限する方法が提案されています．

%chap04/0

この章では，カテゴリ特徴に対する統計的識別手法について説明しました．すべての特徴値の同時確率を求めるのは難しいので，
すべての次元を独立と仮定したナイーブベイズ識別器や，特徴の部分集合間の独立性を仮定したベイジアンネットワーク
を用いて，学習・識別をおこなう方法を説明しました．視点を変えると，人工知能分野における確率推論の基礎的な部分を
学んだことになります．

統計的識別手法のメリットは，識別結果に確率を付けられる点です．病気の識別など，誤分類のタイプによって
コストが大きく異なる場合は，識別結果の確率とコストを掛け合わせて，コストの期待値が最小になるような識別結果を求める
ことが，現実では必要になってきます．
