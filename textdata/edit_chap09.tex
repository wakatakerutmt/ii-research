--------- はじめに　記号の意味と注意（以下Xは数字を表します） -------------
%ID:XXXX(4桁の数字) ・・・IDがXXXXのスライドに対応したテキスト　文章が途中で分かれていて（2つ目）などと表記があってもIDはそのままです。

%ID:なし　　　　     ・・・スライドに直接関係のない、繋ぎや補足の説明。この「ID:なし」の部分から解答を取ってこないようにしてください。
% figure X.X        ・・・図番号。本来ならここに図が入ります、という意味だけで無視して大丈夫です。解答に含めないようにお願いします。
% equation X.X などその他％から始まる記号 ・・・上記同様、解答に含めないようにお願いします。
$\theta$ など       ・・・数式、引用などのTex記号も無視で、解答に含めないようにお願いします。
\cite{mitchell97}

---------　以下からスライドに対応したテキストデータ ------------------

%ID:0901

深層学習
は
Deep learning
の訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．

本章では，深層学習の基本的な考え方を説明します．

深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする
多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークを
Deep Neural Network (DNN) 
とよびます．

深層学習は，
表現学習(representation learning)とよばれることもあります．
特徴抽出前の生データに近い信号から，その内容を
表現する特徴を学習する，というところがポイントです．

%ID:0902

これまでに説明してきた識別問題では，何を特徴とするかは既に
与えられたものとしてきました．音声処理や画像処理では，これまでの経験や分析の結果，どのような特徴が識別に役立つか
が分かってきていて，識別問題の学習をおこなう対象は，それらの特徴からなるベクトルで表現されました．一方，深層学習は，どのような特徴を抽出するの
かもデータから機械学習しようとするものです（図9.2）．

%ID:0903

深層学習に用いられるニューラルネットワークは，問題に応じてさまざまな形に特化してゆきました．本章では，その特化した構造を，多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワークに分類して説明をします．

%ID:0906

多階層ニューラルネットワークは，図9.1のようなニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするものです．
これはよく用いられる3階層ニューラルネットワークの入力側に，もう1層の特徴抽出層を付け加えればよい，というもの
ではありません．識別に有効な特徴が入力の線形結合で表される，という保証はないからです．それでは，もう1層加えて
非線形にすればよいかというと，隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する
空間的なデータの関連性など）
が考慮されていないので，
それでも十分ではないでしょう．つまり，特徴抽出を学習するには十分多くの層を持つニューラルネットワークが必要だという
ことになります．

第8章で説明したニューラルネットワークの学習手法である誤差逆伝播法は多階層構造でもそのまま適用
できます．そうすると，深層学習でも最初からニューラルネットワークを多階層で構成すれば，それで問題が解決するのではないか，と思われるかも
しれません．

しかし，一般に階層が多いニューラルネットワークの学習には問題点があります．
第8章で説明した誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，
多階層では入力側の重みはほとんど動かない，
ということがわかっています（図9.3）．

アルゴリズム8.1の修正量$\delta$には$出力値 \times (1-出力値) \quad ただし，0<出力値<1$が掛けられますが，
この値は最大でも$1/4$です．この値を1階層上の修正量の重み付き和にかけるのですが，重みは大きい値を取るノード以外は
小さくなるように学習されます（正則化の議論を思い出してください）ので，この値もそれほど大きくはなりません．
また，学習係数$\eta$を大きくしても値が振動するだけなので，問題の解決にはなりません．3層のfeed forward型では
この修正量の減少はあまり問題になりませんでしたが，階層が4層，5層と増えてゆくと修正量が急激に減ってしまいます．

%ID:0907

この問題を解決する手法として，事前学習法(pre-training)が考案されました．
誤差逆伝播法を用いた教師あり学習を行う前に，
何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディアです．この事前学習は，入力$\bm{x}$の
情報をなるべく失わないように，入力層側から1層ずつ順に教師なし学習で行います(図9.4)．
入力層から上位に上がるにつれノードの数は減るので，うまく特徴となる情報を抽出しないと情報を保持すること
はできません．このプロセスで，元の情報を保持しつつ，抽象度の高い情報表現を獲得してゆくことを階層を重ねて行うことが
深層学習のアイディアです．

%ID:なし

事前学習を含めた深層学習のアルゴリズムを以下に示します．

%ID:0908

深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像
を学習する手段として，AutoencoderとRestricted Bolzmann Machine(RBM)がよく使われます．ここでは
第8章で説明したフィードフォワード型のニューラルネットワークを用いたAutoencoderに
ついて説明します．

Autoencoderは，図9.5のように，3階層のフィードフォワード型のニューラルネットワークで
自己写像を学習するものです．

自己写像の学習とは，$d$次元の入力${\bf f}$と，同じく$d$次元の出力${\bf y}$の距離（誤差と解釈しても
よいです）の全学習データに
対する総和が最小になるように，ニューラルネットワークの重みを調整することです．

距離は通常，ユークリッド距離が使われます．また，入力が0または1の2値であれば，出力層の
活性化関数としてシグモイド関数が使えるのですが，入力が連続的な値を取るとき，その値を再現するために
出力層では恒等関数を活性化関数として用います．すなわち，中間層の出力の重み付き和をそのまま出力
します．

%ID:なし

例題9.1で作成したニューラルネットの中間層から出力層の重みを調べると
，
正の比較的大きな値と，負の比較的大きな値のいずれかになっています．また，出力ノードによって，
\{正, 正, 負\}の組合せであったり．\{負, 負, 負\}の組合せであったりしますが，同じ並びの
組合せは出現していません．例えば正を1，負を0と考えると，これは中間層で2進エンコーディングの概念が
獲得されていることに該当します．0から7の数は2進数3桁以内で表現できるので，8ノードの入力の情報が，
その取り得る値の組合せが限定されているために，より少ない3ノードの中間の情報で表現できている
ことになります．

%ID:0908 (2つ目)

Autoencoderではこのようにして得られた中間層の値を新たな入力として，1階層上にずらして同様の表現学習を
行います．この手順を積み重ねると，入力に近い側では単純な特徴が，階層が上がってゆくにつれ複雑な特徴が
学習されます．

%ID:0911

ニューラルネットワークの階層を深くすると，それだけパラメータも増えるので，過学習の問題がより深刻になります．そこで，ランダムに一定割合のユニットを消して学習を行う
ドロップアウト
（図9.7）を用いると，過学習が起きにくくなり，汎用性が高まることが報告されています．

ドロップアウトによる学習では，まず各層のユニットを割合$p$でランダムに無効化します．たとえば$p=0.5$とすると，半数のユニットからなるニューラルネットワークができます．そして，このネットワークに対して，ミニバッチ一つ分のデータで誤差逆伝播法による学習を行います．対象とするミニバッチのデータが変わるごとに無効化するユニットを選び直して学習を繰り返します．

学習後，得られたニューラルネットワークを用いて識別を行う際には，重みを$p$倍して計算を行います．これは複数の学習済みネットワークの計算結果を平均化していることになります．

このドロップアウトによって過学習が生じにくくなっている理由は，学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります（図9.7）．

%ID:0912

タスクに特化したディープニューラルネットワークの代表的なものが，画像認識でよく用いられる
畳み込みニューラルネットワーク
(convolutional neural network; CNN)です．CNNは，畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです（図9.8）．

%ID:0914

畳み込み層の処理は，画像にフィルタをかける処理に相当します（図9.9）．最初の畳み込み層は入力画像と同じ大きさのものを，準備したいフィルタの種類数分だけ用意します．畳み込み層の各ユニットは，入力画像中の一部とのみ結合を持ち，その重みは全ユニットで共有されます．この結合を持つ範囲はフィルタサイズに相当し，この範囲のことを
受容野
とよびます．図9.8では，最初の畳み込み層は3種類のフィルタに相当する処理を行っています．ここでは，それぞれ異なるパターンを学習し，フィルタ係数として獲得しています．

プーリング層は畳み込み層よりも少ないユニットで構成されます．各ユニットは，畳み込み層と同様に受容野を持ち，その範囲の値の平均あるいは最大値を出力とします．これは，受容野内のパターンの位置変化を吸収していることになります．

畳み込みニューラルネットワークでは，特定のユニットは，前の階層の特定の領域の出力だけを受けるという制約を設けているので，単純な全結合のネットワークに比べて，ユニット間の結合数が少なくなります．また，同じ階層のユニット間で重みを共有することで，学習すべきパラメータが大幅に減っていることになります．これらの工夫によって，画像を直接入力とする多階層のニューラルネットワークを構成し，特徴抽出処理も学習の対象とすることができました．このことが，畳み込みニューラルネットワークが各種の画像認識タスクにおいて高い性能を示している原因だと考えられます．

%ID:0916

もうひとつのタスクに特化した構造をもつニューラルネットワークとして，
中間層の出力が時間遅れで自分自身に戻ってくる構造をもつ
リカレントニューラルネットワーク
（図9.10(a)）があります．リカレントニューラルネットワーは時系列信号や自然言語などの系列パターンを扱うことができます．

このリカレントニューラルネットワークへの入力は，特徴ベクトルの系列$\bm{x}_1,\bm{x}_2,\dots, \bm{x}_T $という形式になります．たとえば，動画像を入力して異常検知を行ったり，ベクトル化された単語系列を入力して品詞列を出力するようなタスクが具体的に考えられます．これらに共通していることは，単純に各時点の入力からだけでは出力を決めることが難しく，それまでの入力系列の情報が何らかの役に立つという点です．

リカレントニューラルネットワークの中間層は，入力層からの情報に加えて，一つ前の中間層の活性化状態を入力とします．この振舞いを時間方向に展開したものが，図9.10(b)です．時刻$t$における出力は，時刻$t-1$以前のすべての入力を元に計算されるので，これが深い構造をもっていることがわかります．

%ID:0917

そして，問題となる結合重みの学習ですが，単純な誤差逆伝播法ではやはり勾配消失問題が生じてしまいます．この問題に対する対処法として，リカレントニューラルネットワークでは，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします．この方法を，
長・短期記憶
（Long Short-Term Memory; LSTM）とよび，メモリユニットを
LSTMセル
とよびます．LSTMセルは，入力層からの情報と，時間遅れの中間層からの情報を入力として，それぞれの重み付き和に活性化関数をかけて出力を決めるところは通常のユニットと同じです．通常のユニットとの違いは，内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．

%ID:0919

これらのゲートの開閉は入力情報を元に判定され，現在の入力が自分に関係があるものか，自分は出力に影響を与えるべきか，これまでの情報を忘れてもよいかが判断されます（もちろん判断基準も学習の対象です）．
学習時には誤差もゲートで制御されるので，必要な誤差のみが伝播することで，勾配消失問題が回避されています．

%ID:なし

深層学習を実装するためのツールはいくつか公開されています．

TheanoはDNNを実装するためのPythonのライブラリです．
ただし，構造定義をして学習データを渡せば勝手に学習してくれる，というようなツールではなく，
ニューロンの出力や学習のための関数など，詳細な実装を記述しなければなりません．
Theanoが支援してくれるのは，ニューラルネットを実装する際に必要とされる関数記述の支援と，
GPUを利用した計算を明示的に書く必要がないところです．

Kaldiは音声認識のためのツールキットです．
音声認識を実現するための様々なプログラムが提供されており，その中のひとつとしてDNNが実装されて
います．

深層学習の詳しい説明は，IEEE Signal Processing Magagineに掲載された解説\cite{hinton12}が
まとまっています．日本語で読める解説としては\cite{kubo13}, \cite{yasuda13}, \cite{aso13}などがあります．

