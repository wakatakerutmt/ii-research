%chap07/1

第7章から第10章では，教師あり学習全般に用いることができる，
発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの
工夫で回帰問題にも適用できます．

この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．
扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．

サポートベクトルマシンは，
線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．

いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データが
あるとします．

この学習データに対して，識別率100\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)の
どちらの識別境界線（図の実線）も，学習データに関しては識別率100\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の
真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．

この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．
この距離のことをマージン（図の実線と図の点線の距離）といいます．

マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，
学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．

この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法が
サポートベクトルマシン
です．

%chap07/2

まず，学習データが線形識別可能な状況で，マージンが最大となる識別面を求める方法を考えてゆきましょう．

使用するデータは，数値特徴に対して正解情報の付いたデータですが，
ここでは，2値分類問題に限定し，正解情報の値を正例$1$，負例$-1$とします．

識別面は平面を仮定するので，特徴空間上では式(7.1)で表現されます．

そうすると，$i$番目のデータ$\bm{x}_i$と，この識別面との距離$Dist({\bm{x}_i})$は，点と直線の距離の公式を用いて，式(7.2)のように計算できます．

ここで，式(7.1)の左辺は正例に対しては正の値，負例に対しては負の値を出力するようにその重みを調整した式ですが，
右辺の値が0なので，左辺を定数倍しても表す平面は変わりません．そこで，識別面に最も近いデータを
識別面の式に代入したときに，その絶対値が1になるように重みを調整したとします．

そうすると，式(7.2)，(7.3)より，学習パターンと識別面との最小距離は，以下のようになります．

式(7.4)がマージンを表すので，マージンを最大にする識別面を求める問題は，$||\bm{w}||$を最小化する
問題になります．ここで，最小化しやすいように，この問題を$||\bm{w}||^2$の最小化とします．

この式の形だけを見ると，$\bm{w}=\bm{0}$が最小解ですが，これでは識別面になりません．

%chap07/3

識別面として
すべての学習データを識別できるという式(7.5)の条件を加えます．

$y_i=1~or~-1$としていたので，正例・負例両方の制約を一つの式で表すことができました．

ここまでで，マージンを最大にする識別面を求める問題の定式化が終わりました．
式(7.5)の制約下での$||\bm{w} ||^2$の最小化問題になりました．この後，微分を利用して極値を求めて
最小解を導くので，乗数$1/2$を付けておきます．

%chap07/4

ここでは前節で定式化した問題を，ラグランジュの未定乗数法を用いて解決する方法を説明します．

ラグランジュの未定乗数法を用いると，$g(\bm{x})=0$という条件の下で$f(\bm{x})$の最小値（あるいは最大値）を求める問題は，$L(\bm{x}, \lambda)=f(\bm{x})-\lambda g(\bm{x})$ （ただし$\lambda$はラグランジュ乗数）という新しいラグランジュ関数を導入し，この関数の極値を求めるという問題に置き換えることができます．

ラグランジュ関数の$\bm{x}$に関する偏微分を0とすると，式(7.8)のようになります．

%chap07/0

このラグランジュ関数の極値がもとの問題での与えられた条件下での最小値問題に対応していることを図7.2で説明します．元の問題の最小値を求める関数の引数が2次元，制約を1次式と仮定します．

もとの問題で，まったく制約がないときの最小解は点Aです．一方，制約は直線$g(\bm{x})=0$で表されるので，この直線上で
最も$f(\bm{x})$が低い値をとる点を探します．その最小値となる点では，$f(\bm{x})$の等高線と直線$g(\bm{x})=0$が
接します．そうでなければ，その交点をどちらかにずらすことで，より低い値に移ることができるからです．
この等高線と直線が接するという条件は，それらの法線ベクトルが一致するということに等しくなります．
よって，式(7.8)を満たす解が，もとの問題の最小値になります．

%chap07/4
%%chap07/5

ここでは，ラグランジュの未定乗数法を不等式制約条件で用います．そうすると，式(7.6)，式(7.7)の
制約付きの最小化問題は，ラグランジュ乗数$\alpha_i$を導入して，以下の関数$L$の最小値を求めるという問題に置き換えることができます．

%chap07/5

最小値では$L$の勾配が0になるはずなので，以下の式が成り立ちます．

これを式(7.9)に代入して，以下の式を得ます．

次はこれを最小化するわけですが，これは$\alpha_i$に関する2次計画問題と呼ばれるものです．したがって，
Scilabなどの数値計算ソフトウェアを使って簡単に解くことができます．

これを解くと，$\alpha_i \neq 0$となるのは，サポートベクトル
に対応するもののみで，大半は$\alpha_i = 0$となります．
この$\alpha_i$を式(7.11)に代入して，$\bm{w}$を得ることができます．

%chap07/6

また，$w_0$は，$\bm{x}_{+}, \bm{x}_{-}$をそれぞれ正例，負例に属するサポートベクトルとすると，以下の式で求めることが
できます．

この方法で，学習データが線形分離可能な場合にはマージンを最大にする決定境界が見つかります．

%chap07/7

次に，学習データが線形分離可能でない場合を考えます．前節と同様に線形識別面を設定するのですが，その際，
間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶこととします．

%chap07/8

式(7.7)がすべてのデータを正しく識別できる条件なので，この制約を弱める変数（スラック変数とよびます）$\xi_i ~ (\ge 0)$を
導入して，$i$番目のデータが制約を満たしていない程度を示します．

$\xi_i$は制約を満たさない程度を表すので，小さい方が望ましいものです．この値を上記SVMのマージン最大化
（$|| \bm{w} ||^2$の最小化）問題に加えます．

%chap07/9

ここで$C$は制約を満たさないデータをどの程度の重みで組み込むかを決める定数で，$C$が大きければ影響が大きく，
$C$が小さければほとんど無視するような振る舞いになります．

%%chap07/8

これをラグランジュの未定乗数法で解くと，結論として同じ式が出てきて，ラグランジュ乗数$\alpha_i$に
$0 \le \alpha_i \le C$という制約が加わります．

%chap07/10

一般に，特徴空間の次元数$d$が大きい場合は，データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります．
そこで，この性質を逆手にとって，低次元の特徴ベクトルを高次元に写像し(図7.3参照)，線形分離の可能性を高めてしまい，その高次元空間上でSVMを使って識別超平面を求めるという方法が考えられます．

この方法は，むやみに特徴を増やして次元を上げる方法とは違います．識別に役立つ特徴で構成された$d$次元空間
に対して，もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています．一方，識別に無関係な特徴を持ち込むと，
データが無意味な方向に\ruby{疎}{まばら}に分布し，もとの分布の性質がこわされやすくなってしまいます．

しかし問題は，もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が
見つかるかということです．

%chap07/11

ここで，もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて
定義されるある関数$K(\bm{x}, \bm{x}')$を考えます．この関数を
カーネル関数
とよびます．
そして，非線形写像を$\phi$としたときに，以下の関係が成り立つことを仮定します．

つまり，もとの空間での2点間の距離が，非線形写像後の空間における内積に反映されるという形式で，
近さの情報を保存します．

%chap07/15

そうすると，写像後の空間での識別関数は以下のように書くことができます．

ここで，SVMを適用すると，$\bm{w}$は，式(7.11)のようになるので，以下の
識別関数を得ることになります．

同様に，式(7.12)より，学習の問題も以下の式を最大化するという問題になります．

ここで注意すべきなのは，式(7.20), (7.21)のどちらの式からも$\phi$が消えているということです．
カーネル関数$K$さえ定まれば，
識別面を得ることができるのです．このように，複雑な非線形変換を求めるという操作を避ける方法を
カーネルトリック
とよびます．これがSVMがいろいろな応用に使われてきた理由です．

%chap07/12

さて，カーネル関数が正定値関数という条件を満たすときには，このような非線形変換$\phi$が存在することが
わかっています．そのようなカーネル関数の例としては，以下のような多項式カーネル関数や

以下のようなガウシアンカーネル関数

などがあります．

%chap07/13

ここで，簡単なカーネルについてその非線形変換$\phi$を求めてみましょう．特徴ベクトルを2次元として多項式カーネル(p=2)を展開します．

したがって，$\bm{x}=(x_1, x_2)$のとき，$\phi(\bm{x})=(x_1^2, x_2^2, \sqrt{2} x_1 x_2, \sqrt{2} x_1, \sqrt{2} x_2, 1) )$
となります．この変換の第3項に注目してください．特徴の積の項が加わっています．積をとるということは，2つの特徴が
同時に現れるときに大きな値になります．すなわち，共起の情報が加わったことになります．

このような，非線形変換で線形分離可能な高次元にデータを飛ばしてしまい，マージン最大化基準で
信頼できる識別面を求めるというSVMの方法は非常に強力で，文書分類やバイオインフォマティックス
など様々な分野で利用されています．

%chap07/16

ここまで，数値特徴を対象にSVMを説明してきましたが，SVMは一見カテゴリ特徴の問題に見える自然言語処理でも
多用されている機械学習手法です．
ここでは，典型的なSVMの応用事例として，文書分類問題を取り上げます．

分類対象である文書は文字列データです．この文書という対象の特徴ベクトルをどのように考えればよいでしょうか．
ちょっと驚くかもしれませんが，文書分類では，文書に現れうる全単語をそれぞれの次元に設定します．第$i$次元が
単語$word_i$に対応し，文書に出現すれば1，出現しなければ0とします．
そうすると，特徴ベクトルは数千次元から数万次元になります．

SVMは汎化性能が高いので，このような高次元の識別問題に用いることができます．

%chap07/17

Grid search は，パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます．
図7.3のようなパラメータを組み合わせる空間をグリッドとよびます．

SVMでは，たとえばRBFカーネルの範囲を制御する$\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．

%chap07/0

本章ではSVMについて説明しました．SVMは2値識別器なので，多クラス識別問題に適用するときは
$c$クラスの場合，あるクラスとそれ以外のデータを識別するSVMを$c$個作成し，
結果として，最もスコアの高いものを選ぶというような工夫が必要になります．

6章から8章までは，数値特徴の識別問題に対するいくつかの手法を紹介してきました．
これらの手法は，それぞれ異なった背景から導かれ，異なったモデルを作成する
ためのものにみえますが，理論的に考察してゆくと，モデルの良し悪しを評価する
損失関数
の異なりに帰着できることがわかっています．損失関数と各種学習手法の
対応は，参考文献\cite{sugiyama13}の第III部に詳しく説明されています．
