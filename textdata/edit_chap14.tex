--------- はじめに　記号の意味と注意（以下Xは数字を表します） -------------
%ID:XXXX(4桁の数字) ・・・IDがXXXXのスライドに対応したテキスト　文章が途中で分かれていて（2つ目）などと表記があってもIDはそのままです。

%ID:なし　　　　     ・・・スライドに直接関係のない、繋ぎや補足の説明。この「ID:なし」の部分から解答を取ってこないようにしてください。
% figure X.X        ・・・図番号。本来ならここに図が入ります、という意味だけで無視して大丈夫です。解答に含めないようにお願いします。
% equation X.X などその他％から始まる記号 ・・・上記同様、解答に含めないようにお願いします。
$\theta$ など       ・・・数式、引用などのTex記号も無視で、解答に含めないようにお願いします。
\cite{mitchell97}

---------　以下からスライドに対応したテキストデータ ------------------


%ID:1401

第14章と第15章は，教師あり学習と教師なし学習のどちらでもない学習手法について説明します
（図14.1）．

% figure 14.1

この「どちらでもない」ということが何を意味かを正確に定義するのは難しい問題です．
今まで扱ってきた問題を単純化すると，手元の全ての入力例に対して望ましい出力が付けられている状況で，入力から出力へ写像する関数を獲得するという設定か，あるいはとにかく大量のデータがある状況で，それらに内在する性質を発見するという設定のいずれかで，
この2つには当てはまらないけれどデータを活用したい，という状況は現実には様々な設定でありそうです．

まず，典型的な「どちらでもない」状況は，教師信号が一部の学習データにのみ与えられている状況です．
例えば，特定の製品について書かれたブログエントリやツイートなどのWeb文書に対して，肯定的／否定的の分類を行いたいという
問題設定を考えます．クローラープログラムを使えば，その製品名を含むWeb文書を多数入手することができます．
しかし，それらに識別対象のターゲット（肯定的／否定的）を付けるのは手間の掛かる作業なので，
現実的には集めた文書の一部にしかターゲットを与えることができません．すなわち，少量のターゲット付きデータと，
大量のターゲット無しデータがある状況で識別器を構成するという設定になります．このような状況を半教師あり学習
と呼び，本章でその手法を検討してゆきます．

%ID:1402


上で定義したように，本章で扱う半教師あり学習は教師あり／なしの混在型データに対する識別学習です．
基本的には教師ありデータで識別器を作成して，教師なしデータをできるだけ識別器の性能向上に役立てるという
方針で学習手順を考えます．このような目的で教師なしデータを用いる際には，教師なしデータの性質に
ある程度の制約があります．



まず，入力が数値のベクトルである場合を考えましょう．図14.2左の図のように，データが
クラスタを形成していると見なすことができ，同一クラスタ内に異なるクラスのターゲットが混在していない
ような状況では，教師ありデータの分布から教師なしデータのターゲットを比較的高い精度で推測でき，それらが
識別器の性能向上に役立ちそうです．

% figure 14.2

一方，図14.2右の図のように，明確なクラスタが確認されず，識別境界は存在しそうだけど，
どのデータに教師信号が付いているかで推定される識別境界の位置が大きく異なりそうなデータは，間違った
方に分類された教師なしデータがかえって識別器の性能を下げる振る舞いをするかもしれません．
確率的モデルの生成的手法のことばで表現すると，正解なしデータから得られる$p(\bm{x})$に関する
情報が，$P(y|\bm{x})$の推定に役立つことが，半教師あり学習が成立する条件です．


%ID:1403

このようなことを考慮すると，入力が数値のベクトルである場合，半教師あり学習が可能なデータは
以下の仮定を満たしていることが必要になります．

\begin{itemize}
\item 半教師あり平滑性仮定\\
　もし二つの入力 $\bm{x}_1$ と $\bm{x}_2$ が高密度領域
　
　で近ければ，出力 $y_1$ と $y_2$ も関連している
\item クラスタ仮定\\
　もし入力が同じクラスタに属するなら，それらは同じクラスになりやすい
\item 低密度分離\\
　識別境界は低密度領域にある
\item 多様体仮定\\
　高次元のデータは，低次元の多様体上に写像できる
\end{itemize}

最後の仮定は，多次元でも「次元の呪い」にかかっていない，ということです．
第7章で行った，多次元空間への写像の逆が成り立っているということになります．

%ID:1404


カテゴリ特徴の場合は，数値特徴の場合のような一般化は難しいのですが，カテゴリ特徴で
大量に学習データが入手可能な状況というのは，ほぼ言語データの識別問題に絞られます．

最も簡単なケースは図14.3のように，教師ありデータで抽出された
特徴語の多くが教師なしデータに含まれる場合です．このように識別に役立つ語の
オーバーラップが多いデータは教師なしデータに対しても比較的高い精度でターゲットを
付けることができ，口述する自己学習などを使えば，よい識別器ができそうです．

% figure 14.3

%ID:1405

しかし，通常はそう簡単にはゆきません．図14.3の例で示したような
商品の評価を行う文書にしても，褒める言葉やけなす言葉は様々なバリエーションがあります．
顔文字を使ったり，略語を使ったりもするでしょう．
そのような場合，正解付きデータと特徴語のオーバーラップが多い正解なしデータに
ラベル付けを行って正解ありデータに取り込んでしまった後，新たな頻出語を特徴語と
することによって，連鎖的に特徴語を増やしてゆくという手段が考えられます（図14.4）．

% figure 14.4

つまり，ラベル特徴の場合，教師ありデータと教師なしデータにラベル値のオーバーラップが全く見られないデータ
では，半教師あり学習は役に立ちませんが，教師なしデータの一部とでも適当なオーバーラップがあれば，その一部の
教師なしデータが他の教師なしデータを徐々に巻き込んでゆく可能性があります．通常，自然言語で書かれたデータは
この後者の仮定を満たすことが多いので，半教師あり学習は文書分類問題によく適用されます．

%ID:1406


ここまで見てきたように，特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器の
パラメータを，教師なしデータを用いて調整してゆく，というのが半教師あり学習の基本的な進め方です（この章の
最後に紹介するYATSIアルゴリズムは例外です）．

識別器を作成するアルゴリズムはこれまで紹介してきたものを問題に応じて用いればよいのですが，
信用できる出力をする教師なしデータを次回の識別器作成に取り込むためには，ナイーブベイズ
識別器のような，その識別結果に確信度を伴うものが適切です．

一方，繰り返しアルゴリズムに関して，単純に終了のための閾値チェックをするだけなのか，
識別器のパラメータを繰り返しの度に変化させるか，識別器で使う特徴に制限をかけるか，など
様々な設定が可能です．以下では，繰り返しアルゴリズムの違いによって生じる，様々な
半教師あり学習手法について説明してゆきます．

%ID:1407


自己学習(self-training)は，最も単純な半教師あり学習アルゴリズムで，
教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，
自分を再度学習させるということを繰り返すものです(図14.5)

% figure 14.5

自分が出した結果を信じて，再度自分を学習させるというところが自己学習と呼ばれる理由です．
繰り返しによって学習データが増加し，より信頼性の高い識別器ができることをねらっています．

%ID:1408

自己学習は図14.2左の図のような，半教師あり学習に適したデータの場合はよいのですが，
低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りが
ずっと影響を及ぼし続ける性質があります．

%ID:1409


自己学習の問題点は，自分が出した誤りを指摘してくれる他人がいない，というたとえができます．
そこで，判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習
を実現する方法が，共訓練(Co-training)です．

% figure 14.6

共訓練は，異なった特徴を用いて識別器を2つ作成し，相手の識別結果を利用して，それぞれの識別器を学習させる
アルゴリズムです．まず，教師付きデータの分割した特徴から識別器1と識別器2を作成し，教師なしデータをそれぞれで識別
します．識別器1の確信度上位$k$個を教師付きデータとみなして，識別器2を学習します，その後，1と2の役割を入れ替え，
精度の変化が少なくなるまで繰り返します．

%ID:1410

共訓練の特徴は，学習初期の誤りに強いということが挙げられます．欠点としては，それぞれが識別器と
して機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の
方が性能がよいことがあること，などです．

%ID:1411

ラベル特徴の教師あり／なしの混合データに対する半教師あり学習のように，特徴的なラベルが同じクラスの
データで伝播するような性質がある場合は，自己学習や共訓練のような繰り返しアルゴリズムが効果を発揮します．
しかし，数値特徴では繰り返しアルゴリズムがうまく働く場合と，初期の誤りがその後の結果に影響を及ぼし続けて
あまりよい結果とならない場合があります．

そこで，繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で
全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という
単純なアルゴリズムがYATSI(Yet Another Two-Stage Idea)です（図14.7）．

% figure 14.7

教師ありデータを$D_l$，教師なしデータを$D_u$として，以下のアルゴリズムで重み付きk-NN識別器を
作成します．

% algorithm 14.1

ここで，$F$は教師なしデータの寄与分で，どれだけ教師なしデータの識別結果を信用するか，
というパラメータです．このようにして作成した重み付きk-NN識別器でテストデータを識別します．

%ID:1412

ラベル伝搬法の考え方は，
特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
というものです．
近くのノードは同じクラスになりやすいという仮定で，正解なしデータの予測を行います．

評価関数は式(14.1)に示すもので，この評価関数の最小化をおこないます．

% equation 14.1

$f_i$は$i$番目のノードの予測値，$y_i$は$i$番目のノードの正解ラベル{ -1, 0, 1}，$w_{ij}$は$i$番目のノードと$j$番目のノードの結合の有無を表します．

%ID:1413

最小化の手順は，以下の通りです．

\begin{enumerate}
\item データ間の類似度に基づいて，データをノードとしたグラフを構築\\
類似度の基準は，RBF ($K(\bm{x}, \bm{x}') = \exp(-\gamma \| \bm{x} -\bm{x}' \|^2 )$)や，k-NNが使われます．前者は，全ノードが結合し，
連続値の類似度が与えられます．後者は，近傍のk個のノードのみが結合する省メモリの手法で，結合の有無は0または1で表現されます．

%ID:1414

\item ラベル付きノードからラベルなしノードにラベルを伝播させる操作を繰り返し，隣接するノードがなるべく同じラベルを持つように最適化
\end{enumerate}

%ID:なし

本章では，教師あり／なしの混在型データに対する半教師あり学習手法について説明しました．
半教師あり学習はデータの性質を予め検討できる場合や，先行事例でうまくゆくことが報告されている
問題に対しては有効な手法です．しかし，推定した結果を使って新たなデータを識別するので，
その推定が誤っていると，結果は良いものにはなりません．

ビッグデータの時代には，少しの教師ありデータと大量の教師なしデータの組合せで学習ができる，と聞くと
非常に魅力的ですが，ここまで説明してきたように，半教師あり学習は扱うデータの性質にその結果が大きく
影響されるということを忘れないようにしましょう．

半教師あり学習の参考文献として，\cite{Chapelle10}の第1, 2, 3, 6章（それ以外はやや専門的），本章の内容よりも進んだ内容を学べる日本語の文献として\cite{sugiyama13}の第16章をお勧めします．

