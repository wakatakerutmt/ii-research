--------- はじめに　記号の意味と注意（以下Xは数字を表します） -------------
%ID:XXXX(4桁の数字) ・・・IDがXXXXのスライドに対応したテキスト　文章が途中で分かれていて（2つ目）などと表記があってもIDはそのままです。

%ID:なし　　　　     ・・・スライドに直接関係のない、繋ぎや補足の説明。この「ID:なし」の部分から解答を取ってこないようにしてください。
% figure X.X        ・・・図番号。本来ならここに図が入ります、という意味だけで無視して大丈夫です。解答に含めないようにお願いします。
% equation X.X などその他％から始まる記号 ・・・上記同様、解答に含めないようにお願いします。
$\theta$ など       ・・・数式、引用などのTex記号も無視で、解答に含めないようにお願いします。
\cite{mitchell97}

---------　以下からスライドに対応したテキストデータ ------------------

%ID:1301

この章では，もう一度教師あり学習の設定に戻って，系列データを識別する手法について説明します．系列データとは，個々の要素の間に i.i.d. の関係が成立しないものです．
系列データの識別をおこなうモデルを学習するためには，一部教師なし学習の要素が入ってくる場合があるので，
この順序で説明することとしました．

入力の系列長と出力の系列長との関係を整理すると，系列データの識別問題は，以下の三つのパターンに分類されます．

\begin{enumerate}
\item 入力の系列長と出力の系列長が等しい問題
\item 入力の系列長にかかわらず，出力の系列長が1である問題
\item 入力の系列長と出力の系列長の間に，明確な対応関係がない問題
\end{enumerate}

1.は，単語列を入力して，名詞や動詞などの品詞の列を出力する，いわゆる形態素解析処理が典型的な問題です．一つの入力（単語）に一つの出力（品詞）が対応します．この問題の最も単純な解決法としては，これまでに学んだ識別器を入力に対して逐次適用してゆくというものが考えられます．しかし，ある時点の出力がその前後の出力や，近辺の入力に依存している場合があるので，そのような情報をうまく使うことができれば，識別率を上げることができるかもしれません．この問題が本章の主題の一つである系列ラベリング問題です．

2.は，ひとまとまりの系列データを特定のクラスに識別する問題です．たとえば動画像の分類や音声で入力された単語の識別などの問題が考えられます．最も単純には，入力をすべて並べて一つの大きなベクトルにしてしまうという方法が考えられますが，入力系列は一般に時系列信号でその長さは不定なので，そう簡単にはゆきません．また，典型的には入力系列は共通の性質を持ついくつかのセグメントに分割して扱われますが，入力系列のどこにそのセグメントの切れ目があるかという情報は，一般には得られません．そこで，このセグメントの区切りを隠れ変数の値として，その分布を教師なしで学習するという手法と，通常の教師あり学習を組み合わせることになります．これが系列認識問題です．

3.は連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります．
この問題は学習にも認識にも相当込み入った工夫が必要になるので，本章ではその概要のみ説明します．音声認識の詳細に関しては，拙著\cite{araki15}をご覧ください．

%ID:1302

最初の問題設定として，ラベル特徴の系列を入力として，それと同じ長さのラベル系列を出力する識別問題を扱います．

%ID:1303

前節で典型的な例としてあげた形態素解析は，単語の系列を入力として，それぞれの単語に品詞を付けるという問題です
(図13.1)．
形態素の列はある言語の文を構成するので，その言語の文法に従った並び方が要求されます．たとえば，日本語の形態素列は，形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなどの傾向が，明らかに存在します．

また，地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
固有表現抽出
（チャンキングとも呼びます）も，系列ラベリングの典型的な問題です．
1単語が1表現になっていれば形態素解析と同じ問題ですが，複数の単語で一つの表現になっている場合があるので，
その並びにラベルを付けます．ラベルの付け方は，その表現の開始を表すB (Beginning)，2単語目以降の表現の構成要素を指す
I (Inside)，表現外の単語を表すO (Outside)の3種類になります．これは，Iの前は必ずBかIであることや，BやIの連続出現数に
それぞれおおよその上限数があることなど，出力の並びに一定の制約があります．このラベル方式にはIOB2タグという名前がついています．

たとえば，文中から「人を指す表現」を抽出した結果は，図13.2のようになります．  

%ID:1302 (2つ目)

では，このような系列ラベリング問題を，機械学習によって解決する識別器の構成を考えてゆきましょう．

単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点があります．学習データは，入力系列と出力ラベルのペアとして与えられますが，形態素解析や固有表現抽出の例でみたように，出力系列には並びによる依存関係があるので，個々の識別問題として扱うのは不適当だということです（問題点1）．

それでは出力もまとめてしまって，出力系列を一つのクラスとするということも考えられますが，通常，
そのクラス数は膨大な数になってしまいます．たとえば，品詞が10種類で，20単語からなる文にラベル付けする問題で
は$10^{20}$種類の出力が可能になり，これらを個別のクラスとして扱うのはほとんど不可能です（問題点2）．

%ID:1305

前後の入力や一つ前の出力など，役に立ちそうな特徴を利用し，かつ系列としての確からしさを評価しながら探索的に出力を求める手法として，識別モデルの一つである対数線型モデルを応用する方法を考えます．

対数線型モデルに基づくと，入力$\bm{x}$が与えられたときの出力$\bm{y}$の条件付き確率は，式(13.1)のように表現できます．

ここで$\phi(\bm{x}, \bm{y})$は素性ベクトルで，各次元は$\bm{x}, \bm{y}$から定められる様々な素性，$\bm{w}$はそれらの素性の重みからなる重みベクトル，$Z_{\bm{x},\bm{w}}$は$Z_{\bm{x},\bm{w}}=\sum_{\bm{y}} exp(\bm{w}\cdot\phi(\bm{x}, \bm{y}))$で定義される定数で，$\sum_{\bm{y}}P(\bm{y}|\bm{x})=1$を保証するためのものです．

そして，式(13.1)の条件付き確率を用いると，出力$\bm{y}^*$は式(13.2)の最大化問題を解くことによって求まります．

この段階では，素性関数としては前後の入力や出力を自由に組み合わせることができるので，系列としての情報を反映したものを設定することができ，上記の問題点1は解決したように見えます．しかし，式(13.2)ではすべての可能な$\bm{y}$についての値を計算する必要があるので，まだ問題点2が解決していません．

%ID:1304 

そこで，利用できる素性を図13.3に示す組合せに限定します．つまり，出力系列で参照できる情報は一つ前のみ，入力系列は自由な範囲で参照できるとします．出力系列を参照する素性を遷移素性，入力と対応させる素性を観測素性と呼びます．

%ID:1306

そうすると，式(13.2)は式(13.3)のように書き換えることができます．

式(13.3)右辺の和の部分の最大値を求めるには，先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すことになります．このような手順を
ビタビアルゴリズム
とよびます．

このような制限を設け，対数線型モデルを系列識別問題に適用したものを
条件付き確率場（Conditional Random Field: CRF）とよびます．

CRFの学習は，対数線型モデルほど簡単ではありませんが，識別の際の手順と同様に，素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質を利用します．

%ID:1309

次に，「入力の系列長に関わらず出力の系列長が1である問題」を扱います．

出力が一つになったので，ラベルの膨大な組合せを扱う前節の設定よりもやさしく感じるかもしれません．しかし，この問題の難しさは，学習の際に観測されない隠れ変数の値を用いなければならない，という点にあります．

まず，簡単な例題を考えてみましょう．

PCで文書作成を行っているユーザのキー入力・マウス操作をシンボルで表して，その系列で初心者と熟練者を識別する問題を考えます．入力はキー入力・マウス操作を抽象化したもので，10以上の連続通常キー入力k，エラーキー（DeleteキーやBack spaceキー）入力e，ファイル保存や文字修飾などのGUI操作をgとします．

ここで知見として，初心者はキー入力kとGUI入力gを頻繁に繰り返し，かつ時間が経過するにつれてエラーeが増える傾向にあるとします．また，熟練者は最初にキー入力を重点的に，後からGUI入力をまとめて行う，という傾向があるとします．

初心者Bさんの操作記録は以下のようなものでした．

\begin{quote}
\tt k e k g k e k g g k g k k e g e e k e e e g e
\end{quote}

一方，熟練者Sさんの操作記録は以下のようなものでした．

\begin{quote}
\tt k k e k g k k k e k g k g g g e g k g
\end{quote}

そして，問題として以下のような系列が観測されたとき，この人は初心者か，熟練者かを識別するという状況を考えます．

\begin{quote}
\tt k g e k g k k g e k g e k e e k e g e k
\end{quote}

%ID:1310

この与えられた系列を$\bm{x}$として，クラス$y$（ただし，$y= B (初心者) or S (熟練者)$）の事後確率$P(y|\bm{x})$を何らかのモデルを使って計算することを考えます．ここで式(13.4)のような$\bm{x}, y$の同時確率を考える生成モデルアプローチをとるのが
\textbf{HMM}(Hidden Marcov Model: 隠れマルコフモデル)の考え方です．

HMMは，式(13.4)の$P(\bm{x}|y)$の値を与える確率的非決定性オートマトンの一種です．
各状態であるシンボルをある確率で出力し，ある確率で他の状態(あるいは自分自身)に遷移します．

%ID:1311

形式的に定義すると，HMMは以下の要素と確率で定義されます．

\begin{itemize}
\item 状態の集合: $\{S_i\}~~(1 \ge i \ge n)$
\item 初期状態，最終状態の集合
\item 遷移確率: 状態$i$から状態$j$への遷移確率$a_{ij}$
\item 出力確率: 状態$i$で記号$o$を出力する確率$b_i(o)$
\end{itemize}

このようなHMMの構造を仮定すると，状態$\pi_1$から状態$\pi_2$に移るセグメントを隠れ変数
とみてEMアルゴリズムで，各状態の確率を推定します．

HMMは式(13.4)の$P(\bm{x}|y)$を計算するものです．別途推定した$P(y)$との
積を求めることで，事後確率$P(y|\bm{x})$を最大とする$y$を求めることができます．

HMMは，13.1節の冒頭に挙げた3番目の問題，すなわち，入力の系列長と出力の系列長に，
明確な対応関係がない問題にも適用できます．

最も単純なモデル化では，クラス毎に作成したすべてのHMMの初期状態と最終状態をそれぞれ
1つにまとめ，最終状態から初期状態へ戻る遷移を加えれば，任意の長さの出力系列を
表すことができます．

%ID:1313

この連結されたHMMを用いて，入力系列に対して最も確率が高くなる遷移系列を
ビタビアルゴリズムによって求めます．最も確率が高くなる遷移系列が定まるということは，
全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に
対して，出力が定まるということになります．

%ID:なし

この問題設定の典型例である音声認識では，音素をHMMで表現し，
単語辞書で音素系列を，言語モデルで隣接可能な単語（あるいは隣接単語の確率）を
表現して，最適な状態遷移系列を求めるために，探索幅を制限したビームサーチや，
最適解に至る可能性をヒューリスティックで見積もった探索を行うなどの
複雑な工夫をしています．

本章では，系列ラベリング問題について説明しました．識別問題は，入力が何らかの構造を持つと非常に難しくなります．系列はその中でも単純なものですが，入力が木構造・グラフ構造になると，それらの構造が持っているもとの情報をなるべく失わずに，機械学習アルゴリズムで扱える特徴に移してくるところがポイントです．

系列ラベリングについては文献\cite{takamura10}の5章が参考になります．文献\cite{tsuboi06}は，HMMとCRFが本書とは別の観点から比較されています．グラフ構造をもつデータの識別に関しては，文献\cite{kashima10}を参照してください．連続系列識別問題の典型例である音声認識に興味がある方は，文献\cite{araki07}をご覧ください．
