--------- はじめに　記号の意味と注意（以下Xは数字を表します） -------------
%ID:XXXX(4桁の数字) ・・・IDがXXXXのスライドに対応したテキスト　文章が途中で分かれていて（2つ目）などと表記があってもIDはそのままです。

%ID:なし　　　　     ・・・スライドに直接関係のない、繋ぎや補足の説明。この「ID:なし」の部分から解答を取ってこないようにしてください。
% figure X.X        ・・・図番号。本来ならここに図が入ります、という意味だけで無視して大丈夫です。解答に含めないようにお願いします。
% equation X.X などその他％から始まる記号 ・・・上記同様、解答に含めないようにお願いします。
$\theta$ など       ・・・数式、引用などのTex記号も無視で、解答に含めないようにお願いします。
\cite{mitchell97}

---------　以下からスライドに対応したテキストデータ ------------------

%ID:0501

この章では，前章で学んだ統計モデルによる識別法で，数値を要素とする特徴ベクトルを識別する
問題に取り組みます．数値を要素とする特徴ベクトルに対する識別問題は，一般にはパターン認識とよばれます．

第3章と第4章では，カテゴリ特徴に対する識別問題を扱いました．続いて本章では，数値を要素とする特徴ベクトル$\bm{x}$に対する識別問題を扱います（図5.1）．

%ID:0502

識別問題は教師あり学習なので，学習データは特徴ベクトル$\bm{x}_i$と正解情報であるクラス$y_i$のペアからなります．

カテゴリ特徴との違いは，特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点です．特徴の種類数を
$d$個とすると，この空間は$d$次元空間になります．この空間を，
特徴空間
とよびます．学習データ中の各事例は，特徴空間上の点として表すことができます（図5.2）．

もし，特徴抽出段階で適切な特徴が選ばれているならば，図5.3のように，学習データは特徴空間上で，クラスごとにある程度まとまりを構成していることが期待できます．そうでなければ，人間や動物が日常的に識別問題を解決できるはずがありません．このように考えると，数値特徴に対する識別問題は，クラスのまとまり間の境界をみつける，すなわち，特徴空間上でクラスを分離する境界面を探す問題として定義することができます．

境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合は，本章で説明する統計モデルによるアプローチが有効です．

一方，学習データがまとまっているはずだといっても，それが比較的単純な識別面で区別できるほど，きれいには分かれていない場合もあります．その境界は，曲がりくねった非線形曲面になっているかもしれません．また，異なるクラスのデータが一部重なる部分がある可能性があります．そのような，非線形性を持ち，完全には分離できないかもしれないデータに対して識別を試みるには，次章以降で説明する二つのアプローチがあります．

一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です．この代表的な手法が第8章と第9章で説明するニューラルネットワークです．

%ID:0503

第4章では，カテゴリ特徴に対する統計的識別手法を説明してきました．その基本的な考え方は，数値特徴に対しても
適用することができます．

数値特徴の場合のナイーブベイズ識別の結果を求める式は，式(5.1)のようになります．カテゴリ特徴の場合の式(4.12)とほとんど同じですが，尤度が離散事象に対する確率分布$P(x_j|\omega_i)$ではなく，数値に対する確率密度関数$p(x_j|\omega_i)$になっています．

事前確率$P(\omega_i)$に関しては，カテゴリ特徴のときと同様に，学習データ中のクラス$\omega_i$に属するデータを数える最尤推定で求めればよいので，とくに問題はありません．

しかし，尤度$p(x_j \vert \omega_i)$に関しては，求めるものが「クラス$\omega_i$のデータの属性$a_j$が値$x_j$となる確率」で，$x_j$が連続値なので，頻度を数えるという方法を用いることはできません．

そこで，数値特徴に対しては，尤度を計算する確率密度関数に適切な統計モデルをあてはめ，そのモデルのパラメータを学習データから推定する
という方法を取ります．数値データに対する統計的モデル化は，それだけで一冊の本になるぐらい奥が深い問題です．
本書では，教師なし学習におけるモデル推定のところで少し詳しく説明するので，ここでは，最も単純な方法で
考えます．

様々な数値データに対して多く用いられる統計モデルが正規分布です．正規分布は図5.4に示すような釣り鐘型をした分布で，身長・体重の分布や，多人数が受けるテストの点数の分布などがよく当てはまります．

1次元データの正規分布は，式(5.2)のようになります．

ここで，$\mu$は正規分布の平均値，$\sigma$は標準偏差です．この二つを正規分布のパラメータとよび，パラメータの値が決まると，$p(x)$の関数形が決まります．

このような仮定をおいたときの学習は，正規分布の平均値と標準偏差を学習データから推定するという問題になります．
これは，カテゴリカルデータの頻度による推定と同様の考え方で，学習データの平均値をモデルの平均値，学習データの標準偏差をモデルの標準偏差とすることで，最尤推定になります．

%ID:0504

ここで，式(4.2)に基づいて得られた事後確率の計算式を，記号を変えてもう一度見直してみます．

式(5.3)の分子は，
生成モデル
とよばれる考え方で解釈することができます．まず，あるクラス$\omega_i$が確率$P(\omega_i)$で選ばれ，そのクラスから特徴ベクトル$\bm{x}$が確率$p(\bm{x} \vert \omega_i)$に基づいて生成されたという考え方です．これは式(5.4)の分子である特徴ベクトルとクラスの同時確率$p(\omega_i, \bm{x})$を求めていることになります．

この生成モデルアプローチは，（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合には有効です．しかし，そうでない場合は，推定するべきパラメータは，$P(\omega_i|\bm{x})$を直接推定するよりも増えてしまいます．同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなるのが一般的です．つまり，生成モデルアプローチは，本来解くべき問題を，あえて難しい問題にしてしまっているのではないかという疑問が出てくるわけです．

この問題への対処法として，次節では，$P(\omega_i|\bm{x})$を直接推定する方法について説明します．

%ID:0505

$P(\omega_i|\bm{x})$をデータから直接推定するアプローチは，
識別モデル
とよばれます．
識別モデルと近い考え方で，
識別関数法
というものがあります．これは，第1章で説明した関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法です．確率・統計的な手法が主流となる前の時代，すなわちコンピュータがそれほど高速でなく，大規模なデータを用意することが難しかった時代には，この識別関数法はパターン認識の主流の手法でした．

最も古典的な識別関数法の手法は，パーセプトロンとよばれるもので，生物の神経細胞の仕組みをモデル化したものでした．以後，このパーセプトロンを多層に重ねたニューラルネットワーク（多層パーセプトロンともよばれます）について，理論的な研究が進められ，1980年代に誤差逆伝播法によって学習が可能であることが多くの研究者に認知されると，一時的にブームを向かえることになります．しかし，その後は，統計的手法の
発展や，同じ識別関数法でもサポートベクトルマシンの優位性が強調されるにつれて，次第に過去の手法と見なされるようになっていました．

ところが近年，第9章で紹介する深層学習が驚異的な成果を上げていることで，再度注目されるようになっています．

%ID:0506

まず，最も基本的な識別関数法である誤り訂正学習から説明を始めます．

1943年に，McCullochとPittsは神経細胞の数理モデル（図5.5）を組み合わせて，任意の論理関数が計算可能であることを示しました．

図5.5に基づいた計算モデルを単層パーセプトロンとよびます．このモデルを単独で考えると，入力の重み付き和を計算して，その値と閾値を比べて出力を決めるということをしています．閾値との比較をしている部分は，$x_0 = 1$という固定した入力を仮定し，この入力に対する重みを$w_0 = - \theta$とすることで，その他の入力の重み付き和に組み込むことができます．これは$d$次元の特徴空間上で，$g(\bm{x}) = w_0+w_1 x_1+\dots+w_d x_d = 0$という識別超平面を設定し，入力がこの識別超平面のどちら側にあるのかを計算していることと等価になります．

もし，与えられた学習データが特徴空間上で線形分離可能ならば（超平面で区切ることができるならば），以下に示す
パーセプトロンの学習アルゴリズム
で，線形分離面を見つけることができます．

%ID:0507

ここで，$\bm{x}$は特徴ベクトルに$x_0=1$を加えた$d+1$次元ベクトル，$\bm{w}$は$d+1$次元の重みベクトルとします．
また，$\eta$は学習係数で，適当な小さい値を設定します．


%ID:なし
アルゴリズム8.1中の重みの修正式$ \textcircled{1}$は5.3.5節で説明する確率的最急勾配法と同じ式になっています．
すなわち誤り訂正学習は，データが誤識別されたときにのみ重みを修正する
確率的最急勾配法の特殊なケースとみることができます．

%ID:0507 (2つ目)

このアルゴリズムは，学習データが線形分離可能な場合には必ず
識別境界面を見つけて停止します．これを
パーセプトロンの収束定理
とよびます．一方，
学習データが線形分離不可能な場合にはこのアルゴリズムを適用することが
できません．全ての誤りがなくなることが学習の終了条件なので，データが線形分離不可能な場合は
このアルゴリズムは停止しません．

%ID:0508

前節の誤り訂正学習は，学習データが線形分離可能であることを前提にしていました．しかし，現実の
データではそのようなことを保証することはできません．むしろ，線形分離不可能な場合の方が多いと
思われます．

そこで，統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の
良さを定量的に表して，それを最大化するという方法で，線形分離不可能なデータにも対処することを考えます．

しかし，識別関数の「良さ」を定義するよりは，「悪さ」を定義する方が簡単なので，
識別関数が誤る度合いを定量的に表して，それを最小化するという方法を考えます．

個々のデータに対する識別関数の「悪さ」は，その出力と教師信号との差で表すことができます．
しかし，データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，
出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます．そこで，識別関数の出力と
教師信号との差の2乗を，全データに対して足し合わせたものを識別関数の「悪さ」と定義し，この量を
二乗誤差と呼びます．この二乗誤差を最小にするように識別関数を調整する方法が，最小二乗法による学習
です．

%ID:0509

求める関数を線形であると仮定すると，式(5.5)のように表現できます．

この式の係数$\bm{w}$を学習データから推定します．推定の基準として，式(5.5)で
算出された出力と，教師信号との誤差がなるべく少なくなるようにします．
誤差は式(5.5)の係数$\bm{w}$の値によって決まるので，$E(\bm{w})$と表現し，
以下の式で求めます．

ここで，扱いにくい総和演算を消すために，学習データを行列で，教師信号をベクトルであらわします．

$d$次元列ベクトルの学習データ$\bm{x}_i$を$N$個横に並べた行列を$\bm{X}$と表し，
教師信号$y_i$の値を並べた列ベクトルを$\bm{y}$，係数を並べた列ベクトルを$\bm{w}$とすると，
誤差は以下のようになります．

この値が最小になるのは，$\bm{\beta}$で微分した値が0となる極値をとるときなので，
求める係数は以下のようになります．

すなわち，二乗誤差を最小にする重み$\bm{w}$は，学習データから解析的に求めることができます．
また，学習データ数や特徴の次元数が大きく，逆行列を求めることが困難な場合は，5.3.5項で説明する
確率的最急勾配法を用いて，重み$\bm{w}$を学習します．

%ID:0510

次に，この識別関数法の考え方を確率モデルに適用する，識別モデルの考え方を説明します．

第4章で説明したように，データから直接に頻度を数えて事後確率$P(\omega_i|\bm{x})$を求めることはできません．
そこで，識別モデルでは，この事後確率を特徴値の組み合わせから求めるようにモデルを作ります．
つまり，特徴ベクトル$\bm{x$}が与えられたときに，その$\bm{x}$の値を用いて，何らかの方法で，
出力$y$の確率分布を計算するメカニズムをモデル化します．

いま，2値分類問題における特徴ベクトル$\bm{x}=(x_1, \dots, x_d)^T$に対して，各特徴の重み付き和
$w_1 \cdot x_1 + \dots + w_d \cdot x_d$を考え，正例に関しては正の値，負例に関しては負の値を出力するように
重みを調整することを考えます．ただし，これでは原点$\bm{x} = \bm{0}$に対して
判定ができないので，定数$w_0$をパラメータとして加え，改めて$g(\bm{x})=w_0 + w_1 \cdot x_1 + \dots + w_d \cdot x_d 
= w_0 + \bm{w} \cdot \bm{x}$

と定義します．

ここで，$g(\bm{x})=0$とおいたものは，式の形から$d$次元空間上の平面を表しています．もし，この平面が上記のように
ふるまうように調整ができたとすると，この平面上にある点は，どちらのクラスとも判別がつかず，
平面の正の側（$g(\bm{x})>0$となる側）の空間に正例，平面の負の側（$g(\bm{x})<0$となる側）の空間に負例の空間ができるはずです．
このように，特徴空間上でクラスを分割する面を
識別面
とよびます．
また，それぞれの点の判定の確からしさは，識別面からの距離に反映されます．

%ID:0511

ただし，このままでは，$g(\bm{x})$は$\bm{x}$の値次第で，極端に大きな（あるいは小さな）値となる可能性があり，確率と対応づけることが難しくなります．$g(\bm{x})$の望ましいふるまいは，出力範囲が0以上1以下ので，正例に属する$\bm{x}$には1に近い値を，負例に属する$\bm{x}$には0に近い値を出力することです．このようなふるまいは，変換$1/(1+e^{-g(\bm{x})})$を行い，
これを式(5.10)に示すように事後確率$p(\oplus|\bm{x})$（ただし，$\oplus$は正のクラス）と対応付けることで実現できます．

この場合，$\bm{x}$が負のクラスになる確率は$p(\ominus|\bm{x}) = 1 - p(\oplus|\bm{x})$（ただし，$\ominus$は負のクラス）
で求められます．

式(5.10)はシグモイド関数（図5.6）とよばれるもので，$g(\bm{x}) = w_0+\bm{w}\cdot\bm{x}$がどのような値をとっても，シグモイド関数の値は0から1の間となります．また，$g(\bm{x})=0$のとき，式(5.10)の値は0.5となり，これは
確率を表現するのに適しています．

%ID:0512

ロジスティック識別器は重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデルとみなすことができます．
そして，このモデルに学習データ$D$中の$\bm{x}_i$を入力したときの出力を$o_i$とします．望ましい出力は，正解情報$y_i$です．2値分類問題を仮定し，正例では$y_i=1$，負例では$y_i=0$とします．

作成したモデルがどの程度うまく学習データを説明できているか，ということを評価する値として，尤度を式(5.11)
のように定義します．

正例のときは$o_i$がなるべく大きく，負例のときは$1-o_i$がなるべく大きく（すなわち$o_i$がなるべく小さく）なるようなモデルが，よいモデルだということを表現しています．

尤度の最大値を求めるときは，計算をしやすいように対数尤度にして扱います．

最適化問題をイメージしやすくするために，この節では，対数尤度の負号を反転させたものを誤差関数$E(\bm{w})$と定義し，以後，誤差関数の最小化問題を考えます．

これを微分して極値となる$\bm{w}$を求めます．モデルはロジステック識別器なので，その出力である$o_i$はシグモイド関数で与えられます．

シグモイド関数の微分は以下のようになります．

モデルの出力は重み$\bm{w}$の関数なので，$\bm{w}$を変えると誤差の値も変化します（図5.7）．このような問題では，
最急勾配法
によって解を求めることができます．最急勾配法とは，
最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法です．この場合は
パラメータ$\bm{w}$を誤差の$E(\bm{w})$の勾配方向へ少しずつ動かすことになります．この「少し」という量を，学習係数
$\eta$と表すことにすると，最急勾配法による重みの更新式は式(5.16)のようになります．

%ID:0513

そして，誤差$E(\bm{w})$の勾配方向の計算は以下のようになります．$x_{ij}$は，$i$番目の学習データの$j$次元目の値です．

したがって，重みの更新式は以下のようになります．

最急勾配法は，重みの更新量があらかじめ定めた一定値以下になれば終了です．

%ID:0514

ここで説明した最急勾配法は，最適化問題によく用いられる手法ですが，いくつか欠点もあります．

まず，式(5.17)からわかるように，全データに対する誤差を計算してから重みを更新するので，
一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．

また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解
に陥りやすいといわれています．対処法としては，初期値を変えて何回か試行するという方法が取られていますが，
データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります．

これらの問題に対処するために，重みの更新を全データで一括におこなうのではなく，ランダムに学習データを一つ選び，その学習データに対して重みを更新する
という方法が考えられます．この方法を
確率的最急勾配法
とよびます．重みの更新式は，式(5.18)のようになります．

%ID:なし

この方法は，新しくやって来る学習データに対して継続的に学習を続けてゆく，
オンライン学習
にも適用できます．

ただし，$\eta$が定数の場合，いつまでたっても重みが更新され続け，収束の判定ができません．したがって，学習の経過とともに$\eta$の
値を徐々に0に近づけてゆく工夫が必要です．

この方法は，すべてのデータをメモリに蓄えて計算する必要がないので，大規模データを扱う場合に適します．
また，$\eta$を比較的大きな値から減少させてゆくという方法をとるため，局所的最適解を避けられる場合が多い
といわれています．

本章では，確率的識別手法における生成モデルと識別モデルについて説明しました．
ナイーブベイズは生成モデルの一種で，データとクラスの同時確率から事後確率を計算します．一方，ロジスティック識別は識別モデルの一種で，事後確率を直接推定します．

文献\cite{sugiyama09}，\cite{sugiyama13}は，同じ著者が，機械学習の考え方を生成モデル・識別モデルそれぞれの観点から
解説したもので，これらのモデルの基礎になっている考え方の違いや，実現される識別手法の違いなどに
ついて理解を深めることができます．
