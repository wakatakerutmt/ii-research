--------- はじめに　記号の意味と注意（以下Xは数字を表します） -------------
%ID:XXXX(4桁の数字) ・・・IDがXXXXのスライドに対応したテキスト　文章が途中で分かれていて（2つ目）などと表記があってもIDはそのままです。

%ID:なし　　　　     ・・・スライドに直接関係のない、繋ぎや補足の説明。この「ID:なし」の部分から解答を取ってこないようにしてください。
% figure X.X        ・・・図番号。本来ならここに図が入ります、という意味だけで無視して大丈夫です。解答に含めないようにお願いします。
% equation X.X などその他％から始まる記号 ・・・上記同様、解答に含めないようにお願いします。
$\theta$ など       ・・・数式、引用などのTex記号も無視で、解答に含めないようにお願いします。
\cite{mitchell97}

---------　以下からスライドに対応したテキストデータ ------------------


%ID:1101

ここからは，教師なし学習の問題に取り組みます．
この章では，特徴ベクトルの要素が数値である場合に，その特徴ベクトルが生じるもとになった
クラスを推定するモデル推定の問題を扱います(図11.1)．

%ID:1102

教師なし学習とは，正解情報が付けられていないデータを対象に行う学習です．
データの集合を，以下のように定義します．

この章では，この特徴ベクトルの要素がすべて数値である場合を考えます．
要素がすべて数値であるということは，特徴ベクトルを$d$次元空間上の点として考えることができます．
そうすると，モデル推定は，特徴空間上にあるデータのまとまりを見つける問題ということになります．データがまとまっている，
ということは，共通の性質をもつようにみえる，ということなので，図11.2のように，このような個々のデータを生じさせた共通の性質を持つクラスを見つけることが目標になります．

%ID:1103

与えられたデータをまとまりに分ける操作を，クラスタリングといいます．「まとまりに分ける」という部分を
もう少し正確にいうと，

分類対象の集合を，内的結合と外的分離が達成されるような部分集合に分割すること

になります．つまり，一つのまとまりと認められるデータは相互の距離がなるべく近くなるように（内的結合），
一方で，異なったまとまり間の距離はなるべく遠くなるように（外的分離），データを分けるということです．

このようなデータの分割は，個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る
階層的手法と，全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく
分割最適化手法とに分類できます．

%ID:1104

階層的手法の代表的手法である
階層的クラスタリング
は，近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです（図11.3）．

アルゴリズムとしては，一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しい
クラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了です．

%ID:1105

階層的クラスタリングの手順を，アルゴリズム11.1に示します．ここで，simはクラスタ間の類似度を計算する関数です．

%ID:1104 (2つ目)

図11.4のように，データまたはクラスタを融合
する操作を木構造で記録しておけば，全データ数$N$から始まって，1回の操作で
クラスタが一つずつ減ってゆき，最後は一つになるので，任意のクラスタ数からなる結果を得ることができます．

%ID:1106

ここで，クラスタ間の類似度計算に関して，いくつか異なる計算方法があり，そのいずれを採用するかによって，
出来上がるクラスタの性質に違いが生じます．

\begin{itemize}
\item 単連結法(SINGLE)\\
最も近い事例対の距離を類似度とする．クラスタが一方向に伸びやすくなる傾向がある．
\item 完全連結法(COMPLETE)\\
最も遠い事例対の距離を類似度とする．クラスタが一方向に伸びるのを避ける傾向がある．
\item 重心法(CENTROID)\\
クラスタの重心間の距離を類似度とする．クラスタの伸び方型は単連結と完全連結の間をとったようになる．
\item Ward法(WARD)\\
与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする．比較的良い結果が得られることが多いので，階層的クラスタリングでよく用いられる類似度基準である．
\end{itemize}

%ID:なし

この例題では，類似度計算手法を重心法(CENTROID)にすると比較的うまくゆきますが，単連結法(SINGLE)
にすると，50:99:1という非常にアンバランスなクラスタを形成します．完全連結法(COMPLETE)でも，
50:73:27と，重心法(CENTROID)とはかなり異なる結果になります．このように階層的クラスタリングは，
クラスタリング結果が類似度計算手法に大きく影響される手法であるといえます．

%ID:1107

階層的クラスタリングはボトムアップ的にデータをまとめるので，全体的な視点からみると，いびつなクラスタを
形成してしまうことがあります．一方，全体的な視点でまとまりのよいクラスタを求める手順が，
分割最適化クラスタリングです．

分割最適化クラスタリングでは，データ分割のよさを評価する関数を定め，その評価関数の値を最適化することを
目的とします．ところが，データ数を$N$とすると，データを分割する場合の数は，$N$に対して指数的（例えば二つのクラスタに
分ける場合なら$2^N$）になるので，$N$がちょっと大きな数になると，すべての可能な分割の評価値を求めるのは
実質的に不可能になります．

このような場合の常套手段として，探索によって準最適解を求めるということを考えます．

%ID:1108

分割最適化クラスタリングの代表的手法である
k-meansクラスタリング (k-平均法)
では，クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを
最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
（図11.6）．

%ID:1109

ここで，データ分割のよさを評価する関数を，各データと所属するクラスタの中心ベクトルとの距離の総和と定義すると，クラスタ中心の位置更新によって
評価関数の値が増えることはありません．したがって，この手順によって局所的最適解にたどり着くことが
できます．わざわざ「局所的」といっているということは，全体としての最適解であるかどうかは，この手順ではわからない
ということです．したがって，k-means法でクラスタリングをおこなう際は，異なった初期値で複数回おこない，評価値（たとえば，上記の距離の総和）の
もっともよいものを結果として採用することになります．

%ID:なし

上記の例題は2次元データなので，比較的うまくゆきましたが，一般的に
k-meansアルゴリズムは，初期値によって求まるクラスタが異なるという問題点と，クラスタ数$k$の値を決めるのが難しい
という問題点があります．初期値によって結果が不安定になる問題点は，複数の初期値で試すことである程度回避できますが，
最適解が含まれているという保証はありません．
そこで，初期クラスタ中心の選び方の改善法として，データ中の最も離れた
データ組を最初の二つのクラスタ中心とし，これ以降は，設定されたクラスタ中心から
最も遠いデータを新たなクラスタ中心として，一つずつ初期クラスタを追加してゆく方法
（KKZ法）や，これまでに設定されたクラスタ中心からの距離に基づく確率分布
を用いて新たなクラスタ中心とする方法（k-means++法）も提案されています．

また，クラスタ数$k$の値に関しては，$k$が少なすぎるとクラスを構成すること
ができず，逆に$k$が多すぎると個々のデータにマッチしてクラスタリングの目的に合わない結果になってしまいます．そこで次項では，この$k$を自動的に決める方法を紹介します．

%ID:1110

k-Meansアルゴリズムにおける，事前にクラスタ数$k$を固定しなければいけないという問題点を回避する手法として，
クラスタ数を適応的に決定する方法が考えられます．最初は2分割から始まって，得られたクラスタに対して分割が適当
でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもので，この方法を
X-meansアルゴリズム
とよびます．

このアルゴリズムの勘所は，さらなる分割が適当であるかどうかの判断k基準として，
分割前後でモデルのよさを定量的に表したBIC (Bayesian information criterion) 値を比べるという点です．

BICは以下の式で得られる値です．

ただし，$\log L$はモデルの対数尤度（各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの）$q$はモデルのパラメータ数（クラスタ数に比例），$N$はデータ数です．BICは小さいほど，得られた
クラスタリング結果がデータをよく説明しており，かつ詳細になりすぎていない，ということを表します．

モデルを詳細にすればするほど（クラスタリングの場合はクラスタ数を増やせば増やすほど），対数尤度を大きくする
ことができます．極端な場合，1クラスタに1データとすると，そのクラスタの正規分布の平均値がそのデータになるので，
その分布の最も大きい値をとる（BICを小さくする方向に寄与する）ことになります．しかし，その場合，$q$の値が大きくなるので，BIC全体としては大きな値となってしまい，対数尤度とクラスタ数のバランスが取れたものが選ばれるという仕組みになっています．

クラスを表すモデルのよさを定量的に表現する基準はひとつではありませんが，ここで示したBICや，同様の目的で使われるAIC(Akaike information criterion)は，モデルの対数尤度とパラメータの複雑さのバランスを取った式で，その評価値を表していることから，さまざまなモデル選択の状況で用いられています．

%ID:なし

また，scikit-learnには，ラベル伝播に基づいて，自動的にクラスタ数を決める AffinityPropagationが実装されています．AffinityPropagationは，それぞれのデータが，近くにあるデータの情報を交換することで，各データがクラスタ中心らしさを更新してゆくものです．

%ID:1111

教師なし学習の実用的な応用例として，異常検出があります．
異常検出の問題設定は，入力$\{\bm{x}_i\}$に含まれる異常値を，教師信号なしで見つけることです．
ここでは，最も基礎的な異常検出として，外れ値の検出について説明します．

外れ値は，学習データに
含まれるデータの中で，ほかと大きく異なるデータを指します．たとえば，全体的なデータのまとまりから極端に離れた
データや，教師ありデータの中で，一つだけほかのクラスのデータに紛れ込んでしまっているようなデータです．
これらは，計測誤りや，教師信号付与作業上でのミスが原因で生じたと考えられ，学習をおこなう前に
除去しておくのが望ましいデータです．

%ID:1112

ここで紹介する
局所異常因子 (LOF: Local Outlier Factor)の考え方は，
単純にいうと，近くにデータがないか，あるは極端に少ないものを外れ値とみなす，というものです．
ただし，この「近く」という概念は，データの散らばり具合によって異なるので，一定の閾値をあらかじめ定めておくことはできません．
そこで，それぞれのデータにとっての「周辺」を，$k$番目までに近いデータがある範囲と定義し，周辺にあるデータまでの距離の平均を，「周辺密度」として定義します（図11.13）．

そして，あるデータの周辺密度が，近くの$k$個のデータの周辺密度の平均と比べて極端に低い
ときに，そのデータを外れ値とみなします．

%ID:1113

まず，データの個数に応じて$k$を適当な値に定めます．その$k$を用いて，あるデータ$\bm{x}$から，別のデータ$\bm{x}'$への
到達可能距離(reachability distance)を式(11.3)のように定義します．

ここで，$\bm{x}^{(k)}$は，$\bm{x}$に$k$番目に近いデータです．式(11.3)で求まる値は，
$\bm{x}$と$\bm{x}'$が十分に遠ければ，通常の距離です．一方，$\bm{x}'$が$\bm{x}^{(k)}$よりも$\bm{x}$に近ければ，
$|| \bm{x}-\bm{x}^{(k)} ||$に補正されます．

次に，この到達可能距離を用いて，$\bm{x}$の周辺密度を定義します．式(11.4)で定義される量を，
局所到達可能密度 (local reachability density)
とよびます．

$LRD_k(\bm{x})$は，$\bm{x}$から$k$番目までに近いデータとの到達可能距離の平均を求め，その逆数をとったものです．
$k$番目までのデータが近くにあるとき，到達可能距離の平均は小さい値になるので，その逆数である
局所到達可能密度は大きい値になります．

ここで，到達可能距離ではなくなぜ単純に距離を用いないのか，という疑問が出るかもしれません．
これは，単純な距離を用いると$\bm{x}$のごく近い距離にデータがあるときに，その逆数はとても
大きな値になってしまって，安定的な計算ができなくなる可能性があるからです．

そして，この局所到達可能密度を用いて，式(11.4)のように$\bm{x}$の
局所異常因子 $LOF$
を定義します．

$LOF_k(\bm{x})$は，$\bm{x})$に対して$k$番目までに近いデータの局所到達可能密度の平均と，
$\bm{x})$の局所到達可能密度の比です．
$k$番目までに近いデータの局所到達可能密度の平均値に比べて，$\bm{x}$の局所到達可能密度の
値が極端に低い場合，$LOF_k(\bm{x})$は大きな値をとって，$\bm{x}$が外れ値であることを示唆します．
一方，これらの値に大きな違いがないとき，$LOF_k(\bm{x})$は1に近い値となって，
$\bm{x}$が正常なデータであることを示します．

異常検出は，外れ値の検出を応用してセンサー入力から機械の故障を予知したり，
イベント系列特徴を入力して，クレジットカードの不正使用を検出したりする応用が考えられています．

%ID:1114

ここでは，ここまでに説明してきたクラスタリングの結果を用いて，新たなデータが観測されたときに
そのデータが属するクラスタを決める，という問題を考えます．つまり，教師なし学習で
識別器を作りたい，ということと同じです．

k-meansアルゴリズムの結果は，各クラスタの平均ベクトルなので，新しく観測した
データと，各クラスタの平均ベクトルとの距離を求めて，最も近いクラスタに分類する
という手法は，簡単に思いつきます．しかしこの距離計算は，全クラスタの分散が等しい
ことを前提にしており，クラスタ毎にデータの広がり方が異なる場合は，
適切な結果になりません．また，クラスタの事前確率も考慮されていません．

第4章で説明したように，事後確率最大となる識別結果は，
事前確率と尤度（各クラス毎の確率密度関数が観測されたデータを生成する確率）の積を
最大とするクラスとなるので，よい識別器を作るためにはこれらの確率が必要です．

事前確率はクラスタリング結果のデータ数の分布から求まります．そこで，
尤度を計算するための確率モデルを，与えられた教師なし学習データから
求めるという問題設定で，その解決法を考えてゆきましょう．

k-means法を代表とする分割最適化クラスタリングは，平均値のみを推定していたので，
この考え方を一般化します．各クラスタの確率分布の形を仮定して，そのパラメータを学習データから推定する
という問題を設定します．確率分布として式(11.6)のような正規分布（ガウス分布）を仮定すると，教師なし学習データから，クラスタ$c_m$の平均$\bm{\mu}_m$と分散$\bm{\mu}_m$を推定する問題になります．

k-means法では，各データはいずれかのクラスタに属していました．一方，
この方法の場合は，どの$\bm{x}$に対しても，すべてのクラスタが式(11.6)に基づいて，そのデータがそのクラスタから生成された確率を出力します．すなわち，
個々のデータはどのクラスタに属するかを一意に決めることができず，クラスタ1に属する確率が
0.2，クラスタ2に属する確率が0.8，といった表し方になります（図11.16）．このような表現を
混合分布
による表現といいます．

%ID:1115

このような設定で，k-meansアルゴリズを一般化すると，以下のように考えることができます．

\begin{itemize}
\item $k$個の平均ベクトルを乱数で決める\\
 $\Rightarrow$ $k$個の正規分布を乱数で決める
\item 平均ベクトルとの距離を基準に，各データをいずれかのクラスタに所属させる\\
 $\Rightarrow$ 各分布が各データを生成する確率を計算し，それを帰属度として，各クラスタにゆるやかに所属させる
\item 所属するデータをもとに平均ベクトを再計算する\\
 $\Rightarrow$ 各データの帰属度をデータの重みとみなして，各分布のパラメータを再計算する
\end{itemize}

たとえば，クラスタ1に属する確率が0.2であるデータは，クラスタ1の平均ベクトルの再計算の時には0.2個分として計算することになります．
分散の計算に関しても同様です．

%ID:1116

このように，ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズムを
EM アルゴリズム(Expectation-Maximization)
とよびます．

%ID:なし

Eステップのガウス混合モデルによる尤度計算は，以下のようにおこないます．

この各データの尤度を用いた分布パラメータ再計算は，以下のようになります．

本章では，数値特徴に対する「教師なし・モデル推定」に対して，クラスタリングと確率密度推定による
学習手法を説明しました．教師なし学習の評価は難しいのですが，モデル推定問題に関しては，学習された
モデルがどれだけ正しくデータを説明できているか，という観点での評価が可能です．

その際，モデルの複雑度が固定されていれば（たとえばk-meansアルゴリズムでkが固定されていれば），
単純にモデルがどれだけうまくデータに当てはまっているかを調べればよいのですが，モデルの複雑度を
変えられるアルゴリズムの場合，「できるだけ単純なモデルで，よく当てはまるもの」という矛盾する
条件をうまく満たすものを探すという，難しい問題になります．
