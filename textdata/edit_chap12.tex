--------- はじめに　記号の意味と注意（以下Xは数字を表します） -------------
%ID:XXXX(4桁の数字) ・・・IDがXXXXのスライドに対応したテキスト　文章が途中で分かれていて（2つ目）などと表記があってもIDはそのままです。

%ID:なし　　　　     ・・・スライドに直接関係のない、繋ぎや補足の説明。この「ID:なし」の部分から解答を取ってこないようにしてください。
% figure X.X        ・・・図番号。本来ならここに図が入ります、という意味だけで無視して大丈夫です。解答に含めないようにお願いします。
% equation X.X などその他％から始まる記号 ・・・上記同様、解答に含めないようにお願いします。
$\theta$ など       ・・・数式、引用などのTex記号も無視で、解答に含めないようにお願いします。
\cite{mitchell97}

---------　以下からスライドに対応したテキストデータ ------------------


%ID:1201

パターンマイニングはデータマイニングともよばれ，ビッグデータ活用の一つとして注目を集めているものです．
パターンマイニングの応用例としては，ネットショッピングサイトなどでの「おすすめ商品」の提示

や，データからの連想規則（あるいは相関規則）の抽出による新たな知見の獲得などが試みられています．

この章では，まず，パターンマイニングの基本的な手法であるApriori（アプリオリ）アルゴリズムとその高速化版であるFP-Growthについて説明します．次に，問題設定を推薦システムに絞り，協調フィルタリングとMatrix Factorizationについて概説します．


この章で扱う問題は，カテゴリからなる特徴ベクトルに対して，正解が付与されていない状況（すなわち「教師なし」の状況）で，そのデータに潜んでいる有用なパターンを見つけてくる，というものです（図12.1）．一般にパターンマイニングとよばれます．

% figure 12.1

パターンマイニングの基本技術は
頻出項目抽出
です．これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．この頻出項目から，
連想規則抽出
をおこなうことができます．

%ID:1202

また，この章で扱うカテゴリ特徴は，あるユーザがある商品を「購入した」／「購入しなかった」のような2値に縮約させて扱うことが可能なことが多いので，これらを数値1, 0と置き換えると，学習データ全体を，値がまばらに入っている巨大な行列とみなすことができます．この行列に対して，次元削減手法を適用して，役に立つ情報を抽出することも，パターンマイニングの一部として扱います．

%ID:1203

パターンマイニングで扱うデータは，一般的には\ruby{疎}{まばら}なデータです．典型的なものはスーパーマーケットの売上記録で，
そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．
そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録をトランザクションとよびます．
スーパーで揃えている商品の種類数（こちらは数千から数万）に比べて，一回の買い物で客が買う商品の種類数（こちらはせいぜい数十）は桁違いに少ないので，各トランザクションでは特徴ベクトルのほとんどの次元の値が空白で，ごく小数の次元のデータが埋まっている状況になります．このようなデータを疎なデータとよびます．

イメージしやすいようにごく小さな例を示しましょう．商品点数が$\{ミルク, パン, バター, 雑誌\}$の4点，トランザクションが6件の
データを表12.1に示します．
ここで，t は各トランザクションにおいて，その商品が買われたことを意味します．以下，商品を項目とよびます．

% table 12.1

ざっと見たところ，パンを買った人が多いようです．また，ミルクとパンが一緒に売れることも多いようです．
「多いようだ」「少ないようだ」という定性的な判断ではなく，定量的な基準を決めて，その基準を超えるパターンを
見つけましょう．

データ中によく現れるということは，「全データに対して，ある項目集合が出現する割合が一定以上である」と解釈します．
これを
支持度 (support)
とよび，式(12.1)で計算します．ただし，$T$は全トランザクション件数，
$T_{\mbox{items}}$は項目集合 items が出現するトランザクション件数です．

% equation 12.1

表12.1の項目集合$\{ミルク, パン\}$の支持度は，$\mbox{support}(\{ミルク, パン\})= 3/6 =0.5$となります．

%ID:1204

ここでやっていることは，項目集合を作って数えれば終わりなので，あまり難しいことをしているようには見えません．
ところが，この「項目集合を作って」というところが厄介なのです．実際のスーパーマーケットの売上記録を対象として
特徴ベクトルを構成すると，数千次元を超えるものになります．低く見積もって1,000種類の商品点数だとしても，
可能な項目集合の数は$2^{1000}-1$となります．
これは，まともにすべての可能な組合せに
ついて計算することは難しい状況です．

したがって，重要なことは，支持度を計算する項目集合をいかにして絞り込むか，ということになります．

%ID:1205

ここでもう一度，表12.1の小規模データに戻って，項目集合を絞り込む方法を考えましょう．今度は図示しやすいように，
項目に通し番号を付けて，$\{0,1,2,3\}$と表します．すべての可能な項目の組合せは$2^4-1=15$で，図12.2に丸で示すもの（ただし，最上段の空集合$\emptyset$は除く）になります．

% figure 12.2

ここで，a prioriな原理
として，

ある項目集合が頻出ならば，その部分集合も頻出である

を考えます．

%ID:1206

これは，命題論理でいうところの "AならばB" という形をしています．
命題論理では，"AならばB" が成り立つならば（真ならば），その対偶である "BでないならばAでない" も必ず
成り立ちます．

%ID:1205 (2つ目)

そうすると，上で述べたa prioriな原理から，

ある項目集合が頻出でないならば，その項目集合を含む集合も頻出でない

が成り立ちます．

%ID:1207

図で表すと，図12.3のようになります．

% figure 12.3

この a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法が，
Aprioriアルゴリズム
です．

%ID:1208

アルゴリズム12.1 に，Aprioriアルゴリズムの手順を示します．ここで，$F_k$は要素数$k$の頻出項目集合，$C_k$は$F_{k-1}$の要素を組み合わせて作られる頻出項目集合の候補です．また，あらかじめ抽出する項目集合の支持度の閾値を決めておき，それを超えるものを頻出項目集合とします．

% algorithm 12.1

%ID:1209

この節では，教師あり／教師なしのそれぞれのデータから規則を学習する方法を考えてみます．規則の学習では，学習データが教師ありか教師なしかで，問題設定や学習アルゴリズムが異なります．

%ID:なし

ここでの教師ありデータの典型例は，表3.3のGolfデータのようなもので，目標は，正例 (yes) を判定する規則を「IF 条件部 THEN yes」のような形式で得ることです．ある正例が，この条件部にあてはまるとき，この事例はこの規則にカバーされると表現します．

教師ありデータに対する規則学習は，まず，なるべく多くの正例をカバーする規則を導き，その規則によって正例と判断された学習データを取り除きます．そして，残りのデータに対して，さらにその中のなるべく多くの正例をカバーする規則を導くという操作を，正例がなくなるまで再帰的に繰り返します．この手順を
カバーリングアルゴリズム
(covering algorithm) とよびます．

規則学習の代表的アルゴリズムであるRipperでは，「IF 条件部 THEN yes」という規則を，条件部にリテラルをAND条件で加えてゆくことで作成してゆきます．加えるリテラルを選ぶ基準は，加える前の規則をR，
加えた後の規則をR'として，式(12.2)で定義する$\mbox{Gain}(R', R)$が最大となるものとします．

% equation 12.2

ただし，$N$は規則$R$によってカバーされる事例数，$N_+$はその中の正例数で，$N', N_+'$は規則$R'$に対する同様の数，
$s$は規則$R$によってカバーされる正例のなかで，規則$R'$によってもカバーされる正例の数です．

%ID:1209 (2つ目)

教師ありデータからの規則の学習では，結論部はclass特徴と決まっていました．
今から考える教師なしデータの場合，どの特徴（あるいはその組合せ）が結論部になるのかはわかりません．
むしろ，結果として得られた規則のそれぞれが，異なった条件部・結論部をもつことが多くなります．たとえば，「商品Aを購入
したならば，商品Bを購入することが多い」，「商品Cを購入したならば，商品Dと商品Eを購入することが多い」といった
規則になります．

%ID:1211

このような規則を作るために，まずAprioriアルゴリズムで頻出項目を抽出します．次に，その頻出項目の要素を，条件部と結論部に分けて可能な規則集合を生成
します．そして，その規則の有用性を評価し，役に立ちそうなものだけに絞り込みます．

%ID:なし

ここで考えるべき事は二つあります．

%ID:1209 (3つ目)

まず，規則の有用性をどのようにして評価するか，ということです．ここでは，確信度(confidence)とLift値を紹介します．

確信度は，規則の条件部が起こったときに結論部が起こる割合を表し，式(12.3)で計算します．この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます．

% align 12.3

リフト値は，規則の結論部だけが単独で起こる割合と，
条件部が起こったときに結論部が起こる割合との比を表し，式(12.4)で計算します．この値が高いほど，得られる情報の多い規則であることを表しています．

% align 12.4

%ID:なし

次に，評価する規則集合の数をいかにしておさえるか，という問題を考えます．要素数が$N$個の頻出項目集合に対して，そのうちの
$k$個の項目を条件部に，残りの$N-k$個の項目を結論部において規則を作るとします．条件部の$k$個を決めると，結論部は自動的に決まるので，
結局$N$個から可能な組合せをすべて作り($2^N$)，規則として意味のないもの（条件部・結論部のいずれかに要素を1つも含まないもの）
を除くと，$2^N-2$個の規則に対して評価を行うことになります．頻出項目集合が$\{0,1,2,3\} (N=4)$の場合の可能な規則集合を
図12.5に示します．

% figure 12.5

ここでの項目数$N$は，前節の頻出項目抽出ほどには大きくならないことが期待されますが，それでも学習アルゴリズム中に，
規則毎に全トランザクション集合を対象として評価値を計算する処理が入ってくる以上，少しでも調べる数を減らす必要があります．

%ID:1212

ここでも a priori原理に基づき，評価値の高い規則を絞り込むことを試みます．以下，評価値を確信度とした場合について説明します．

ここでの原理は，

「ある項目集合を結論とする規則の確信度が高ければ，その部分集合を結論とする規則の確信度も高い」

を考えます．

これは
「商品Aを購入したならば，商品Bと商品Cを購入する」という規則の確信度が高ければ，
「商品Aを購入したならば，商品Bを購入する」という規則の確信度も高い，という当たり前のことです．

その対偶は

「ある項目集合を結論とする規則の確信度が低ければ，その項目集合を含む項目集合を結論とする規則の確信度も低い」

となります．少しややこしいですが，図で表すと，図12.6のようになります．

% figure 12.6

%ID:なし

そして，連想規則を学習するアルゴリズムはアルゴリズム12.2のようになります．
ここで，$F_k$は要素数$k$の頻出項目集合，$H_m$は要素数mの結論部の集合です．
また，あらかじめ抽出する連想規則の確信度の閾値を決めておきます．

% algorithm 12.2

% algorithm 12.3


%ID:1214


前節で説明したAprioriアルゴリズムの問題点として，やはり計算量が膨大であることが挙げられます．
一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に
(候補数)×(トランザクション数) のループがあるので，この部分をなんとか減らさなければ高速化はできません．

そこで，高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対して
パターンマイニングを行うという手法が考えられました．この手法を
FP-Growthアルゴリズム
とよびます．

トランザクションデータをコンパクトにする手段として，まず特徴を出現頻度順に並べ替えます．そして頻度の高い特徴から
順にその情報をまとめると，「商品Aの購入が100件，そのうち商品Bも同時に購入が40件，商品Cも同時に購入が30件，...」のように
多数のトランザクションの情報を手短に表現できます．ただし，このような自然言語による表現はプログラムによる処理に
向かないので，この情報を木構造で保持します．

ここまでの手順を，例を用いて説明します．まず，以下のようなトランザクション集合を学習データとします．
ここで出現する文字は特徴名で，出現しているときはその値がtとなっているものとします．

% center

次に，特徴をその出現頻度順にソートし，出現頻度が低い特徴（ここでは2回以下しか現れないもの）をフィルタにかけて消去します．
出現頻度が低い特徴は，Aprioriアルゴリズムでの頻出項目抽出や連想規則抽出でも，余計な候補を生成しないために最初に除かれて
いました．

ソート，フィルタリング後の結果は以下のようになります．

% center

%ID:1215

そして，このデータから
FP-木 (Frequent Pattern Tree) 
を作成します．

FP-木は，最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
挿入アルゴリズムは以下のようになります．

% algorithm 12.4

具体的な FP-木の作成手順は図12.7のようになります．まず，$\{z,r\}$がnull($\emptyset$)を根とする
FP-木に挿入されます．次に，$\{z,x,y,s,t\}$の挿入です．まず$z$はFP-木にあるのでカウントを
1増やして2として，この$z:2$をルートとするFP-木に対して，残りの$\{x,y,s,t\}$を挿入します．
次の$x$はFP-木にないので，新たにノードを作って$z:2$につなぎます．そして，この新しい$x:1$をルートとするFP-木
に対して残りの要素を挿入する作業を再帰的に繰り返します．

% figure 12.7

できあがったFP-木に対して，特徴を見出しとするヘッダテーブルを作成し，その頻度を記録しておくとともに
FP-木に出現する同じ要素をリンクで結んでおきます（図12.8上）．特定の特徴は，自分より頻度の高い特徴の出現の有無に
応じて，複数の枝に分かれて出現します．このリンクをたどって集めた出現数は，全体のトランザクション集合での
出現数に一致します．

% figure 12.8

%ID:1216

パターンマイニングは作成したFP-木を対象に行います（図12.8下）．たとえば，頻出項目抽出で$\{x,r\}$の頻度を求めたいときは，
まずヘッダテーブルを引いて頻度の少ない方を選びます．ここでは$r$が3回，$x$が4回なので，$r$のリンクをたどりながら
頻度計算を行います．ヘッダテーブルからスタートして，最初の$r$から親を辿り，$x$が見つからないので，
このパスでは$\{x,r\}$が共起していない，ということになります．次の$r$のリンクをたどり，その親をたどると，
今度は$x$が見つかりました．その場合は，$r$の頻度をカウントに加えます．同様に最後まで$r$のリンクをたどってゆき，
親に$x$が出現するパスにある$r$の頻度を足してゆくと，最後は$\{x,r\}$の頻度になります．


%ID:1217


前節までで扱ったトランザクションデータは，それぞれのデータが1件分の売り上げを表していました．このデータを個人に対応づけてまとめると，どの個人がどの商品を購入しているのかがわかるデータになります．本節では，このようなデータを対象に，個人に対して推薦をおこなうシステムの構築をテーマに，そのための機械学習手法を説明してゆきます．

%ID:1219

協調フィルタリング
の前提は，どの個人がどの商品を購入したかが記録されているデータがあることです．そして，新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦するというのが，基本的な考え方です．

しかし，この個人別の購入データは，前節までのトランザクションデータとどうように，まばらに値が入っているデータです．購入パターンが似ているユーザを探す際に，データをベクトルとみなして，コサイン類似度による計算をおこなっても，ほとんど一致する項目数を数えているに過ぎないような状況になってしまいます．


そこで，購入データをもっと低次元の行列に分解し，ユーザ・商品の特徴を低次元のベクトルで抽出する方法が考えられました．

%ID:1220

Matrix Factorizationは，まばらなデータを低次元行列の積に分解する方法の一つです．一般に，行列分解にはSVD (Singular Value Decomposition) とよばれる方法がありますが，推薦システムにこの方法を適用しても，うまくいかないことが多いといわれています．その理由として，購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するからです．購入したものを1，しなかったものを0として行列で表現した購入データをそのまま推薦に利用すると，1が「好き」，0が「きらい」に対応するものとして扱ってしまいます．

そのようなことを避けるために，値の入っているところのみで最適化をおこなう手法が，
Matrix Factorizationです．

%ID:なし

本章では，カテゴリデータに対する教師なし学習の典型的な手法であるパターンマイニングについて説明しました．
パターンマイニングは他の教師あり学習と比べて桁違いに大きなデータを対象とすることが多く，その高速化の
ために近年開発されているテクニックは非常に高度で難解です．その手法をすべて理解していなくては何もできない，
というわけではなく，基本原理さえ理解できていれば，あとはツールをうまく使ってデータと戦うことはできます．

その際，ツールを全くのブラックボックスとして使うのではなく，「あれ，何も結果が出なかったな．確信度の
閾値を少し下げてみるか...」ぐらいの工夫はできるようになったといえるのではないでしょうか．

高速化テクニックに関しても，性能を下げずに高速化を実現したのか，何かを犠牲にして高速化を実現したのか，
その犠牲になったものは対象とする問題で本質的なものかどうか，という検討ができれば，最新の話題にもついてゆけるようになるでしょう．
