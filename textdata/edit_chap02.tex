%chap02/1

本章では，機械学習の基本的な手順を学びましょう．まず，それぞれのステップで理解しておくべき内容を解説した後で，各ステップの作業を支援してくれるツールやコマンドを使いながら，具体的なデータでその内容を説明します．学習の中身に関しては次章以降で学ぶので，この章では少し特殊な学習法である「学習しない」機械学習手法（k-NN法）を使って，機械学習全体の手順（図2.1）を説明します．

% figure 2.1

この章ではまず，WekaのExplorerインタフェースを用いて機械学習全体の流れをGUI (graphical user interface)を用いて実行する手順を説明します．その後，Pythonの機械学習ライブラリ scikit-learnで，同様の手順を実行する方法を説明します．次章以降の例題・演習問題では，これら2つの環境を必要に応じて使い分けて用います．

%chap02/2

Wekaは，機械学習を含むデータマイニング一般のアルゴリズムを実装したJavaのライブラリとして開発されました．用途に応じたGUIも複数備えています．本節では，そのGUIの一つであるExplorerインタフェースを用いて，図2.1に示した機械学習の手順を構成してみます．

%chap02/3

機械学習の第一段階はデータ収集です．購買記録からのパターンマイニングなどのように使用するデータがあらかじめ存在する場合と，自分でタスクと問題を設定して，そのために必要なデータを集める場合とがあります．教師あり学習を行う場合には，さらに正解の付与作業が必要になります．

第1章で説明したように，機械学習に用いる学習データは多次元ベクトルの集合です．この場合，ベクトルの各要素をカンマで区切り，1行に1事例ずつデータを並べてゆくというのが，最も単純な形式になります．この形式は CSV (comma separated values) 形式とよばれ，表計算ソフトやテキストエディタで表示・編集・保存ができます．ただしCSV形式では，カンマで区切られた何番目の要素が，どのような特徴を表しているのかはデータだけからはわかりません．最初に見出し行を付けるという方法も考えられますが，データの型など，もう少し情報をつけておきたいこともあります．それぞれの要素は数値なのかカテゴリなのか，カテゴリの場合はどのような値が可能なのかという情報も，自分が作成したものではないデータを扱う際には役に立ちます．CSV形式にこれらの情報をヘッダ情報として加えたものがWekaの標準データフォーマットである
ARFF (attribute-relation file format) 
形式です．

ARFF形式のデータの例として，Wekaに付属の iris.arff を図2.2に示します．これはアヤメ (iris) の種類を，その萼（がく）の長さ (sepal length) ・幅 (sepal width) ，花びらの長さ (petal length) ・幅 (petal width) の，計四つの特徴を用いて識別するための学習データです．各事例の最後には，正解情報（Iris-setosa, Iris-versicolor, Iris-virginicaのいずれか）が付いています． 

% figure 2.2

ARFF形式のファイルにはデータセット名，特徴の情報，学習データを，この順で記述します．それぞれの記述方法について表2.1にまとめます．

% table 2.1

{\tt \%}で始まる行はコメントです．Weka付属のデータのいくつかには，コメントとしてそのデータの作成元の情報，関係論文，特徴の詳細説明などが書かれています．

{\tt @relation}で指定するデータセット名は，ほかのデータ集合と区別する目的で，英数字または記号を用いて記述します．

{\tt @attribute}では，特徴名とその型を指定します．特徴の型は，{\tt numeric}（数値），カテゴリ，{\tt date}（日付），{\tt string}（文字列）のいずれかです，数値型として，{\tt integer} や {\tt real} と書かれているデータもありますが，いずれも {\tt numeric} と見なされます．カテゴリ型の場合は，カテゴリとして可能な値を {\tt \{yes, no\}} のようにカンマ区切りで並べて波括弧で囲みます．識別問題の正解情報は，このカテゴリ型を用いて，特徴名として {\tt class} と指定することが一般的です．

{\tt @data} と書いた次の行から，学習に用いるデータを1行に1事例のCSV形式で記述します．

%chap02/0

Wekaにはirisデータ以外にも，表2.2のようなさまざまなデータが，ARFF形式で用意されています．

% table 2.2

本書ではこれらのデータを演習問題で用いますが，その際は特徴と正解情報の組合せに注目して使用してください．特徴（次元数，データの散らばり具合，スパース性，欠損値の有無などの情報も含めて）と正解情報の組合せを見ただけで，適用すべきアルゴリズムが浮かぶようになれば，機械学習初心者卒業といえるでしょう．

%chap02/0

ここからWekaを用いて，irisデータに対する学習をおこなってみましょう．Wekaを起動すると，図2.3に示す起動画面が表示されます．この起動画面から Explorer と表示されたボタンをクリックして，Explorer インタフェースを起動します（図2.4）．

% figure 2.3

% figure 2.4

Explorer インタフェースは，Explorer（探検者）という名前のとおり，あるデータに対して，あるアルゴリズムが適用できるのか，学習時間はどれくらいかかりそうか，ある程度意味のある結果は得られそうか，などをアルゴリズムやパラメータを変化させて，手探りで動かしてみるための環境です．図2.1で示した作業を，複数のタブを渡り歩いて行います．

ここでは，データの読み込みと前処理を Preprocess タブで行います．

まず，図2.4の [Open file...] というボタンをクリックし，ファイル読み込みのダイアログボックスを開いて，iris.arff を読み込みます．読み込みが完了すると，図2.4の左下にあるAttributes領域に特徴名の一覧が表示され，その右側のSelected attribute領域に，Attributes領域で選択されている特徴の統計情報（数値型の場合は最大値・最小値・平均値・標準偏差等，カテゴリ型の場合は頻度）が表示されます．

次に同じ Preprocess タブで，データの前処理を行います．

%chap02/4

ここでは，次元削減と標準化を紹介します．

次元削減
とは，特徴ベクトルの次元数を減らすことです．せっかく用意した特徴を減らすと聞くと，不思議な感じがするかもしれません．しかし一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています．これを「
次元の呪い
」とよびます．したがって，特徴ベクトルの次元削減は，より汎化能力の高いモデルを学習するという観点から，重要な前処理ということになります．

%chap02/5

ここでは，特徴数削減の手法として，
主成分分析 (principal component analysis: PCA) 
を紹介します．図2.5に，2次元から1次元への削減を例として，主成分分析の考え方を示します．

% figure 2.5

主成分分析とは，相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作です．次元削減の対象である高次元特徴空間上にデータがどのように散らばっているかという情報は，もとのデータの統計的性質をあらわす共分散行列によって表現することができるので，この共分散行列の情報を基にして，低次元空間への写像をおこなう行列を作ってゆきます．

学習データ$\{\bm{x} | \bm{x} \in D\}$の共分散行列$\Sigma$は式(2.1)を用いて計算されます．

% equation 2.1

ここで，$\bm{\mu}$は$D$の平均ベクトル，$N$は$D$の要素数です．平均ベクトル$\bm{\mu}$は式(2.2)を用いて計算されます．

% equation 2.2

図2.5左上に示すような2次元データの場合，平均ベクトルを$\bm{\mu}=(\bar{x_1}, \bar{x_2})^T$とすると，
共分散行列$\Sigma$は式(2.3)のようになります．

% equation 2.3

対角成分は，次元ごとの散らばり具合を表す分散に対応し，非対角成分は次元間の相関を表します．

次に，この共分散行列の固有値と固有ベクトルを求めると，固有値の大きい順にその対応する固有ベクトルの方向が，データの散らばりが大きい（すなわち，識別するにあたって情報が多い）方向となります．固有ベクトルどうしは直交するので，固有値の大きい順に軸として採用し，特徴空間を構成すると，たとえば上位$n$位までなら$n$次元空間が構成でき，これらはもとの多次元特徴空間のデータの散らばりを最もよく保存した$n$次元空間ということになります．特徴空間の次元数が下がれば下がるほど，学習において推定するべきパラメータ数が少なくなるので，学習結果の信頼性が高まります．もっとも，もとのデータの情報が大きく損なわれるほどに次元を削減してしまっては意味がないので，そのあたりの調整は難しいところです．主成分分析によって構成した軸では，対応する固有値が分散になるので，「すべての軸の固有値の和」に対する「採用した軸の固有値の和」の比（累積寄与率）を計算することで，次元削減後の空間が，もとのデータの情報をどの程度保存しているのか，見当をつけることができます．

%chap02/4

また，特徴の値の範囲を揃えておく
標準化 (standardization)
も，前処理としては重要な処理です．一般に，特徴はそれぞれ独立の基準で計測・算出するので，その絶対値や分散が大きく異なります．これをベクトルとして組み合わせて，そのまま学習をおこなうと，絶対値の大きい特徴量の寄与が大きくなりすぎるという問題があるので，値のスケールを合わせる必要があります．また，入力の平均値を特定の値に合わせておくと，学習対象のパラメータの初期値を個別のデータに合わせて調整する必要がなくなります．このようなことを目的として，一般的には式(2.4)に従ってそれぞれの次元の平均値を0に，標準偏差を1に揃えます．この処理を標準化とよびます．

% equation 2.4

%chap02/0

今回用いるirisデータは特徴の次元数が4であまり多くないので，次元削減は行わず，特徴間で値のスケールを合わせる標準化のみ行います．前処理は，図2.4に示したPreprocessタブ中のFilter領域で，[Choose]ボタンから個々の前処理にあたるフィルタを選択し，必要であればパラメータを調整した後，
[Apply]ボタンでそのフィルタを適用します．標準化を行うフィルタである standardization を選んで
適用した後，Selected attribute領域でそれぞれの特徴が平均値0，標準偏差1になっていることを確認してください．

次に，Visualizeタブに移動して，前処理後のデータを確認してみます．Visualizeタブでは，図2.6に示すように，読み込んだデータの分布を任意の2次元を選んで表示させることができます．表示が小さい場合は，PlotSizeでグラフの大きさを，PointSizeで点の大きさを変更することができます．いずれも，大きさを設定した後，[Update]ボタンで設定を更新する操作が必要です．

% figure 2.6

irisデータの3クラスが，色分けされて表示されています．いずれかの特徴の組み合わせで，それぞれのクラスがある程度の塊になっていれば，識別はそれなりの性能が期待できます．一方，どの組み合わせでもクラスがべったり重なっていれば，このまま識別に進んでもあまり高い性能は期待できないので，使用する特徴を見直す必要があります．

%chap02/6

それでは学習です，とゆきたいところですが，その前に学習結果の評価基準を設定します．

ここで扱っているデータはirisデータなので，教師あり・識別の場合の評価基準を考えます．この場合，学習データに対して正解率100\%でも意味がありません．未知データに対してどれだけの正解率が期待できるかが評価のポイントですが，どうやって未知データで評価すればよいのでしょうか．

学習データが大量にある場合は，半分を学習用，残り半分を評価用として分ける方法が考えられます．この方法を
分割学習法
とよびます．
評価用に半分というのは，多すぎるように見えるかもしれませんが，評価用データがあまりに少ないと，未知データの分布と全く異なる可能性が高くなり，評価そのものが信頼できなくなります．また，学習パラメータの調整をおこなうような場合では，データを学習用・調整用・評価用と分けるケースもあります．

しかし，irisデータは150事例しかないので，分割学習法で評価するのは難しそうです．このような場合，一般的には
交差確認法(cross validation method: CV法)
とよばれる方法を用いて評価します(図2.7)．この方法では学習データを$m$個の集合に分割し，そのうちの$m-1$個で学習を行い，除外した残りの一つで評価を行います．そして，その除外するデータを順に交換することで，合計$m$回の学習と評価を行います．これで，全データがひととおり評価に使われ，かつその評価時に用いられる識別器は評価用データを除いて構築されたものとなっています．$m$を交差数とよび，技術論文では交差数$m$を10とするケース (10-fold CV) や，データの個数とするケースがよく見られます．$m$がデータの個数の場合を
一つ抜き法(leave-one-out method)
とよびます．

%chap02/7

% figure 2.7

%chap02/0

評価基準の設定はClassifyタブ（図2.8）で行います．
交差確認法で評価する場合は，Test options領域で Cross-varidationにチェックを入れ，Foldsの横のテキストボックスに分割数を入力します．

% figure 2.8

%chap02/8

さて，いよいよ学習です．ここでは学習アルゴリズムとして，入力されたデータに近い学習データを近い順に$k$個選び，多数決などで所属するクラスを決定する
k-NN法 (k-nearest neighbor method) 
を使います（図2.9）．

% figure 2.9

k-NN法は，いわば学習データを集めるだけの学習法です．$k=1$の場合，識別したいデータと最も近い学習データを探して，その学習データが属するクラスを答えとします．$k>1$の場合は，多数決を取るか，距離の重み付き投票で識別結果を決めます．このk-NN法で調整するべきパラメータは，近傍としていくつまでの学習データを考えるか（すなわち$k$の値）になります．また，学習データが多い場合，効率よく近傍を探索するアルゴリズムを組み合わせることもあります．

これから機械学習を学ぼうと意気込んでいるみなさんに，最初に紹介するのが「学習しない」学習法なので，がっかりされたかもしれません．しかし，データが大量に入手・記録可能で，かつ並列で高速に近傍計算ができる現在では，k-NN法は驚くほどの性能を示すこともあります．たとえば，スマートフォンの音声対話アプリで実現されている発話理解手法の一部は，k-NN法の考え方に近いものです．

%chap02/0

Explorerインタフェースでは，Classifyタブ（図2.8）で学習をおこないます．Classifier領域の[Choose]ボタンから，k-NN法の実装である IBk を選択し，選択後に右側のテキストボックスをクリックするとパラメータ設定画面が表示されます．k-NN法の主たるパラメータは考慮する近傍の事例数$k$で，その他に距離尺度の設定や，高速計算法の設定がおこなえます．

Explorerインタフェースの左側中央にある Start ボタンを押すと，設定などに誤りがなければ，学習がおこなわれ，その結果が Classifier output 領域に表示されます．

%chap02/9

最後のステップは，学習結果の可視化です．識別結果からいくつかの評価指標の値を計算し，表やグラフとして表示します．

%chap02/0

Explorerインタフェースでは，学習が終了するとClassifier output 領域に主要な評価指標が自動的に表示されます．左下の Result list 領域には，学習を実行した時間と識別器名のペアで結果が表示されており，識別器によっては，右クリックから可視化できる情報を持つものがありますが，今回のk-NN法では， Classifier output 領域の表示内容の解釈を中心に説明します．

%chap02/9

まず，説明を単純にするために2クラス識別問題の評価法を考えます．
2クラス問題では，入力がある概念に当てはまるか否かを判定します．たとえば，ある病気か否か，迷惑メールか否か，というような問題です．設定した概念に当てはまる学習データを正例 (positve) ，当てはまらないデータを負例 (negative) といいます．
迷惑メールの識別問題では，迷惑メールが正例なので，これをpositiveと見なすのは少し変な気がしますが，惑わされないでください．あくまでも設定した概念に当てはまるか否かで，正例・負例が決まります．

さて，識別器を作成し，テストデータでその評価を行うと，その結果は表2.3で表すことができます．正例を正解+，負例を正解-，識別器が正と判定したものを予測+，負と判定したものを予測-とします．この表のことを
混同行列(confusion matrix)
あるいは
分割表(contingency table)
とよびます．

%chap02/0

% table 2.3

合計覧を除いた対角成分が正解数，非対角成分が間違いの数を示します．たとえば，正解+の行の数値は，正例50個のうち，識別器が正と判定したものが30個，負と判定したものが20個であったことを示します．この表から得られるもっとも単純な評価指標は，識別器が正しい答えを出す割合で，正解数を全データ数で割ることで求めます．表2.3の場合は$(30+40)/100=0.7$となり，この値を
正解率(accuracy)
とよびます．

%chap02/9

実は，機械学習の評価は，正解率を算出して終わり，というほど単純なものではありません．たとえば，正例に比べて負例が大量にあるデータを考えてみましょう．もし，正例のデータがでたらめに判定されていても，負例のデータがほとんど正確に判定されていたとしたら，正解率は相当高いものになります．そのような状況を見極めるために，機械学習の結果は様々な指標で評価する必要があります．有効な指標を紹介する前に，まず，混同行列の各要素に表2.4に示す名前をつけておきます．

% table 2.4

たとえば，左上の要素は，正例に対して識別器が正 (positive) であると正しく (true) 判定したので，true positive といいます．一方，右上の要素は，正例に対して識別器が負 (negative) であると間違って (false) 判定したので，false negativeとよびます．前の語が判定の成否 (true or false) を，後の語が判定結果 (positive or negative) を表します．

これらの定義を用いると，正解率 Accuracy は式(2.5)のように定義できます．

% equation 2.5

また，識別器が正例と判断したときに，それがどれだけ信頼できるかという指標を表すために，
精度(precision)
が式(2.6)のように定義されます．
% equation 2.6

さらに，正例がどれだけ正しく判定されているか
という指標を表すために，
再現率(recall)
が式(2.7)のように定義されます．

% equation 2.7

精度と再現率を総合的に判断するために，その調和平均

をとったものをF値(F-measure)とよび，式(2.8)のように定義されます．

% equation 2.8

%chap02/10

精度と再現率は一般にトレードオフの関係にあり，識別器によっては，パラメータ設定でその値を調整することができます．
たとえば，irisデータのsepallength特徴だけを用いて閾値$\theta$を設定し，入力が$\theta$以下であればIris-setosa(正例)と判定する
単純な識別器を考えてみます．sepallength特徴の値の分布は図2.10のようになり，どこに閾値$\theta$を設定しても精度・再現率ともに1となることはありません．$\theta$を小さめ(たとえば4.8)に設定すれば，精度は1ですが，再現率が悪くなります．一方，$\theta$を大きめ(例えば6.0)に設定すれば再現率は1になりますが，精度が下がります．

% figure 2.10

タスクによっては，精度を重視してパラメータを設定することもあれば，逆に再現率を重視する場合もあるでしょう．特にどちらかを重視という状況でなければ，F値で性能を測定するのが妥当です．

%chap02/11

精度あるいは再現率のどちらかを重視する場合に，閾値を変えたときの精度と再現率の関係を見ることができれば，タスクで要求される適切な設定にすることができます．このためには，
ROC曲線(ROC curve)
（図2.11）を用います．

% figure 2.11

ROC曲線は，横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたものです．

ROC曲線は必ず原点から始まり，必ず(1,1)の点で終わります．図2.11の例では，識別器のパラメータを$\theta < 4.3$で正例と
判定するように設定すれば，すべてのテストデータが負と判定され，TPR=FPR=0となるので，この識別器はROC曲線の原点に対応します．
一方，識別器のパラメータを$\theta < 8.0$で正例と判定するように設定すれば，すべてのテストデータが正と判定され，TPR=FPR=1となるので，この識別器はROC曲線の(1,1)の点に対応します．このパラメータを4.3から8.0まで小刻みに変化させてゆくと，原点から始まり，(1,1)で終わる図2.12のようなROC曲線を描くことができます．

% figure 2.12

%chap02/0

比較のために，異なる識別器のROC曲線を考えてみましょう．もしランダムに正負を出力する
識別器があれば，その正負の出力の割合を変えることで，ROC曲線は原点と(1,1)を結ぶ直線になります（図2.11の「ランダムな識別器のROC曲線」）．
一方，パラメータ調整によって100\%の正解率を達成できる完璧な識別器は，原点から
出発し，$\theta$を大きくするにつれてTPRの軸に沿って昇ってゆき，(0,1)すなわちFPR=0, TPR=1の理想的な点に達します．
その後，$\theta$を大きしてゆくと，FPRが大きくなり，(1,1)に達します（図2.11の「完璧な識別器のROC曲線」）．

通常の識別器に対する
ROC曲線はランダムな識別器と完璧な識別器の間に存在します．完璧な識別器までの近さは，ROC曲線の下側の面積である
AUR(area under ROC curve)の値で
評価することができます．完璧な識別器はAUR=1，ランダム識別器はAUR=0.5となるので，AURが1に近いほど，よい識別器である
ということができます．

%chap02/11

このように，機械学習の結果は様々な評価指標やグラフを使って評価することになります．

ここまで説明してきた2クラスの評価手法を多クラスに適用する場合は，クラス毎の精度や再現率を求め，そのクラスのデータ数に応じた割合を掛けることで，総合的な評価を行います．

%chap02/0

ExplorerインタフェースのClassifyタブ（図2.11）では，startボタンで学習・評価が行われ，右側のClassifier output領域に結果が表示されます．ここで識別に用いているk-NN法は，精度・再現率を調整するパラメータを持たないので，ここでは結果末尾の混同行列から，正解率（Correctly Classified Instances の右に正しく分類された事例数が表示され，その後の数字が正解率）・精度・制限率・F値を確認しておきましょう．

%chap02/12

プログラミング言語Pythonはオブジェクト指向スクリプト言語です．コンパイルが不要なので，短いコードを書いてその実行結果を確認しながらプログラムを組んでゆくことができます．

%chap02/13

本節では，scikit-learnパッケージを使って機械学習の手順をコーディングします．典型的な手順を図2.13に示し，それぞれについて以下の節で説明します．

% figure 2.13

scikit-learnでは，全体のパッケージであるsklearnのサブパッケージとして，データの読み込みから学習を行った結果表示までを行うための機能が提供されているので，これらのサブパッケージから必要なメソッドやクラスをコードの冒頭でimportしておきます．また，scikit-learnでは処理の高速化のためにnumpyを使用しているので，ほとんどすべてのコードでnumpyのimportが必要になります．これ以外に，グラフの表示のためのライブラリや，データ読み込みのためのライブラリも必要に応じて読み込んでおきます．

ここでは，前節で説明したものと同様の手順で学習をおこなうため，以下のライブラリを読み込みます．


scikit-learnでもWekaと同様にいくつかのサンプルデータが用意されています．前節で取り上げたirisデータを利用する際には，以下に示すコードを書きます．このコードで変数irisは，特徴ベクトル，正解データ，特徴名，データの説明などのさまざまな情報が詰まったオブジェクトになります．


サンプルデータのいくつかには，DESCR属性の値としてデータの概要・特徴の説明・統計情報などが記述されていますので，表示して内容を確認しています．
特徴ベクトルはdata属性の値として，また正解データはtarget属性の値として格納されているので，ここでは，以降の章での数式を用いた説明に合わせて，特徴ベクトルの集合をX，教師信号をyという変数に入れ直しておきます．

ここで，Xはnumpyのn次元アレイ(ndarray)で，データ数$\times$特徴の次元数からなる150行4列の行列です．このように特徴ベクトルを転置して，列方向に並べたものを
パターン行列
とよびます．
また，yもnumpyのn次元アレイで，150個の要素からなる列ベクトル（すなわち150行1列の行列）です．値はクラス名をあらわす文字列ではなく，それをクラス番号に置き換えた数値（irisデータは3クラスなので 0, 1, 2）が入っています．

ここではまず，主成分分析によって4次元データを2次元にする次元削減をおこない，散布図を出力して，識別問題がどのくらい難しいのかの見当をつけてみます．
主成分分析は，decompositionパッケージにあるPCAクラスのインスタンスを作成し，そのインスタンスの主成分分析をおこなうメソッドに対して変換したいデータを与えることで行います．削減後の次元数は，インスタンス作成時に n\_components 引数の値として与えます．

この結果を，matplotlibライブラリを用いてグラフで出力します．グラフの種類は，2次元平面上にデータを点でプロットする散布図を使います．
matplotlib.pyplotパッケージのplotメソッドを，第1引数としてx軸の値の並び，第2引数としてy軸の値の並び，第3引数として点の種類をあらわす記号を与えて呼び出すと，図2.14のようなグラフが表示されます．

% figure 2.14

plotメソッドの第1, 第2引数にあるX2[y==0,0]のような記述は，行列X2の中から，ベクトルyの値が0である行に対応する行だけ取り出し，その0列目を抜き出すということを示しています．また，第3引数の1文字目は色，2文字目は点の形を示しています．

ここでは，主成分分析の結果は，データのクラスごとのまとまり具合を見る程度にしておいて，識別にはもとの4次元データを使います．次に，パターン行列Xに対して，各特徴の平均と分散を揃える標準化処理を行います．scikit-learnでは，さまざまな前処理がpreprocessingサブパッケージに用意されています．標準化を行うメソッドはscaleです．

この処理で平均0，標準偏差1になっていることは，\texttt{X\_scaled.mean(axis=0)}，\texttt{X\_scaled.std(axis=0)}の結果を表示させて確認することができます．なお，それぞれのメソッドの引数\texttt{axis=0}は，列単位でそれぞれの処理を行うことを指示しています．

scikit-learnでは，さまざまな学習アルゴリズムを実現した識別器がクラスとして用意されています．scikit-learnにおける学習の基本的な手順
は以下のようになります．
\begin{enumerate}
\item 学習時のパラメータを引数として，識別器クラスのインスタンスを作成
\item 特徴ベクトルと教師ベクトルを引数として，fitメソッドを実行
\item 識別したい特徴ベクトルを引数として，predictメソッドを実行
\end{enumerate}

学習時のパラメータの種類やその値は，実装されているアルゴリズムによって異なりますが，一度インスタンスを作成すると，
fitメソッドで学習，predictメソッドで予測という手順は共通になります．

特定の学習アルゴリズムを交差確認法で評価するには，識別器のインスタンスを作成し，交差確認をおこなうメソッドに学習データとともに渡すという手順になります．まず，パラメータを与えて識別器のインスタンスを作成します．k-NN法は，neighborsパッケージにKNeighborsClassifierクラスとして実装されています．
ここでは，探索する近傍のデータ数は1として，インスタンスを作成します．

識別器のインスタンスの値の表示には，パラメータの値も含まれます．インスタンス作成時に指定していないパラメータがいくつも現れていますが，これはPythonでの関数呼び出しにおけるデフォルト引数の機能を使ったものです．明示的に指定していないパラメータは，デフォルトの値が適用されます．パラメータの種類やデフォルトの値は，API Referenceを読んで確認しておくようにしましょう．

学習と評価は，model\_selectionパッケージのcross\_val\_scoreメソッドを，識別器のインスタンス・学習データ・教師データ・交差数などを引数として呼び出します．戻り値のscoreは，交差数を要素数とするベクトルで，各要素は正解率を表します．

単純な正解率は，ベクトルscoreの平均で求まります．また，各交差の安定性を見るために標準偏差も合わせて表示しておきます．

結果は，95.33 +/- 6.70 \% のように表示されます．

また，混合行列を求める場合は，cross\_val\_scoreメソッドではなく，cross\_val\_predictメソッドを用いて識別器の出力を記録しておいて，
metricsパッケージのconfusion\_matrixを用いて計算します．


精度・再現率・F値はこれらの値から計算することができます．

%chap02/0

本章ではWekaのExplorerインタフェースとPythonのscikit-learnを用いて，機械学習の典型的な手順を説明しました．以後，説明の中心となるのは学習アルゴリズムです．入力データの種類や学習の目的を変えたときに，どのような学習アルゴリズムを用いることができるのか，また，学習結果の評価はどのようにすればよいか，ということを第1章で述べた分類に従って順に説明してゆきます．

なお，Wekaの詳細については，ツールの作者らの著書\cite{witten11}を参照してください．scikit-learnについては，（本書も含めて）書籍になっている情報は古い可能性があるので，プロジェクトのサイトにあるドキュメントが一番信頼できる情報源です．
