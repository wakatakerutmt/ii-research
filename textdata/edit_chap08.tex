--------- はじめに　記号の意味と注意（以下Xは数字を表します） -------------
%ID:XXXX(4桁の数字) ・・・IDがXXXXのスライドに対応したテキスト　文章が途中で分かれていて（2つ目）などと表記があってもIDはそのままです。

%ID:なし　　　　     ・・・スライドに直接関係のない、繋ぎや補足の説明。この「ID:なし」の部分から解答を取ってこないようにしてください。
% figure X.X        ・・・図番号。本来ならここに図が入ります、という意味だけで無視して大丈夫です。解答に含めないようにお願いします。
% equation X.X などその他％から始まる記号 ・・・上記同様、解答に含めないようにお願いします。
$\theta$ など       ・・・数式、引用などのTex記号も無視で、解答に含めないようにお願いします。
\cite{mitchell97}

---------　以下からスライドに対応したテキストデータ ------------------


%ID:0801

この章では，数値データからなる特徴ベクトルとその正解クラスの情報からクラスを識別できる，
神経回路網のモデルを作成する方法について説明します．

ニューラルネットワークは，図8.1のような計算ユニットを，階層的に組み合わせて，入力から出力を
計算するメカニズムです．

% figure 8.1

このモデルは生物の神経細胞のモデルであると考えられています．
神経細胞への入力はそれぞれ重み$w$をもっていて，入力があるとそれらの重み付き和が
計算されます．そして，その結果が一定の閾値$\theta$を超えていれば，この神経細胞が出力信号を出し，それが
他の神経細胞へ(こちらも重み付きで)伝播されます．

人間の脳はこのようにモデル化されるニューロンと呼ばれる神経細胞がおよそ$10^{11}$個集まったものです．これらの神経細胞がシナプス結合と呼ばれる方法で互いに結びついていて，この結合が変化することで学習が行われます．

%ID:なし

Wekaで，このユニットの学習をおこなうためには，多層ニューラルネットワーク用に用意された識別器を単層にして利用します．図5.5で説明した単層パーセプトロンとの違いは，閾値関数をシグモイド関数にしている点です（図8.1）．関数の形が似ているので，単層パーセプトロンだけを取り出したときには性能の違いはないのですが，シグモイド関数は微分可能なので，後で説明する誤差逆伝播法が利用可能になります．

ここで述べた最小二乗法は，線形分離不可能な可能性がある学習データに対しても
線形識別面を学習する手法として説明しましたが，最小二乗法そのものは，
対象とする識別関数が非線形であっても，その係数が線形であれば適用できます．

すなわち，特徴ベクトル$\bm{x}$に対する基底関数ベクトル
$\Phi(\bm{x})=(\phi_1(\bm{x}),\dots,\phi_b(\bm{x}))^T$
を考え，識別関数を式(8.1)のように表現すると，この重み$\bm{w}$も，最小二乗法で学習
することができます．

% equation 8.1

たとえば，スカラー値$x$に対して，
$\Phi(\bm{x})=(1, x,\dots ,x^{b-1})^T$
という基底関数ベクトルを使って識別関数を構成すると，二乗誤差を最小にする$b-1$次関数を
求めることができます．

%ID:0802

前節で述べた誤り訂正学習は，特徴空間上では線形識別面を設定することに相当します．

この識別器を神経細胞のニューロンとみなし，脳の構造に倣って階層状に重ねることで，
非線形識別面が実現できます．図8.3のようにパーセプトロンをノードとして，階層状に
結合したものを多層パーセプトロンあるいは
ニューラルネットワーク
とよびます．

% figure 8.3

脳のニューロンは，一般には図8.3のようなきれいな階層状の結合をしておらず，
階層内の結合，階層を飛ばす結合，入力側に戻る結合が入り乱れていると考えられます．このような
任意の結合を持つニューラルネットワークモデルを考えることもできるのですが，学習が非常に複雑に
なるので，まずは図8.3のような3階層のフィードフォワード型ネットワークを対象とします．
一般に3階層のフィードフォワード型モデルでは，入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，
出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します．
中間層は隠れ層(hidden layer)とも呼ばれます．

%ID:0803

前項で説明した基底関数ベクトルによる非線形識別面は，重みパラメータに対しては
線形で，入力を非線形変換することによって実現されています．一方，ここで説明する
ニューラルネットワークによる非線形識別面は，非線形な重みパラメータによって
実現されています．

なぜノードを階層的に組むと非線形識別面が実現できるかというと，図8.4
に示すように，個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，
データの境界の形に合わせた識別面を構成することができるからです．

% figure 8.4

%ID:0805

ノードを階層状に組むことによって，複雑な非線形識別面が実現することが可能なことはわかりました．
問題は，その非線形識別面のパラメータをどのようにしてデータから獲得するか，ということです．

もし，入力層から中間層への重みが固定されていると仮定すると，各学習データに対して，各中間層の
値が決まります．それを新たな学習データとみなして，もとのターゲットを教師信号とする
ロジスティック識別器の学習を行えば，中間層から出力層への重みの学習を行うことができます．

しかし，この方法では中間層が出力すべき値がわからないので，入力層から中間層への重みを学習する
ことができません．出力すべき値はわかりませんが，望ましい値との差であれば計算することができます．
出力層では，出力値とターゲットとの差が，望ましい値との差になります．もし，出力層がすべて正しい
値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正される
べきです．この，重みと出力層の更新量の積が，中間層が出力すべき値との差になります．

このように，出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法(図8.5)を
{\bf 誤差逆伝播法}とよびます．

% figure 8.5

%ID:0806

ニューラルネットワークでも，出力と正解情報の二乗誤差$(y_i - o_i)^2$が最小になるように学習を行います．

% align 8.2

ここでは，最急勾配法を用いて重みを学習します．

% equation8.3

%ID:0807

誤差の二乗和$E(\bm{w})$の勾配方向の計算は以下のようになります．

% align*

%ID:0808

ここで，出力$o_j$を重み$w_i$で偏微分したものは以下の合成微分で求めます．

% equation 8.4

第1項はシグモイド関数の微分です．

% equation 8.5

第2項は入力の重み付き和の微分です．

% equation 8.6

従って，出力層の重みの更新式は以下のようになります．

% equation 8.7

通常，ニューラルネットワークの学習は確率的最急勾配法を用いるので，式8.7の全データに対して和をとる操作を削除します．

また，中間層は，この修正量を重み付きで足し合わせます．

%ID:なし

まとめると，誤差逆伝播法のアルゴリズムは以下のようになります．
ただし，$nn(\bm{w}, [bm{x})$は，重みパラメータを$\bm{w}$に設定されたニューラルネットワークに，$\bm{x}$を入力したときの出力層の出力をベクトルにして返す関数です．


% algorithm 8.1

%ID:0810


人間の神経回路網は，3 階層のフィードフォワード型ネットワークよりはるかに複
雑な構造をもっているはずです．そこで，誤差逆伝播法による学習が流行した1980 年
代後半にも，ニューラルネットワークの階層を深くして性能を向上させようとする試
みはなされてきました．しかし，多段階に誤差逆伝播法を適用すると，誤差が小さく
なって消失してしまうという問題点があり，深い階層のニューラルネットワークの学
習はうまくはゆきませんでした．しかし，2006 年頃に考案された事前学習法をきっか
けに階層の深いディープニューラルネットワークに関する研究が盛んになり，様々な
成果をあげてきました．



ディープニューラルネットワークは，フィードフォワードネット
ワークの中間層を多層にして，性能を向上させたり，適用可能なタスクを増やそうと
したものです．しかし，誤差逆伝播法による多層ネットワークの学習は，重みの修正
量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上
は長い間実現できませんでした．

%ID:0811

勾配消失問題への別のアプローチとして，ユニットの活性化関数を工夫する方法があります．シグモイド関数ではなく，rectified linear関数とよばれる$f(x)=\max(0,x)$（引数が負のときは0，0以上のときはその値を出力）を活性化関数とした ユニットを，
ReLU
(rectified linear unit)（図8.8）とよびます．

% figure 8.8

ReLuを用いると，半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行えることが報告されています．

%ID:なし

本章では，ニューロンを計算単位とする識別手法について説明しました．特徴空間上では識別境界を学習していることになり，ロジスティック識別器では線形な超平面，ニューラルネットワークでは非線形な曲面を学習できます．

ニューラルネットワークは階層を3層に制限しましたが，人間の脳はもっと多くの階層で情報処理を行っています．
ニューラルネットワークの階層を増やして，誤差逆伝播法で学習させようとすると，入力層に近づくほど修正量が
小さくなり，学習が進まないという問題点があります．この問題点を克服したのが第9章で解説する
深層学習です．
